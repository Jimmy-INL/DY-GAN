{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import tensorflow\n",
      "import keras\n",
      "import matplotlib\n",
      "import sklearn\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "\"\"\"This version of the file is just for cooking up new ideas. gan_skip.ipynb is the master file.\"\"\"\n",
    "\n",
    "filename=\"gan_skip_2.ipynb\" #important for bookkeeping since ipython can't use __file__\n",
    "\n",
    "import os\n",
    "# running with non gpu singularity container, so commented out the next line to use CPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(42)\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "print \"import tensorflow\"\n",
    "           \n",
    "import keras.backend.tensorflow_backend as K\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Dense, Dropout, Flatten, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D, LeakyReLU, Lambda\n",
    "from keras.layers import Input, merge, Concatenate, concatenate, Add\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.utils import plot_model\n",
    "print \"import keras\"\n",
    "\n",
    "import numpy as np\n",
    "#from tqdm import tqdm\n",
    "import time\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "print \"import matplotlib\"\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from scipy.stats import binned_statistic_2d\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "print \"import sklearn\"\n",
    "\n",
    "np.random.seed(42)\n",
    "cov_hash = None\n",
    "cov_ans = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "def onetime(func):\n",
    "    \"\"\"stores the functions output, returns the output if called again on the same input, else computes new output\"\"\"\n",
    "    def decorated(*args, **kwargs):\n",
    "        global cov_ans\n",
    "        global cov_hash\n",
    "        new_hash=hashlib.md5(str(args)+str(kwargs)).hexdigest() \n",
    "        if new_hash != cov_hash:\n",
    "            #print(\"computing\")\n",
    "            cov_ans = func(*args, **kwargs)\n",
    "        cov_hash=new_hash\n",
    "        return cov_ans\n",
    "    return decorated\n",
    "    \n",
    "\n",
    "@onetime\n",
    "def covariance_metrics(real_data, predictions):\n",
    "    \"\"\"Takes in real_data matrix with real entries as rows and predictions matrix with generated events as rows and returns the covariance matricies for the two as well as the average, maximum, and std. dev of the difference between the entries in the coverance matrix as well as in the average of the variables.\"\"\"\n",
    "    \n",
    "    cov_pred = np.cov(predictions.T)\n",
    "    avg_pred = predictions.mean(axis=0)\n",
    "    cov_real = np.cov(real_data.T)\n",
    "    avg_real = real_data.mean(axis=0)\n",
    "    \n",
    "    #cov_diff = np.abs((cov_pred - cov_real)/np.sqrt(np.abs(np.outer(avg_real, avg_pred))))\n",
    "    cov_diff = np.abs((cov_pred - cov_real)/cov_real)\n",
    "    ar=avg_real\n",
    "    ar[ar == 0] = 1\n",
    "    avg_diff = np.abs((avg_pred - avg_real)/ar)\n",
    "    \n",
    "    return cov_diff, avg_diff\n",
    "\n",
    "\n",
    "def get_score(real_data, predictions, weight_cov = (1/361.), weight_avg = (1/19.)):\n",
    "    cov_diff, avg_diff = covariance_metrics(real_data, predictions)\n",
    "    return weight_cov*np.sum(cov_diff)+weight_avg*np.sum(avg_diff)\n",
    "\n",
    "def getKS(real_data, predictions):\n",
    "    return ks_2samp(Minv(real_data,ptetaphi=False,nopy2=True), Minv(predictions,ptetaphi=False,nopy2=True))\n",
    "#a = np.matrix([[1,2,3],[4,5,6],[7,8,9]])\n",
    "#ap = np.matrix([[2,2,4],[4,5,6],[7,8,9]])\n",
    "#b = np.matrix([[-1,-2,-3],[-4,-5,-6],[-7,-8,-9]])\n",
    "\n",
    "\n",
    "#print(get_score(a,ap))\n",
    "#print(covariance_metrics(a,b))\n",
    "#print(covariance_metrics(a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Minv(cols,ptetaphi=False,nopy2=False):\n",
    "    \"\"\"\n",
    "    Computes M for two objects given the cartesian momentum projections\n",
    "    if `ptetaphi` is True, then assumes the 8 input columns are cylindrical eptetaphi\n",
    "    if `nopy2` is True, input is 7 columns with no py2\n",
    "    \"\"\"\n",
    "    if ptetaphi:\n",
    "        cols = ptetaphi_to_cartesian(cols)\n",
    "    if nopy2:\n",
    "        M2 = (cols[:,0]+cols[:,4])**2\n",
    "        M2 -= (cols[:,1]+cols[:,5])**2\n",
    "        M2 -= (cols[:,2]          )**2\n",
    "        M2 -= (cols[:,3]+cols[:,6])**2\n",
    "    else:\n",
    "        M2 = (cols[:,0]+cols[:,4])**2\n",
    "        M2 -= (cols[:,1]+cols[:,5])**2\n",
    "        M2 -= (cols[:,2]+cols[:,6])**2\n",
    "        M2 -= (cols[:,3]+cols[:,7])**2\n",
    "    return np.sqrt(M2)\n",
    "\n",
    "def cartesian_to_ptetaphi(eight_cartesian_cols):\n",
    "    \"\"\"\n",
    "    Takes 8 columns as cartesian e px py pz e px py pz\n",
    "    and converts to e pt eta phi e pt eta phi\n",
    "    \"\"\"\n",
    "    e1 =  eight_cartesian_cols[:,0]\n",
    "    e2 =  eight_cartesian_cols[:,4]\n",
    "    px1 = eight_cartesian_cols[:,1]\n",
    "    px2 = eight_cartesian_cols[:,5]\n",
    "    py1 = eight_cartesian_cols[:,2]\n",
    "    py2 = eight_cartesian_cols[:,6]\n",
    "    pz1 = eight_cartesian_cols[:,3]\n",
    "    pz2 = eight_cartesian_cols[:,7]\n",
    "    p1 = np.sqrt(px1**2+py1**2+pz1**2)\n",
    "    p2 = np.sqrt(px2**2+py2**2+pz2**2)\n",
    "    pt1 = np.sqrt(px1**2+py1**2)\n",
    "    pt2 = np.sqrt(px2**2+py2**2)\n",
    "    phi1 = np.arctan2(py1,px1)\n",
    "    phi2 = np.arctan2(py2,px2)\n",
    "    eta1 = np.arctanh(pz1/p1)\n",
    "    eta2 = np.arctanh(pz2/p2)\n",
    "    return np.c_[e1,pt1,eta1,phi1,e2,pt2,eta2,phi2]\n",
    "\n",
    "def ptetaphi_to_cartesian(eight_eptetaphi_cols):\n",
    "    \"\"\"\n",
    "    Takes 8 columns as e pt eta phi e pt eta phi\n",
    "    and converts to e px py pz e px py pz\n",
    "    \"\"\"\n",
    "    e1 =  eight_eptetaphi_cols[:,0]\n",
    "    e2 =  eight_eptetaphi_cols[:,4]\n",
    "    pt1 =  eight_eptetaphi_cols[:,1]\n",
    "    pt2 =  eight_eptetaphi_cols[:,5]\n",
    "    eta1 =  eight_eptetaphi_cols[:,2]\n",
    "    eta2 =  eight_eptetaphi_cols[:,6]\n",
    "    phi1 =  eight_eptetaphi_cols[:,3]\n",
    "    phi2 =  eight_eptetaphi_cols[:,7]\n",
    "    px1 = np.abs(pt1)*np.cos(phi1)\n",
    "    px2 = np.abs(pt2)*np.cos(phi2)\n",
    "    py1 = np.abs(pt1)*np.sin(phi1)\n",
    "    py2 = np.abs(pt2)*np.sin(phi2)\n",
    "    pz1 = np.abs(pt1)/np.tan(2.0*np.arctan(np.exp(-1.*eta1)))\n",
    "    pz2 = np.abs(pt2)/np.tan(2.0*np.arctan(np.exp(-1.*eta2)))\n",
    "    return np.c_[e1,px1,py1,pz1,e2,px2,py2,pz2]\n",
    "\n",
    "def get_dphi(px1,py1,px2,py2):\n",
    "    phi1 = np.arctan2(py1,px1)\n",
    "    phi2 = np.arctan2(py2,px2)\n",
    "    dphi = phi1-phi2\n",
    "    dphi[dphi>np.pi] -= 2*np.pi\n",
    "    dphi[dphi<-np.pi] += 2*np.pi \n",
    "    return dphi\n",
    "\n",
    "def get_rotated_pxpy(px1,py1,px2,py2):\n",
    "    \"\"\"\n",
    "    rotates two leptons such that phi2 = 0\n",
    "    \"\"\"\n",
    "    pt1 = np.sqrt(px1**2+py1**2)\n",
    "    pt2 = np.sqrt(px2**2+py2**2)\n",
    "    phi1 = np.arctan2(py1,px1)\n",
    "    phi2 = np.arctan2(py2,px2)\n",
    "    px1p = pt1*np.cos(phi1-phi2)\n",
    "    py1p = pt1*np.sin(phi1-phi2)\n",
    "    px2p = pt2*np.cos(phi2-phi2)\n",
    "    return px1p,py1p,px2p,np.zeros(len(pt2))\n",
    "    \n",
    "def cartesian_zerophi2(coords,ptetaphi=False):\n",
    "    \"\"\"\n",
    "    returns 8-1=7 columns rotating leptons such that phi2 is 0 (and removing it)\n",
    "    if `ptetaphi` is True, then return eptetaphi instead of epxpypz\n",
    "    \"\"\"\n",
    "    lepcoords_cyl = cartesian_to_ptetaphi(coords)\n",
    "    phi1 = lepcoords_cyl[:,3]\n",
    "    phi2 = lepcoords_cyl[:,7]\n",
    "    dphi = phi1-phi2\n",
    "    dphi[dphi>np.pi] -= 2*np.pi\n",
    "    dphi[dphi<-np.pi] += 2*np.pi\n",
    "    lepcoords_cyl[:,3] = dphi\n",
    "    lepcoords_cyl[:,7] = 0.\n",
    "    if ptetaphi:\n",
    "        return np.delete(lepcoords_cyl, [7], axis=1)\n",
    "    else:\n",
    "        return np.delete(ptetaphi_to_cartesian(lepcoords_cyl), [6], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.stderr = open('test2.txt', 'w')\n",
    "\n",
    "def invmass_from_8cartesian(x):\n",
    "    invmass = K.sqrt(\n",
    "                (x[:,0:1]+x[:,4:5])**2-\n",
    "                (x[:,1:2]+x[:,5:6])**2-\n",
    "                (x[:,2:3]+x[:,6:7])**2-\n",
    "                (x[:,3:4]+x[:,7:8])**2\n",
    "                )\n",
    "    return invmass\n",
    "\n",
    "def invmass_from_8cartesian_nopy2(x):\n",
    "    invmass = K.sqrt(\n",
    "                (x[:,0:1]+x[:,4:5])**2-\n",
    "                (x[:,1:2]+x[:,5:6])**2-\n",
    "                (x[:,2:3]         )**2-\n",
    "                (x[:,3:4]+x[:,6:7])**2\n",
    "                )\n",
    "    return invmass\n",
    "\n",
    "def get_first_N(x,N=19):\n",
    "    return x[:,0:N]\n",
    "\n",
    "def add_invmass_from_8cartesian(x):\n",
    "    return K.concatenate([x,invmass_from_8cartesian(x)])\n",
    "\n",
    "\n",
    "def fix_outputs(x):\n",
    "    \"\"\"\n",
    "    Take nominal delphes format of 19 columns and fix some columns\n",
    "    \"\"\"\n",
    "    return K.concatenate([\n",
    "        # x[:,0:21],\n",
    "        x[:,0:7], # epxpypz for lep1,lep2 -1 for no py2\n",
    "        x[:,7:8], # nvtx\n",
    "        K.sign(x[:,8:10]), # q1 q2\n",
    "        x[:,10:12], # iso1 iso2\n",
    "        x[:,12:14], # met, metphi\n",
    "        x[:,14:19], # jet pts\n",
    "        ])\n",
    "\n",
    "def custom_loss(c, loss_type = \"force_mll\"):\n",
    "    if loss_type == \"force_mll\":\n",
    "        def loss_func(y_true, y_pred_mll):\n",
    "            y_true = y_true[:,0]\n",
    "            y_pred = y_pred_mll[:,0]\n",
    "            mll_pred = y_pred_mll[:,1]\n",
    "\n",
    "            mll_loss = K.mean(K.abs(mll_pred - 91.2))\n",
    "\n",
    "    #         pseudomll = K.random_normal_variable(shape=(1,1), mean=91.2, scale=2)\n",
    "    #         mll_loss = K.mean((mll_pred - pseudomll)**2)\n",
    "\n",
    "            return binary_crossentropy(y_true, y_pred) + c*mll_loss\n",
    "    \n",
    "    elif loss_type == \"force_z_width\":\n",
    "        def loss_func(y_true, y_pred_mll):\n",
    "            y_true = y_true[:,0]\n",
    "            y_pred = y_pred_mll[:,0]\n",
    "            mll_pred = y_pred_mll[:,1]\n",
    "            \n",
    "            mll_loss = K.mean(K.abs(mll_pred - 91.2))\n",
    "            mll_sigma_loss = K.abs(K.std(mll_pred) - 7.67)\n",
    "\n",
    "            return binary_crossentropy(y_true, y_pred) + c*mll_loss + c*mll_sigma_loss\n",
    "    \n",
    "    elif loss_type == \"MLLKS\":\n",
    "        def loss_func(y_true, y_pred_mll):\n",
    "            y_true = y_true[:,0]\n",
    "            y_pred = y_pred_mll[:,0]\n",
    "            mll_pred = y_pred_mll[:,1]\n",
    "            \n",
    "            rmll = mll_pred[:256]\n",
    "            gmll = mll_pred[256:]\n",
    "            \n",
    "            # To sum the differences between the mll_pred vector and rmll, we need a matrix that has\n",
    "            # the rmll vector as it's rows. \n",
    "            #\n",
    "            # Each element in the row should be subtracted by an element of the mll_pred vector.\n",
    "            # Summing accross the rows of that difference matrix will give the desired sum, which are\n",
    "            # the differences between the particular mll value and all the elements in the real mll vector.\n",
    "            #\n",
    "            \n",
    "            rmll_matrix = rmll*K.ones((512, 256))                  #Matrix where each row is a copy of the rmll vector\n",
    "            gmll_matrix = gmll*K.ones((512, 256))                  #Matrix where each row is a copy of the gmll vector\n",
    "            combined_matrix = K.transpose(K.transpose(K.ones((512, 256)))*mll_pred)   #Matrix where each column is a copy of the mll_pred vector\n",
    "            f=open(\"test.txt\",'w+')\n",
    "            #f.write(K.eval(rmll_matrix))\n",
    "            f.write(\"\\n\")\n",
    "            #f.write(K.eval(gmll_matrix))\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "            s_max = 5\n",
    "            s_count = 5\n",
    "            # Plug the difference between the all predictions and the real mll predictions into a sigmoid to map between 0 and 1.\n",
    "            # When summed over axis 1, this gives a rough count of the number of ones or zeros in each row, which represents the \n",
    "            # number of elements which are smaller than the given element of the mll_vector. This essentially gives the CDF value\n",
    "            # of r_mll at the value of mll_pred we're considering.\n",
    "            rCDF = (1.0/256) * K.sum(1.0/(1+K.exp(s_count*(rmll_matrix - combined_matrix))), axis=1) \n",
    "            #rCDF = K.sum(K.exp(s_count*(rmll_matrix)), axis=1) \n",
    "            f.write(str(K.print_tensor(rCDF)))\n",
    "            f.write(\"\\n\")\n",
    "            gCDF = (1.0/256) * K.sum(1.0/(1+K.exp(s_count*(gmll_matrix - combined_matrix))), axis=1) \n",
    "            f.write(str(K.print_tensor(gCDF)))\n",
    "            f.write(\"\\n\")\n",
    "            diff_CDF = K.abs(rCDF - gCDF)\n",
    "            #f.write(str(K.eval(diff_CDF)))\n",
    "            f.write(\"\\n\")\n",
    "            mll_loss = (1.0/s_max)*K.log(K.sum(K.exp(s_max*diff_CDF)))\n",
    "            #f.write(str(K.eval(mll_loss)))\n",
    "            f.write(\"\\n\")\n",
    "            #r_mll=K.backend.sigmoid(r_mll)\n",
    "            #gmll=K.backend.sigmoid(g_mll)\n",
    "            \n",
    "            #rsmll, indexes = tf.nn.top_k(mll_pred[:256], 256, True)\n",
    "            #gsmll, indexes = tf.nn.top_k(mll_pred[256:], 256, True)\n",
    "            #rsmll = np.array(rsmll)\n",
    "            #gsmll = np.array(gsmll)\n",
    "            #f=open(\"test.txt\",'w+')\n",
    "            #f.write(str(rsmll))\n",
    "            #f.write(\"\\n\")\n",
    "            #f.write(str(gsmll))\n",
    "            #f.close()\n",
    "            #all_mll = tf.concat(rsmll,gsmll)\n",
    "            #rs_cdf = np.searchsorted(rsmll,all_mll,side='right')/(1.0*256)\n",
    "            #gs_cdf = (np.searchsorted(gsmll,all_mll,side='right'))/(1.0*256)\n",
    "            #mll_loss = np.max(np.absolute(rscdf-gscdf2))\n",
    "            #mll_loss = K.mean(K.abs(rsmll - gsmll))\n",
    "            \n",
    "            #mll_loss = ks_2samp(np.array(mll_pred), np.array(mll_pred))\n",
    "            #mll_loss = K.mean(K.abs(mll_pred - K.mean(K.abs(mll_pred)))) \n",
    "            #f.write(ks_2samp(np.array(rmll), np.array(gmll)))\n",
    "            f.write(\"\\n\")\n",
    "            f.close()\n",
    "            \n",
    "            #return binary_crossentropy(y_true, y_pred) + 10*mll_loss\n",
    "            #return ks_2samp(np.array(mll_pred), np.array(mll_pred)) - mll_loss\n",
    "            return mll_loss\n",
    "    \n",
    "    elif loss_type == \"discriminator\":\n",
    "        def loss_func(y_true, y_pred_mll):\n",
    "            y_true = y_true[:,0]\n",
    "            y_pred = y_pred_mll[:,0]\n",
    "            \n",
    "            return binary_crossentropy(y_true, y_pred)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(\"Can not make loss function of type %s\" % loss_type)\n",
    "    \n",
    "    return loss_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def make_plots(preds,reals,title=\"\",fname=\"\"):\\n    nrows, ncols = 5,5\\n    fig, axs = plt.subplots(nrows,ncols,figsize=(16,13))\\n    fig.subplots_adjust(wspace=0.1,hspace=0.3)\\n\\n\\n    #print(preds)\\n    info = [\\n        [\"mll\",(60,120,50)],\\n        [\"lep1_e\",(0,250,50)],\\n        [\"lep1_px\",(-100,100,50)],\\n        [\"lep1_py\",(-100,100,50)],\\n        [\"lep1_pz\",(-200,200,50)],\\n        [\"lep2_e\",(0,250,50)],\\n        [\"lep2_px\",(-100,100,50)],\\n        [\"lep2_pz\",(-200,200,50)],\\n        [\"dphi\",(-4,4,50)],\\n        [\"nvtxs\",(0,50,350)],\\n        [\"met\",(0,150,50)],\\n        [\"metphi\",(-6,6,50)],\\n        [\"lep1_charge\",(-7,7,30)],\\n        [\"lep2_charge\",(-7,7,30)],\\n        [\"lep1_iso\",(0,0.2,30)],\\n        [\"lep2_iso\",(0,0.2,30)],\\n        [\"genjet_pt1\",(0,100,50)],\\n        [\"genjet_pt2\",(0,100,50)],\\n        [\"genjet_pt3\",(0,100,50)],\\n        [\"genjet_pt4\",(0,100,50)],\\n        [\"genjet_pt5\",(0,100,50)],\\n\\n    ]\\n    for ic,(cname,crange) in enumerate(info):\\n        if cname == \"mll\":\\n            real = reals[\"mll\"]\\n            pred = Minv(preds,ptetaphi=False,nopy2=True)\\n        elif cname == \"lep1_e\": real, pred = reals[cname], preds[:,0]\\n        elif cname == \"lep1_pz\": real, pred = reals[cname], preds[:,3]\\n        elif cname == \"lep2_e\": real, pred = reals[cname], preds[:,4]\\n        elif cname == \"lep2_pz\": real, pred = reals[cname], preds[:,6]\\n        elif cname == \"lep1_px\": \\n            real = get_rotated_pxpy(reals[\"lep1_px\"], reals[\"lep1_py\"], reals[\"lep2_px\"], reals[\"lep2_py\"])[0]\\n            pred = preds[:,1]\\n        elif cname == \"lep1_py\":\\n            real = get_rotated_pxpy(reals[\"lep1_px\"], reals[\"lep1_py\"], reals[\"lep2_px\"], reals[\"lep2_py\"])[1]\\n            pred = preds[:,2]\\n        elif cname == \"lep2_px\":\\n            real = get_rotated_pxpy(reals[\"lep1_px\"], reals[\"lep1_py\"], reals[\"lep2_px\"], reals[\"lep2_py\"])[2]\\n            pred = preds[:,5]\\n        elif cname == \"dphi\":\\n            real = get_dphi(reals[\"lep1_px\"], reals[\"lep1_py\"], reals[\"lep2_px\"], reals[\"lep2_py\"])\\n            pred = get_dphi(preds[:,1], preds[:,2], preds[:,5], np.zeros(len(preds)))\\n        elif cname == \"nvtxs\": real, pred = reals[cname], np.round(preds[:,7])\\n        elif cname == \"lep1_charge\": real, pred = reals[cname], preds[:,8]\\n        elif cname == \"lep2_charge\": real, pred = reals[cname], preds[:,9]\\n        elif cname == \"lep1_iso\": real, pred = reals[cname], preds[:,10]\\n        elif cname == \"lep2_iso\": real, pred = reals[cname], preds[:,11]\\n        elif cname == \"met\": real, pred = reals[cname], preds[:,12]\\n        elif cname == \"metphi\": real, pred = reals[cname], METPhiMap(preds[:,13])\\n        elif cname == \"genjet_pt1\": real, pred = reals[cname], preds[:,14]\\n        elif cname == \"genjet_pt2\": real, pred = reals[cname], preds[:,15]\\n        elif cname == \"genjet_pt3\": real, pred = reals[cname], preds[:,16]\\n        elif cname == \"genjet_pt4\": real, pred = reals[cname], preds[:,17]\\n        elif cname == \"genjet_pt5\": real, pred = reals[cname], preds[:,18]\\n        idx = ic // ncols, ic % ncols\\n        bins_real = axs[idx].hist(real, range=crange[:2],bins=crange[-1], histtype=\"step\", lw=2,density=True)\\n        bins_pred = axs[idx].hist(pred, range=crange[:2],bins=crange[-1], histtype=\"step\", lw=2,density=True)\\n        axs[idx].set_xlabel(\"{}\".format(cname))\\n        axs[idx].get_yaxis().set_visible(False)\\n    #     axs[idx].set_yscale(\"log\", nonposy=\\'clip\\')\\n    _ = axs[0,0].legend([\"True\",\"Pred\"], loc=\\'upper right\\')\\n    _ = axs[0,0].set_title(title)\\n    plt.tight_layout()\\n    if fname:\\n        fig.savefig(fname)'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def METPhiMap(metphis):\n",
    "    \"\"\"Maps number line to period boundary conditions between [-pi,pi]\"\"\"\n",
    "    #Add or subtract the proper number of factors of 2pi. If number is between [-3pi, -pi] add 2pi,\n",
    "    #if between [-5pi, -3pi], add 4pi. For positive intervals subtract instead of add. To get the \n",
    "    #number of 2pis to subtract or add, take the floor of the abs of the number over pi, that gives\n",
    "    #the integer number of pis away from 0. Subtract 2pi for every 2pi if you are greater than pi,\n",
    "    #and subtract another for every 2pi greater than pi. The same holds in reverse for negative values.\n",
    "    return metphis - np.sign(metphis)*np.ceil(np.floor(np.abs(metphis)/(np.pi))/2)*2*np.pi\n",
    "\n",
    "def M4(E,px,py,pz):\n",
    "    \"\"\"Takes in 4 vector components in cartesian and outputs minkowski scalar invariant\"\"\"\n",
    "    return np.sqrt(E*E - px*px - py*py - pz*pz)\n",
    "        \n",
    "#a = np.array([100,200,300,400,500,600,700,800,900])\n",
    "#b = np.array([-1,-2,-3,-4,-5,-6,-7,-8,-9])\n",
    "#c = np.array([10,20,30,40,50,60,70,80,90])\n",
    "#d = np.array([-10,-20,-30,-40,-50,-60,-70,-80,-90])\n",
    "#print(M4(a,b,c,d))\n",
    "\n",
    "def make_plots(preds,reals,title=\"\",fname=\"\",show_pred=True,wspace=0.1,hspace=0.3,tightlayout=True,visible=False):\n",
    "    nrows, ncols = 5,5\n",
    "    fig, axs = plt.subplots(nrows,ncols,figsize=(16,13))\n",
    "#     fig, axs = plt.subplots(nrows,ncols,figsize=(12,10))\n",
    "#     fig.subplots_adjust(wspace=0.1,hspace=0.3)\n",
    "    fig.subplots_adjust(wspace=wspace,hspace=hspace)\n",
    "\n",
    "\n",
    "    info = [\n",
    "        [\"lep1_e\",(0,250,50)],\n",
    "        [\"lep1_px\",(-100,100,50)],\n",
    "        [\"lep1_py\",(-100,100,50)],\n",
    "        [\"lep1_pz\",(-200,200,50)],\n",
    "        [\"lep2_e\",(0,250,50)],\n",
    "        [\"lep2_px\",(-100,100,50)],\n",
    "        [\"lep2_pz\",(-200,200,50)],\n",
    "        [\"dphi\",(-4,4,50)],\n",
    "        [\"nvtxs\",(0,50,350)],\n",
    "        [\"met\",(0,150,50)],\n",
    "        [\"metphi\",(-6,6,50)],\n",
    "        [\"lep1_charge\",(-7,7,30)],\n",
    "        [\"lep2_charge\",(-7,7,30)],\n",
    "        [\"lep1_iso\",(0,2.0,30)],\n",
    "        [\"lep2_iso\",(0,2.0,30)],\n",
    "        [\"jet_pt1\",(0,100,50)],\n",
    "        [\"jet_pt2\",(0,100,50)],\n",
    "        [\"jet_pt3\",(0,100,50)],\n",
    "        [\"jet_pt4\",(0,100,50)],\n",
    "        [\"jet_pt5\",(0,100,50)],\n",
    "        [\"mll\",(60,120,50)],\n",
    "        [\"lep1_mass\",(0,1,50)],\n",
    "        [\"lep2_mass\",(0,1,50)],\n",
    "        [\"njets\",(0,7,7)],\n",
    "\n",
    "    ]\n",
    "    for axx in axs:\n",
    "        for ax in axx:\n",
    "            ax.get_yaxis().set_visible(False)\n",
    "    for ic,(cname,crange) in enumerate(info):\n",
    "        if cname == \"mll\":\n",
    "            real = reals[\"mll\"]\n",
    "            pred = Minv(preds,ptetaphi=False,nopy2=True)\n",
    "        elif cname == \"lep1_mass\": real, pred = M4(reals[\"lep1_e\"], reals[\"lep1_px\"], reals[\"lep1_py\"], reals[\"lep1_pz\"]), M4(preds[:,0], preds[:,1], preds[:,2], preds[:,3])\n",
    "        elif cname == \"lep2_mass\": real, pred = M4(reals[\"lep2_e\"], reals[\"lep2_px\"], 0, reals[\"lep2_pz\"]), M4(preds[:,4], preds[:,5], preds[:,6], preds[:,7])\n",
    "        elif cname == \"lep1_e\": real, pred = reals[cname], preds[:,0]\n",
    "        elif cname == \"lep1_pz\": real, pred = reals[cname], preds[:,3]\n",
    "        elif cname == \"lep2_e\": real, pred = reals[cname], preds[:,4]\n",
    "        elif cname == \"lep2_pz\": real, pred = reals[cname], preds[:,6]\n",
    "        elif cname == \"lep1_px\": \n",
    "            real = reals[cname]\n",
    "            pred = preds[:,1]\n",
    "        elif cname == \"lep1_py\":\n",
    "            real = reals[cname]\n",
    "            pred = preds[:,2]\n",
    "        elif cname == \"lep2_px\":\n",
    "            real = reals[cname]\n",
    "            pred = preds[:,5]\n",
    "        elif cname == \"dphi\":\n",
    "            real = get_dphi(reals[\"lep1_px\"], reals[\"lep1_py\"], reals[\"lep2_px\"], np.zeros(len(reals)))\n",
    "            pred = get_dphi(preds[:,1], preds[:,2], preds[:,5], np.zeros(len(preds)))\n",
    "        elif cname == \"nvtxs\": real, pred = reals[cname], np.round(preds[:,7])\n",
    "        elif cname == \"lep1_charge\": real, pred = reals[cname], preds[:,8]\n",
    "        elif cname == \"lep2_charge\": real, pred = reals[cname], preds[:,9]\n",
    "        elif cname == \"lep1_iso\": real, pred = reals[cname], preds[:,10]\n",
    "        elif cname == \"lep2_iso\": real, pred = reals[cname], preds[:,11]\n",
    "        elif cname == \"met\": real, pred = reals[cname], preds[:,12]\n",
    "        elif cname == \"metphi\": real, pred = reals[cname], METPhiMap(preds[:,13])\n",
    "        elif cname == \"jet_pt1\": real, pred = reals[cname], preds[:,14]\n",
    "        elif cname == \"jet_pt2\": real, pred = reals[cname], preds[:,15]\n",
    "        elif cname == \"jet_pt3\": real, pred = reals[cname], preds[:,16]\n",
    "        elif cname == \"jet_pt4\": real, pred = reals[cname], preds[:,17]\n",
    "        elif cname == \"jet_pt5\": real, pred = reals[cname], preds[:,18]\n",
    "        elif cname == \"njets\":\n",
    "            real = \\\n",
    "                1*(reals[\"jet_pt1\"] > 10) + \\\n",
    "                1*(reals[\"jet_pt2\"] > 10) + \\\n",
    "                1*(reals[\"jet_pt3\"] > 10) + \\\n",
    "                1*(reals[\"jet_pt4\"] > 10) + \\\n",
    "                1*(reals[\"jet_pt5\"] > 10)\n",
    "            pred = \\\n",
    "                1*(preds[:,14] > 10) + \\\n",
    "                1*(preds[:,15] > 10) + \\\n",
    "                1*(preds[:,16] > 10) + \\\n",
    "                1*(preds[:,17] > 10) + \\\n",
    "                1*(preds[:,18] > 10)\n",
    "        idx = ic // ncols, ic % ncols\n",
    "        bins_real = axs[idx].hist(real, range=crange[:2],bins=crange[-1], histtype=\"step\", lw=1.5,density=True)\n",
    "        if show_pred:\n",
    "            bins_pred = axs[idx].hist(pred, range=crange[:2],bins=crange[-1], histtype=\"step\", lw=1.5,density=True)\n",
    "        axs[idx].set_xlabel(\"{}\".format(cname))\n",
    "        axs[idx].get_yaxis().set_visible(False)\n",
    "    #     axs[idx].set_yscale(\"log\", nonposy='clip')\n",
    "    _ = axs[0,0].legend([\"True\",\"Pred\"], loc='upper right')\n",
    "    _ = axs[0,0].set_title(title)\n",
    "    if tightlayout:\n",
    "        plt.tight_layout()\n",
    "    if fname:\n",
    "        fig.savefig(fname)\n",
    "    if not visible:\n",
    "        plt.close(fig)\n",
    "\n",
    "\"\"\"def make_plots(preds,reals,title=\"\",fname=\"\"):\n",
    "    nrows, ncols = 5,5\n",
    "    fig, axs = plt.subplots(nrows,ncols,figsize=(16,13))\n",
    "    fig.subplots_adjust(wspace=0.1,hspace=0.3)\n",
    "\n",
    "\n",
    "    #print(preds)\n",
    "    info = [\n",
    "        [\"mll\",(60,120,50)],\n",
    "        [\"lep1_e\",(0,250,50)],\n",
    "        [\"lep1_px\",(-100,100,50)],\n",
    "        [\"lep1_py\",(-100,100,50)],\n",
    "        [\"lep1_pz\",(-200,200,50)],\n",
    "        [\"lep2_e\",(0,250,50)],\n",
    "        [\"lep2_px\",(-100,100,50)],\n",
    "        [\"lep2_pz\",(-200,200,50)],\n",
    "        [\"dphi\",(-4,4,50)],\n",
    "        [\"nvtxs\",(0,50,350)],\n",
    "        [\"met\",(0,150,50)],\n",
    "        [\"metphi\",(-6,6,50)],\n",
    "        [\"lep1_charge\",(-7,7,30)],\n",
    "        [\"lep2_charge\",(-7,7,30)],\n",
    "        [\"lep1_iso\",(0,0.2,30)],\n",
    "        [\"lep2_iso\",(0,0.2,30)],\n",
    "        [\"genjet_pt1\",(0,100,50)],\n",
    "        [\"genjet_pt2\",(0,100,50)],\n",
    "        [\"genjet_pt3\",(0,100,50)],\n",
    "        [\"genjet_pt4\",(0,100,50)],\n",
    "        [\"genjet_pt5\",(0,100,50)],\n",
    "\n",
    "    ]\n",
    "    for ic,(cname,crange) in enumerate(info):\n",
    "        if cname == \"mll\":\n",
    "            real = reals[\"mll\"]\n",
    "            pred = Minv(preds,ptetaphi=False,nopy2=True)\n",
    "        elif cname == \"lep1_e\": real, pred = reals[cname], preds[:,0]\n",
    "        elif cname == \"lep1_pz\": real, pred = reals[cname], preds[:,3]\n",
    "        elif cname == \"lep2_e\": real, pred = reals[cname], preds[:,4]\n",
    "        elif cname == \"lep2_pz\": real, pred = reals[cname], preds[:,6]\n",
    "        elif cname == \"lep1_px\": \n",
    "            real = get_rotated_pxpy(reals[\"lep1_px\"], reals[\"lep1_py\"], reals[\"lep2_px\"], reals[\"lep2_py\"])[0]\n",
    "            pred = preds[:,1]\n",
    "        elif cname == \"lep1_py\":\n",
    "            real = get_rotated_pxpy(reals[\"lep1_px\"], reals[\"lep1_py\"], reals[\"lep2_px\"], reals[\"lep2_py\"])[1]\n",
    "            pred = preds[:,2]\n",
    "        elif cname == \"lep2_px\":\n",
    "            real = get_rotated_pxpy(reals[\"lep1_px\"], reals[\"lep1_py\"], reals[\"lep2_px\"], reals[\"lep2_py\"])[2]\n",
    "            pred = preds[:,5]\n",
    "        elif cname == \"dphi\":\n",
    "            real = get_dphi(reals[\"lep1_px\"], reals[\"lep1_py\"], reals[\"lep2_px\"], reals[\"lep2_py\"])\n",
    "            pred = get_dphi(preds[:,1], preds[:,2], preds[:,5], np.zeros(len(preds)))\n",
    "        elif cname == \"nvtxs\": real, pred = reals[cname], np.round(preds[:,7])\n",
    "        elif cname == \"lep1_charge\": real, pred = reals[cname], preds[:,8]\n",
    "        elif cname == \"lep2_charge\": real, pred = reals[cname], preds[:,9]\n",
    "        elif cname == \"lep1_iso\": real, pred = reals[cname], preds[:,10]\n",
    "        elif cname == \"lep2_iso\": real, pred = reals[cname], preds[:,11]\n",
    "        elif cname == \"met\": real, pred = reals[cname], preds[:,12]\n",
    "        elif cname == \"metphi\": real, pred = reals[cname], METPhiMap(preds[:,13])\n",
    "        elif cname == \"genjet_pt1\": real, pred = reals[cname], preds[:,14]\n",
    "        elif cname == \"genjet_pt2\": real, pred = reals[cname], preds[:,15]\n",
    "        elif cname == \"genjet_pt3\": real, pred = reals[cname], preds[:,16]\n",
    "        elif cname == \"genjet_pt4\": real, pred = reals[cname], preds[:,17]\n",
    "        elif cname == \"genjet_pt5\": real, pred = reals[cname], preds[:,18]\n",
    "        idx = ic // ncols, ic % ncols\n",
    "        bins_real = axs[idx].hist(real, range=crange[:2],bins=crange[-1], histtype=\"step\", lw=2,density=True)\n",
    "        bins_pred = axs[idx].hist(pred, range=crange[:2],bins=crange[-1], histtype=\"step\", lw=2,density=True)\n",
    "        axs[idx].set_xlabel(\"{}\".format(cname))\n",
    "        axs[idx].get_yaxis().set_visible(False)\n",
    "    #     axs[idx].set_yscale(\"log\", nonposy='clip')\n",
    "    _ = axs[0,0].legend([\"True\",\"Pred\"], loc='upper right')\n",
    "    _ = axs[0,0].set_title(title)\n",
    "    plt.tight_layout()\n",
    "    if fname:\n",
    "        fig.savefig(fname)\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN():\n",
    "    def __init__(self, **kwargs):\n",
    "\n",
    "        self.args = dict(kwargs)\n",
    "\n",
    "        self.tag = kwargs[\"tag\"]\n",
    "        self.input_file = str(kwargs[\"input_file\"])\n",
    "        self.noise_shape = (int(kwargs[\"noise_size\"]),)\n",
    "        self.output_shape = (int(kwargs[\"output_size\"]),)\n",
    "        self.noise_type = int(kwargs[\"noise_type\"])\n",
    "        self.ntest_samples = int(kwargs[\"ntest_samples\"])\n",
    "        self.nepochs_dump_pred_metrics = int(kwargs[\"nepochs_dump_pred_metrics\"])\n",
    "        self.nepochs_dump_models = int(kwargs[\"nepochs_dump_models\"])\n",
    "        self.nepochs_dump_plots = int(kwargs[\"nepochs_dump_plots\"])\n",
    "        self.nepochs_max = int(kwargs[\"nepochs_max\"])\n",
    "        self.batch_size = int(kwargs[\"batch_size\"])\n",
    "        self.do_concatenate_disc = kwargs[\"do_concatenate_disc\"]\n",
    "        self.do_concatenate_gen = kwargs[\"do_concatenate_gen\"]\n",
    "        self.do_batch_normalization_disc = kwargs[\"do_batch_normalization_disc\"]\n",
    "        self.do_batch_normalization_gen = kwargs[\"do_batch_normalization_gen\"]\n",
    "        self.do_soft_labels = kwargs[\"do_soft_labels\"]\n",
    "        self.do_noisy_labels = kwargs[\"do_noisy_labels\"]\n",
    "        self.do_tanh_gen = kwargs[\"do_tanh_gen\"]\n",
    "        self.nepochs_decay_noisy_labels = int(kwargs[\"nepochs_decay_noisy_labels\"])\n",
    "        self.use_ptetaphi_additionally = kwargs[\"use_ptetaphi_additionally\"]\n",
    "        self.optimizer_gen = kwargs[\"optimizer_gen\"]\n",
    "        self.optimizer_disc = kwargs[\"optimizer_disc\"]\n",
    "        self.depth_disc = kwargs[\"depth_disc\"]\n",
    "        self.width_disc = kwargs[\"width_disc\"]\n",
    "        self.depth_gen = kwargs[\"depth_gen\"]\n",
    "        self.width_gen = kwargs[\"width_gen\"]\n",
    "        self.beefy_generator = kwargs[\"beefy_generator\"]\n",
    "        self.beefy_discriminator = kwargs[\"beefy_discriminator\"]\n",
    "        self.add_invmass_disc = kwargs[\"add_invmass_disc\"]\n",
    "        self.fix_delphes_outputs = kwargs[\"fix_delphes_outputs\"]\n",
    "        self.use_delphes = kwargs[\"use_delphes\"]\n",
    "        self.use_mll_loss = kwargs[\"use_mll_loss\"]\n",
    "        self.loss_mll_weight = kwargs[\"loss_mll_weight\"]\n",
    "        self.do_skip_connection = kwargs[\"do_skip_connection\"]\n",
    "        self.terminate_early = kwargs[\"terminate_early\"]\n",
    "        self.loss_type = kwargs[\"loss_type\"]\n",
    "        if self.use_ptetaphi_additionally: self.output_shape = (self.output_shape[0]+8,)\n",
    "            \n",
    "        print(self.__dict__)\n",
    "\n",
    "        os.system(\"mkdir -p progress/{}/\".format(self.tag))\n",
    "\n",
    "        self.scaler_type = kwargs[\"scaler_type\"]\n",
    "        self.scaler = None\n",
    "        if self.scaler_type.lower() == \"minmax\":\n",
    "            self.scaler = MinMaxScaler(feature_range=(-1.,1.))\n",
    "        elif self.scaler_type.lower() == \"robust\":\n",
    "            self.scaler = RobustScaler()\n",
    "        elif self.scaler_type.lower() == \"standard\":\n",
    "            self.scaler = StandardScaler()\n",
    "\n",
    "        self.data = None\n",
    "        self.data_ref = None\n",
    "        self.d_epochinfo = {}\n",
    "        self.X_train = None\n",
    "\n",
    "        # optimizer = Adam(0.0002, 0.5)\n",
    "        optimizer_d = self.optimizer_disc\n",
    "        # optimizer_d = \"sgd\"\n",
    "        optimizer_g = self.optimizer_gen\n",
    "        # optimizer_g = \"adam\"\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        if self.use_mll_loss:\n",
    "            gen_loss = custom_loss(c=self.loss_mll_weight, loss_type=self.loss_type)\n",
    "        else:\n",
    "            gen_loss = \"binary_crossentropy\"\n",
    "            \n",
    "        self.gen_loss=gen_loss\n",
    "        self.disc_loss=custom_loss(c=0, loss_type=\"discriminator\")\n",
    "            \n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss=self.disc_loss,\n",
    "            optimizer=optimizer_d,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build and compile the generator\n",
    "        self.generator = self.build_generator()\n",
    "        self.generator.compile(loss=self.gen_loss, optimizer=optimizer_g)\n",
    "\n",
    "        # The generator takes noise as input and generated imgs\n",
    "        z = Input(shape=self.noise_shape)\n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The valid takes generated images as input and determines validity\n",
    "        valid = self.discriminator(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator) takes\n",
    "        # noise as input => generates images => determines validity\n",
    "        self.combined = Model(z, valid)\n",
    "        self.combined.compile(loss=gen_loss, optimizer=optimizer_g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(GAN):\n",
    "    \n",
    "    def build_generator(self):\n",
    "\n",
    "        inputs = Input(shape=self.noise_shape)\n",
    "\n",
    "        ## Head\n",
    "        x = Dense(64)(inputs)\n",
    "        if self.do_batch_normalization_gen:\n",
    "            x = BatchNormalization()(x)\n",
    "        x = LeakyReLU(alpha=0.2)(x)\n",
    "        if self.do_concatenate_gen:\n",
    "            x = Lambda(lambda x: K.concatenate([x*x,x]))(x)\n",
    "            x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "        ## Main Body\n",
    "        if self.depth_gen > 0 and self.width_gen > 0:\n",
    "            for level in xrange(0,self.depth_gen):\n",
    "                size=self.width_gen/(2**level)\n",
    "                if(size<self.output_shape[0]):\n",
    "                    raise ValueError(\"The layer size %d would be smaller than the output size, make sure you have a wide enough network to deal with %s layers\" % (size, self.depth_gen))\n",
    "                x = Dense(size)(x) #Triangle with width halved at each level\n",
    "                x = LeakyReLU(alpha=0.2)(x)\n",
    "        elif self.beefy_generator:\n",
    "            for size in [128,256,512,256,128]:\n",
    "                x = Dense(size)(x)\n",
    "                x = LeakyReLU(alpha=0.2)(x)\n",
    "        else:\n",
    "            for size in [128,128,128,64,32]:\n",
    "                x = Dense(size)(x)\n",
    "                x = LeakyReLU(alpha=0.2)(x)\n",
    " \n",
    "        ## Tail\n",
    "        x = Dense(self.output_shape[0])(x)\n",
    "        \n",
    "#         if False:\n",
    "        if self.do_skip_connection:\n",
    "            # get the non-noise part of the input, and add it to the tail\n",
    "            y = Lambda(get_first_N, arguments={'N': self.output_shape[0]})(inputs)\n",
    "#             print y\n",
    "            x = Add()([x,y])\n",
    "#             print x\n",
    "            \n",
    "        if self.do_tanh_gen:\n",
    "            x = Activation(\"tanh\")(x)\n",
    "        elif self.fix_delphes_outputs:\n",
    "            x = Lambda(fix_outputs,\n",
    "                input_shape=self.output_shape,\n",
    "                output_shape=self.output_shape\n",
    "                )(x)\n",
    "            \n",
    "#         model = Model(inputs=inputs, outputs=concatenate([out,mll]))\n",
    "        model = Model(inputs=inputs, outputs=[x])\n",
    "        \n",
    "        print \"Generator params: {}\".format(model.count_params())\n",
    "#         model.summary()\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(GAN):\n",
    "    \n",
    "    def build_discriminator(self):\n",
    "\n",
    "        inputs = Input(self.output_shape)\n",
    "        mll = Lambda(invmass_from_8cartesian_nopy2)(inputs)\n",
    "        x = Dense(128)(inputs)\n",
    "        if self.do_batch_normalization_disc:\n",
    "            x = BatchNormalization()(x)\n",
    "        x = LeakyReLU(alpha=0.2)(x)\n",
    "        if self.do_concatenate_disc:\n",
    "            x = Lambda(lambda x: K.concatenate([x*x,x]))(x)\n",
    "            x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "        ## Main Body\n",
    "        if self.depth_disc > 0 and self.width_disc > 0:\n",
    "            for level in xrange(0,self.depth_disc):\n",
    "                x = Dense(self.width_disc/(2**level))(x) #Triangle with width halved at each level\n",
    "                x = LeakyReLU(alpha=0.2)(x)\n",
    "        elif self.beefy_generator:\n",
    "            for size in [128,256,256,128,64,32,16,8]:\n",
    "                x = Dense(size)(x)\n",
    "                x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "        else:\n",
    "            for size in [128]*5 + [64,32,16,8]:\n",
    "                x = Dense(size)(x)\n",
    "                x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "        ## Tail\n",
    "        out = Dense(1,activation='sigmoid')(x)\n",
    "        \n",
    "        if self.use_mll_loss:\n",
    "            model = Model(inputs=inputs, outputs=concatenate([out,mll]))\n",
    "        else:\n",
    "            model = Model(inputs=inputs, outputs=out)\n",
    "#         print model.output_shape\n",
    "#         model.summary()\n",
    "        print \"Discriminator params: {}\".format(model.count_params())\n",
    "        \n",
    "        return model\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(GAN):\n",
    "    \n",
    "    def load_data(self):\n",
    "        if self.data is not None: return\n",
    "        \n",
    "        if self.use_delphes:\n",
    "            self.data = np.load(self.input_file)\n",
    "        else:\n",
    "            self.data = np.load(self.input_file)\n",
    "            \n",
    "        self.data = self.data[self.data[\"genmll\"] > 50.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(GAN):\n",
    "\n",
    "    def get_noise(self, amount=1024):\n",
    "        # nominal\n",
    "        if self.noise_type == 1:\n",
    "            noise_half = np.random.normal(0, 1, (amount//2, self.noise_shape[0]))\n",
    "            noise_full = np.random.normal(0, 1, (amount, self.noise_shape[0]))\n",
    "\n",
    "        elif self.noise_type == 2: # random soup, 4,2,2 have to be modified to sum to noise_shape[0]\n",
    "            ngaus = self.noise_shape[0] // 2\n",
    "            nflat = (self.noise_shape[0] - ngaus) // 2\n",
    "            nexpo = self.noise_shape[0] - nflat - ngaus\n",
    "            noise_gaus = np.random.normal( 0, 1, (amount//2+amount, ngaus))\n",
    "            noise_flat = np.random.uniform(-1, 1, (amount//2+amount, nflat))\n",
    "            noise_expo = np.random.exponential( 1,    (amount//2+amount, nexpo))\n",
    "            noise = np.c_[ noise_gaus,noise_flat,noise_expo ]\n",
    "            noise_half = noise[:amount//2]\n",
    "            noise_full = noise[-amount:]\n",
    "            \n",
    "        elif self.noise_type == 3: # truth conditioned\n",
    "            \n",
    "#             noise_half = np.c_[ \n",
    "#                     self.X_train[np.random.randint(0, self.X_train.shape[0], amount//2)], \n",
    "#                     np.random.normal(0, 1, (amount//2,self.noise_shape[0]-self.X_train.shape[1]))\n",
    "#                     ]\n",
    "#             noise_full = np.c_[ \n",
    "#                     self.X_train[np.random.randint(0, self.X_train.shape[0], amount)], \n",
    "#                     np.random.normal(0, 1, (amount,self.noise_shape[0]-self.X_train.shape[1]))\n",
    "#                     ]\n",
    "            \n",
    "            npurenoise = self.noise_shape[0]-self.X_train.shape[1]\n",
    "            ngaus = npurenoise // 2\n",
    "            nflat = (npurenoise - ngaus) // 2\n",
    "            nexpo = npurenoise - nflat - ngaus\n",
    "            noise_gaus = np.random.normal( 0, 1, (amount//2+amount, ngaus))\n",
    "            noise_flat = np.random.uniform(-1, 1, (amount//2+amount, nflat))\n",
    "            noise_expo = np.random.exponential( 1,    (amount//2+amount, nexpo))\n",
    "            truenoise = self.X_train[np.random.randint(0, self.X_train.shape[0], amount//2+amount)]\n",
    "            noise = np.c_[ truenoise,noise_gaus,noise_flat,noise_expo ]\n",
    "            noise_half = noise[:amount//2]\n",
    "            noise_full = noise[-amount:]\n",
    "\n",
    "        return noise_half, noise_full\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(GAN):\n",
    "            \n",
    "    def train(self):\n",
    "\n",
    "        self.load_data()\n",
    "        \n",
    "        if self.use_delphes:\n",
    "            lepcoords = np.c_[\n",
    "                self.data[\"lep1_e\"],\n",
    "                self.data[\"lep1_px\"],\n",
    "                self.data[\"lep1_py\"],\n",
    "                self.data[\"lep1_pz\"],\n",
    "                self.data[\"lep2_e\"],\n",
    "                self.data[\"lep2_px\"],\n",
    "#                 self.data[\"lep2_py\"],\n",
    "                self.data[\"lep2_pz\"],\n",
    "            ]\n",
    "#             lepcoords_dphi = cartesian_zerophi2(lepcoords)\n",
    "            \n",
    "            nvtx_smeared = np.round(np.random.normal(self.data[\"nvtxs\"],0.5))\n",
    "            self.X_train = np.c_[\n",
    "#                 lepcoords_dphi, # 7 columns\n",
    "                lepcoords, # 7 columns\n",
    "                nvtx_smeared, # 1 column\n",
    "                self.data[\"lep1_charge\"], self.data[\"lep2_charge\"],\n",
    "                self.data[\"lep1_iso\"], self.data[\"lep2_iso\"],\n",
    "                self.data[\"met\"], self.data[\"metphi\"],\n",
    "                self.data[\"jet_pt1\"],\n",
    "                self.data[\"jet_pt2\"],\n",
    "                self.data[\"jet_pt3\"],\n",
    "                self.data[\"jet_pt4\"],\n",
    "                self.data[\"jet_pt5\"],\n",
    "            ].astype(np.float32)\n",
    "        else:\n",
    "            self.X_train = self.data[:,range(1,1+8)]\n",
    "            if self.use_ptetaphi_additionally:\n",
    "                self.X_train = np.c_[self.X_train, cartesian_to_ptetaphi(self.X_train)]\n",
    "\n",
    "        # # NOTE. StandardScaler should be fit on training set\n",
    "        # # and applied the same to train and test, otherwise we\n",
    "        # # introduce a bias\n",
    "        if self.scaler:\n",
    "            self.scaler.fit(self.X_train)\n",
    "            self.X_train = self.scaler.transform(self.X_train).astype(np.float32)\n",
    "            pickle.dump(self.scaler, open(\"progress/{}/scaler.pkl\".format(self.tag),'w'))\n",
    "\n",
    "        # make an alias to save typing\n",
    "        X_train = self.X_train\n",
    "        \n",
    "        half_batch = int(self.batch_size / 2)\n",
    "\n",
    "        prev_gen_loss = -1\n",
    "        prev_disc_loss = -1\n",
    "        n_loss_same_gen = 0  # number of epochs for which generator loss has remained ~same (within 0.01%)\n",
    "        n_loss_same_disc = 0  # number of epochs for which discriminator loss has remained ~same (within 0.01%)\n",
    "        old_info = -1, -1\n",
    "        \n",
    "        logfile = open(\"progress/{}/log.txt\".format(self.tag),'w+')\n",
    "        os.system(\"cp {} progress/{}/\".format(filename, self.tag))\n",
    "        logfile.write(\"Getting Started! Copied notebook into progress directory.\\n\")\n",
    "        for epoch in range(self.nepochs_max):\n",
    "\n",
    "            if self.terminate_early:\n",
    "                if n_loss_same_gen > 1000 or n_loss_same_disc > 1000:\n",
    "                    print \"BREAKING because disc/gen loss has remained the same for {}/{} epochs!\".format(n_loss_same_disc,n_loss_same_gen)\n",
    "                    break\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random half batch of images\n",
    "            idx = np.random.randint(0, X_train.shape[0], half_batch)\n",
    "            imgs = X_train[idx]\n",
    "            \n",
    "            noise_half, noise_full = self.get_noise(self.batch_size)\n",
    "            \n",
    "            # Generate a half batch of new images\n",
    "            gen_imgs = self.generator.predict(noise_half)\n",
    "\n",
    "            # Train the discriminator\n",
    "            ones = np.ones((half_batch, 1))\n",
    "            zeros = np.zeros((half_batch, 1))\n",
    "\n",
    "            if self.do_soft_labels:\n",
    "                ones *= 0.9\n",
    "\n",
    "            if self.do_noisy_labels:\n",
    "                frac = 0.3*np.exp(-epoch/self.nepochs_decay_noisy_labels)\n",
    "                if frac > 0.005:\n",
    "                    ones[np.random.randint(0, len(ones), int(frac*len(ones)))] = 0\n",
    "                    zeros[np.random.randint(0, len(zeros), int(frac*len(zeros)))] = 1\n",
    "\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, ones)\n",
    "            #print(\"Real Disc loss: %s \" % str(d_loss_real[0]))\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, zeros)\n",
    "            #print(\"Fake Disc loss: %s \" % str(d_loss_real[0]))\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "            #print(\"Avg Disc loss: %s \" % str(d_loss[0]))\n",
    "            \n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "            # The generator wants the discriminator to label the generated samples\n",
    "            # as valid (ones)\n",
    "            valid_y = np.array([1] * self.batch_size)\n",
    "\n",
    "            # Train the generator\n",
    "            g_loss = self.combined.train_on_batch(noise_full, valid_y)\n",
    "\n",
    "            if (g_loss - prev_gen_loss) < 0.0001: n_loss_same_gen += 1\n",
    "            else: n_loss_same_gen = 0\n",
    "            prev_gen_loss = g_loss\n",
    "\n",
    "            if (d_loss[0] - prev_disc_loss) < 0.0001: n_loss_same_disc += 1\n",
    "            else: n_loss_same_disc = 0\n",
    "            prev_disc_loss = d_loss[0]\n",
    "\n",
    "            # Plot the progress\n",
    "#             print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "            sys.stdout.write(\"\\r{} [D loss: {}, acc.: {:.2f}%] [G loss: {}] [mll={:.3f}+-{:.3f}]\".format(epoch, d_loss[0], 100.0*d_loss[1], g_loss, old_info[0], old_info[1]))\n",
    "\n",
    "            if epoch % self.nepochs_dump_pred_metrics == 0 and epoch > 0:\n",
    "            \n",
    "                _, noise_test = self.get_noise(self.ntest_samples)\n",
    "            \n",
    "                sys.stdout.write(\"\\n\") # break up the stream of text\n",
    "\n",
    "                gen_imgs = self.generator.predict(noise_test)\n",
    "                \n",
    "                if self.scaler:\n",
    "                    gen_imgs = self.scaler.inverse_transform(gen_imgs)\n",
    "\n",
    "                masses = Minv(gen_imgs,nopy2=True)\n",
    "                masses = masses[np.isfinite(masses)]\n",
    "                old_info = masses.mean(), masses.std()\n",
    "\n",
    "                cov_diff, avg_diff = covariance_metrics(X_train, gen_imgs)\n",
    "                \n",
    "                if \"epoch\" not in self.d_epochinfo:\n",
    "                    self.d_epochinfo[\"epoch\"] = []\n",
    "                    self.d_epochinfo[\"d_acc\"] = []\n",
    "                    self.d_epochinfo[\"d_loss\"] = []\n",
    "                    self.d_epochinfo[\"g_loss\"] = []\n",
    "                    self.d_epochinfo[\"mass_mu\"] = []\n",
    "                    self.d_epochinfo[\"mass_sig\"] = []\n",
    "                    self.d_epochinfo[\"time\"] = []\n",
    "                    self.d_epochinfo[\"avg_cov_diff\"] = []\n",
    "                    self.d_epochinfo[\"max_cov_diff\"] = []\n",
    "                    self.d_epochinfo[\"std_dev_cov_diff\"] = []\n",
    "                    self.d_epochinfo[\"avg_mean_diff\"] = []\n",
    "                    self.d_epochinfo[\"max_mean_diff\"] = []\n",
    "                    self.d_epochinfo[\"std_dev_mean_diff\"] = []\n",
    "                    self.d_epochinfo[\"args\"] = self.args\n",
    "                else:\n",
    "                    self.d_epochinfo[\"epoch\"].append(epoch)\n",
    "                    self.d_epochinfo[\"d_acc\"].append(100*d_loss[1])\n",
    "                    self.d_epochinfo[\"d_loss\"].append(d_loss[0])\n",
    "                    self.d_epochinfo[\"g_loss\"].append(g_loss)\n",
    "                    self.d_epochinfo[\"mass_mu\"].append(masses.mean())\n",
    "                    self.d_epochinfo[\"mass_sig\"].append(masses.std())\n",
    "                    self.d_epochinfo[\"time\"].append(time.time())\n",
    "                    self.d_epochinfo[\"avg_cov_diff\"].append(cov_diff.mean())\n",
    "                    self.d_epochinfo[\"max_cov_diff\"].append(cov_diff.max())\n",
    "                    self.d_epochinfo[\"std_dev_cov_diff\"].append(cov_diff.std())\n",
    "                    self.d_epochinfo[\"avg_mean_diff\"].append(avg_diff.mean())\n",
    "                    self.d_epochinfo[\"max_mean_diff\"].append(avg_diff.max())\n",
    "                    self.d_epochinfo[\"std_dev_mean_diff\"].append(avg_diff.std())\n",
    "                    \n",
    "\n",
    "                pickle.dump(self.d_epochinfo, open(\"progress/{}/history.pkl\".format(self.tag),'w'))\n",
    "\n",
    "            if epoch % self.nepochs_dump_plots == 0 and epoch > 0:\n",
    "                _, noise = self.get_noise(self.ntest_samples)\n",
    "                preds = self.generator.predict(noise)\n",
    "                reals = self.data[:15000]\n",
    "                _ = make_plots(preds,reals,title=\"{}: epoch {}\".format(self.tag,epoch),\n",
    "                               fname=\"progress/{}/plots_{:06d}.png\".format(self.tag,epoch))\n",
    "            \n",
    "            if epoch % self.nepochs_dump_models == 0 and epoch > 0:\n",
    "                self.discriminator.save(\"progress/{}/disc_{}.weights\".format(self.tag,epoch))\n",
    "                self.generator.save(\"progress/{}/gen_{}.weights\".format(self.tag,epoch))\n",
    "                #self.discriminator.save(\"test.weights\")\n",
    "                score = get_score(X_train, gen_imgs)\n",
    "                mll_ks_score = ks_2samp(Minv(X_train,ptetaphi=False,nopy2=True), Minv(gen_imgs,ptetaphi=False,nopy2=True))\n",
    "                MetPhi_ks_score = ks_2samp(METPhiMap(X_train[:,13]), METPhiMap(gen_imgs[:,13]))\n",
    "                Lep1Iso_ks_score = ks_2samp(X_train[:,10], gen_imgs[:,10])\n",
    "                trial = self.tag[1:self.tag.index('_')]\n",
    "                print \"epoch %d trial %s StatsScore %f MLLKSStatistic %f MLLKSPval %f METPhiKSStatistic %f METPhiKSPval %f Lep1IsoKSStatistic %f Lep1IsoKSPval %f \" % (epoch, trial, score, mll_ks_score[0], mll_ks_score[1], MetPhi_ks_score[0], MetPhi_ks_score[1], Lep1Iso_ks_score[0], Lep1Iso_ks_score[1])\n",
    "                logfile.write(\"epoch %d trial %s StatsScore %f MLLKSStatistic %f MLLKSPval %f METPhiKSStatistic %f METPhiKSPval %f Lep1IsoKSStatistic %f Lep1IsoKSPval %f\\n\" % (epoch, trial, score, mll_ks_score[0], mll_ks_score[1], MetPhi_ks_score[0], MetPhi_ks_score[1], Lep1Iso_ks_score[0], Lep1Iso_ks_score[1]))\n",
    "        \n",
    "        logfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'width_disc': 0, 'ntest_samples': 10000, 'output_size': 19, 'terminate_early': False, 'do_batch_normalization_disc': False, 'use_delphes': True, 'nepochs_dump_plots': 500, 'use_ptetaphi_additionally': False, 'do_noisy_labels': False, 'tag': 'v3_batch512_bgbd_KSMLL_NonTC_newdata_mllfix', 'nepochs_dump_pred_metrics': 250, 'loss_mll_weight': 0.01, 'do_batch_normalization_gen': False, 'add_invmass_disc': False, 'width_gen': 0, 'fix_delphes_outputs': False, 'loss_type': 'MLLKS', 'nepochs_dump_models': 500, 'input_file': '/home/users/bhashemi/Projects/GIT/DY-GAN/delphes/total_Zmumu_13TeV_PU20_v2.npa', 'noise_type': 1, 'scaler_type': 'minmax', 'batch_size': 512, 'do_concatenate_disc': False, 'do_soft_labels': False, 'depth_gen': 0, 'noise_size': 19, 'optimizer_disc': 'adadelta', 'nepochs_max': 100001, 'beefy_discriminator': True, 'depth_disc': 0, 'do_tanh_gen': False, 'do_skip_connection': False, 'use_mll_loss': True, 'beefy_generator': True, 'nepochs_decay_noisy_labels': 2000, 'optimizer_gen': 'adadelta', 'do_concatenate_gen': False}\n",
      "{'width_disc': 0, 'ntest_samples': 10000, 'beefy_generator': True, 'loss_type': 'MLLKS', 'terminate_early': False, 'do_batch_normalization_disc': False, 'use_delphes': True, 'use_ptetaphi_additionally': False, 'do_noisy_labels': False, 'tag': 'v3_batch512_bgbd_KSMLL_NonTC_newdata_mllfix', 'noise_shape': (19,), 'fix_delphes_outputs': False, 'nepochs_dump_pred_metrics': 250, 'loss_mll_weight': 0.01, 'do_batch_normalization_gen': False, 'add_invmass_disc': False, 'width_gen': 0, 'output_shape': (19,), 'noise_type': 1, 'do_skip_connection': False, 'nepochs_dump_models': 500, 'input_file': '/home/users/bhashemi/Projects/GIT/DY-GAN/delphes/total_Zmumu_13TeV_PU20_v2.npa', 'args': {'width_disc': 0, 'ntest_samples': 10000, 'output_size': 19, 'terminate_early': False, 'do_batch_normalization_disc': False, 'use_delphes': True, 'use_ptetaphi_additionally': False, 'optimizer_disc': 'adadelta', 'tag': 'v3_batch512_bgbd_KSMLL_NonTC_newdata_mllfix', 'fix_delphes_outputs': False, 'nepochs_dump_pred_metrics': 250, 'do_batch_normalization_gen': False, 'add_invmass_disc': False, 'width_gen': 0, 'depth_disc': 0, 'do_skip_connection': False, 'noise_type': 1, 'loss_type': 'MLLKS', 'nepochs_dump_models': 500, 'input_file': '/home/users/bhashemi/Projects/GIT/DY-GAN/delphes/total_Zmumu_13TeV_PU20_v2.npa', 'scaler_type': 'minmax', 'batch_size': 512, 'do_concatenate_disc': False, 'do_soft_labels': False, 'depth_gen': 0, 'noise_size': 19, 'do_noisy_labels': False, 'nepochs_max': 100001, 'beefy_discriminator': True, 'loss_mll_weight': 0.01, 'do_tanh_gen': False, 'nepochs_decay_noisy_labels': 2000, 'use_mll_loss': True, 'nepochs_dump_plots': 500, 'beefy_generator': True, 'optimizer_gen': 'adadelta', 'do_concatenate_gen': False}, 'batch_size': 512, 'do_concatenate_disc': False, 'do_soft_labels': False, 'depth_gen': 0, 'optimizer_disc': 'adadelta', 'nepochs_max': 100001, 'beefy_discriminator': True, 'depth_disc': 0, 'do_tanh_gen': False, 'do_concatenate_gen': False, 'use_mll_loss': True, 'nepochs_dump_plots': 500, 'nepochs_decay_noisy_labels': 2000, 'optimizer_gen': 'adadelta'}\n",
      "Discriminator params: 161793\n",
      "Generator params: 340883\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_21 (InputLayer)           (None, 19)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_137 (Dense)               (None, 128)          2560        input_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_121 (LeakyReLU)     (None, 128)          0           dense_137[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_138 (Dense)               (None, 128)          16512       leaky_re_lu_121[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_122 (LeakyReLU)     (None, 128)          0           dense_138[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_139 (Dense)               (None, 256)          33024       leaky_re_lu_122[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_123 (LeakyReLU)     (None, 256)          0           dense_139[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_140 (Dense)               (None, 256)          65792       leaky_re_lu_123[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_124 (LeakyReLU)     (None, 256)          0           dense_140[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_141 (Dense)               (None, 128)          32896       leaky_re_lu_124[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_125 (LeakyReLU)     (None, 128)          0           dense_141[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_142 (Dense)               (None, 64)           8256        leaky_re_lu_125[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_126 (LeakyReLU)     (None, 64)           0           dense_142[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_143 (Dense)               (None, 32)           2080        leaky_re_lu_126[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_127 (LeakyReLU)     (None, 32)           0           dense_143[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_144 (Dense)               (None, 16)           528         leaky_re_lu_127[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_128 (LeakyReLU)     (None, 16)           0           dense_144[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_145 (Dense)               (None, 8)            136         leaky_re_lu_128[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_129 (LeakyReLU)     (None, 8)            0           dense_145[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_146 (Dense)               (None, 1)            9           leaky_re_lu_129[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_9 (Lambda)               (None, 1)            0           input_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 2)            0           dense_146[0][0]                  \n",
      "                                                                 lambda_9[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 323,586\n",
      "Trainable params: 161,793\n",
      "Non-trainable params: 161,793\n",
      "__________________________________________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_22 (InputLayer)        (None, 19)                0         \n",
      "_________________________________________________________________\n",
      "dense_147 (Dense)            (None, 64)                1280      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_130 (LeakyReLU)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_148 (Dense)            (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_131 (LeakyReLU)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_149 (Dense)            (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_132 (LeakyReLU)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_150 (Dense)            (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_133 (LeakyReLU)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_151 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_134 (LeakyReLU)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_152 (Dense)            (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_135 (LeakyReLU)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_153 (Dense)            (None, 19)                2451      \n",
      "=================================================================\n",
      "Total params: 340,883\n",
      "Trainable params: 340,883\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# defaults\n",
    "params = {\n",
    "        \"input_file\": \"data_xyz.npy\",\n",
    "        \"output_size\": 19,\n",
    "        \"noise_size\": 19,\n",
    "        \"noise_type\": 1,\n",
    "        \"ntest_samples\": 10000,\n",
    "        \"nepochs_dump_pred_metrics\": 250,\n",
    "        \"nepochs_dump_plots\": 500,\n",
    "        \"nepochs_dump_models\": 5000,\n",
    "        \"nepochs_max\": 100001,\n",
    "        \"batch_size\": 200,\n",
    "        \"do_concatenate_disc\": False,\n",
    "        \"do_concatenate_gen\": False,\n",
    "        \"do_batch_normalization_disc\": False,\n",
    "        \"do_batch_normalization_gen\": False,\n",
    "        \"do_soft_labels\": False,\n",
    "        \"do_noisy_labels\": False,\n",
    "        \"do_tanh_gen\": False,\n",
    "        \"nepochs_decay_noisy_labels\": 3000,\n",
    "        \"use_ptetaphi_additionally\": False,\n",
    "        \"scaler_type\": \"minmax\",\n",
    "        \"optimizer_disc\": \"adadelta\",\n",
    "        \"optimizer_gen\": \"adadelta\",\n",
    "        \"beefy_generator\": False,\n",
    "        \"beefy_discriminator\": False,\n",
    "        \"depth_gen\": 0,\n",
    "        \"width_gen\": 0,\n",
    "        \"depth_disc\": 0,\n",
    "        \"width_disc\": 0,\n",
    "        \"add_invmass_disc\": False,\n",
    "        \"fix_delphes_outputs\": True,\n",
    "        \"use_delphes\": False,\n",
    "        \"use_mll_loss\": True,\n",
    "        \"do_skip_connection\": False,\n",
    "        \"terminate_early\": True,\n",
    "        \"loss_type\": \"force_mll\"\n",
    "        }\n",
    "\n",
    "# for delphes:\n",
    "params.update({\n",
    "    \"use_delphes\": True,\n",
    "    #\"fix_delphes_outputs\": True,\n",
    "    \"fix_delphes_outputs\": False,\n",
    "    \"do_soft_labels\": False,\n",
    "    \"do_noisy_labels\": False,\n",
    "    \"nepochs_decay_noisy_labels\": 2000,\n",
    "    \"input_file\": \"/home/users/bhashemi/Projects/GIT/DY-GAN/delphes/total_Zmumu_13TeV_PU20_v2.npa\",\n",
    "    \"output_size\": 19,\n",
    "})\n",
    "params.update({\n",
    "    \"noise_type\": 1,\n",
    "    \"noise_size\": 19, # 19 for the true events and 8 more for noise\n",
    "    \"use_mll_loss\": True,\n",
    "    \"loss_mll_weight\": 0.01,\n",
    "    \"nepochs_max\": 100001,\n",
    "    #\"nepochs_max\": 1001,\n",
    "    \"batch_size\": 512,\n",
    "    \"do_skip_connection\": False,\n",
    "    \"terminate_early\": False,\n",
    "    \"nepochs_dump_models\": 500,\n",
    "    \"beefy_generator\": True,\n",
    "    \"beefy_discriminator\": True,\n",
    "    #\"loss_type\": \"force_mll\",\n",
    "    #\"loss_type\": \"force_z_width\",\n",
    "    \"loss_type\": \"MLLKS\",\n",
    "    #\"depth_gen\": 8,\n",
    "    #\"width_gen\": 10000\n",
    "    #\"nepochs_dump_plots\": 1,\n",
    "    #\"nepochs_dump_models\": 1,\n",
    "    #\"nepochs_max\": 10,\n",
    "    \n",
    "})\n",
    "\n",
    "# change tag for provenance\n",
    "# params[\"tag\"] = \"v1_512_bgbd_nomll\"\n",
    "params[\"tag\"] = \"v3_batch512_bgbd_KSMLL_NonTC_newdata_mllfix\"\n",
    "\n",
    "print params\n",
    "gan = GAN(**params)\n",
    "\n",
    "#plot_model(model, to_file='progress/%s/model.png' % params[\"tag\"], show_shapes=True, show_layer_names=True)\n",
    "gan.discriminator.summary()\n",
    "gan.generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250 [D loss: 6.22181141807e-06, acc.: 50.59%] [G loss: 1.24766492844] [mll=-1.000+--1.000]\n",
      "500 [D loss: 2.80575568468e-06, acc.: 51.17%] [G loss: 1.24766492844] [mll=1484.222+-334.461]\n",
      "epoch 500 trial 3 StatsScore 210569844.245891 MLLKSStatistic 0.999477 MLLKSPval 0.000000 METPhiKSStatistic 0.397533 METPhiKSPval 0.000000 Lep1IsoKSStatistic 1.000000 Lep1IsoKSPval 0.000000 \n",
      "750 [D loss: 1.32591003421e-06, acc.: 50.59%] [G loss: 1.24766492844] [mll=1493.223+-331.991]\n",
      "1000 [D loss: 1.99502392206e-06, acc.: 50.00%] [G loss: 1.24766492844] [mll=1494.531+-339.960]\n",
      "epoch 1000 trial 3 StatsScore 215868264.053636 MLLKSStatistic 0.999477 MLLKSPval 0.000000 METPhiKSStatistic 0.383301 METPhiKSPval 0.000000 Lep1IsoKSStatistic 1.000000 Lep1IsoKSPval 0.000000 \n",
      "1250 [D loss: 9.31319789288e-07, acc.: 50.59%] [G loss: 1.24766492844] [mll=1491.241+-340.689]\n",
      "1500 [D loss: 6.31203477042e-07, acc.: 50.20%] [G loss: 1.24766492844] [mll=1492.103+-343.908]\n",
      "epoch 1500 trial 3 StatsScore 217637414.878735 MLLKSStatistic 0.999477 MLLKSPval 0.000000 METPhiKSStatistic 0.385478 METPhiKSPval 0.000000 Lep1IsoKSStatistic 1.000000 Lep1IsoKSPval 0.000000 \n",
      "1750 [D loss: 6.40764710624e-07, acc.: 50.39%] [G loss: 1.24766492844] [mll=1492.500+-347.657]\n",
      "2000 [D loss: 4.24888725092e-07, acc.: 50.20%] [G loss: 1.24766492844] [mll=1496.735+-340.041]\n",
      "epoch 2000 trial 3 StatsScore 215364149.349224 MLLKSStatistic 0.999477 MLLKSPval 0.000000 METPhiKSStatistic 0.385286 METPhiKSPval 0.000000 Lep1IsoKSStatistic 1.000000 Lep1IsoKSPval 0.000000 \n",
      "2250 [D loss: 3.427576587e-07, acc.: 50.20%] [G loss: 1.24766492844] [mll=1491.505+-338.236]6]\n",
      "2500 [D loss: 3.30793682224e-07, acc.: 50.20%] [G loss: 1.24766492844] [mll=1499.739+-335.771]\n",
      "epoch 2500 trial 3 StatsScore 221826629.365960 MLLKSStatistic 0.999477 MLLKSPval 0.000000 METPhiKSStatistic 0.386959 METPhiKSPval 0.000000 Lep1IsoKSStatistic 1.000000 Lep1IsoKSPval 0.000000 \n",
      "2750 [D loss: 5.02327509366e-07, acc.: 50.59%] [G loss: 1.24766492844] [mll=1491.184+-345.659]\n",
      "3000 [D loss: 3.72151390593e-07, acc.: 50.59%] [G loss: 1.24766492844] [mll=1488.439+-350.123]\n",
      "epoch 3000 trial 3 StatsScore 222249900.316512 MLLKSStatistic 0.999477 MLLKSPval 0.000000 METPhiKSStatistic 0.384945 METPhiKSPval 0.000000 Lep1IsoKSStatistic 1.000000 Lep1IsoKSPval 0.000000 \n",
      "3250 [D loss: 3.82393238851e-07, acc.: 50.59%] [G loss: 1.24766492844] [mll=1487.214+-346.029]\n",
      "3500 [D loss: 2.94430833492e-07, acc.: 50.39%] [G loss: 1.24766492844] [mll=1495.291+-342.017]\n",
      "epoch 3500 trial 3 StatsScore 221450006.550195 MLLKSStatistic 0.999477 MLLKSPval 0.000000 METPhiKSStatistic 0.386112 METPhiKSPval 0.000000 Lep1IsoKSStatistic 1.000000 Lep1IsoKSPval 0.000000 \n",
      "3750 [D loss: 3.05918575805e-07, acc.: 50.20%] [G loss: 1.24766492844] [mll=1490.587+-344.627]\n",
      "4000 [D loss: 2.68009813453e-07, acc.: 50.20%] [G loss: 1.24766492844] [mll=1495.936+-343.392]\n",
      "epoch 4000 trial 3 StatsScore 223542143.590010 MLLKSStatistic 0.999477 MLLKSPval 0.000000 METPhiKSStatistic 0.387547 METPhiKSPval 0.000000 Lep1IsoKSStatistic 1.000000 Lep1IsoKSPval 0.000000 \n",
      "4250 [D loss: 2.51404799201e-07, acc.: 50.20%] [G loss: 1.24766492844] [mll=1488.409+-352.474]\n",
      "4500 [D loss: 2.26394064384e-07, acc.: 50.59%] [G loss: 1.24766492844] [mll=1492.451+-345.956]\n",
      "epoch 4500 trial 3 StatsScore 218417182.721701 MLLKSStatistic 0.999477 MLLKSPval 0.000000 METPhiKSStatistic 0.386385 METPhiKSPval 0.000000 Lep1IsoKSStatistic 1.000000 Lep1IsoKSPval 0.000000 \n",
      "4750 [D loss: 2.17658339352e-07, acc.: 50.20%] [G loss: 1.24766492844] [mll=1485.662+-348.832]\n",
      "5000 [D loss: 2.02084294187e-07, acc.: 50.59%] [G loss: 1.24766492844] [mll=1490.724+-346.216]\n",
      "epoch 5000 trial 3 StatsScore 215877831.308483 MLLKSStatistic 0.999477 MLLKSPval 0.000000 METPhiKSStatistic 0.385562 METPhiKSPval 0.000000 Lep1IsoKSStatistic 1.000000 Lep1IsoKSPval 0.000000 \n",
      "5250 [D loss: 1.98423805386e-07, acc.: 50.00%] [G loss: 1.24766492844] [mll=1489.703+-342.306]\n",
      "5500 [D loss: 1.69694104102e-07, acc.: 50.98%] [G loss: 1.24766492844] [mll=1491.585+-342.777]\n",
      "epoch 5500 trial 3 StatsScore 228890976.786425 MLLKSStatistic 0.999477 MLLKSPval 0.000000 METPhiKSStatistic 0.383854 METPhiKSPval 0.000000 Lep1IsoKSStatistic 1.000000 Lep1IsoKSPval 0.000000 \n",
      "5750 [D loss: 2.21120473043e-07, acc.: 50.39%] [G loss: 1.24766492844] [mll=1491.371+-343.285]\n",
      "6000 [D loss: 1.69502101244e-07, acc.: 50.59%] [G loss: 1.24766492844] [mll=1485.849+-344.169]\n",
      "epoch 6000 trial 3 StatsScore 221992567.289022 MLLKSStatistic 0.999477 MLLKSPval 0.000000 METPhiKSStatistic 0.387340 METPhiKSPval 0.000000 Lep1IsoKSStatistic 1.000000 Lep1IsoKSPval 0.000000 \n",
      "6054 [D loss: 1.81581555125e-07, acc.: 50.20%] [G loss: 1.24766492844] [mll=1486.762+-349.890]"
     ]
    }
   ],
   "source": [
    "gan.train()\n",
    "#os.system(\"gfal-copy -p -f -t 4200 --verbose file://`pwd`/progress/%s gsiftp://gftp.t2.ucsd.edu/cms/store/user/bhashemi/GAN/%s --checksum ADLER32\" % (gan.tag, gan.tag))\n",
    "#os.system(\"rm -rf progress/%s\" % gan.tag)\n",
    "\n",
    "#for i in xrange(17,100):\n",
    "#for i in xrange(17,30):\n",
    "#    params[\"tag\"] = \"v%d_batch512_bgbd_mllANDwidth_NonTC_newdata_mllfix_batchrun\" % i\n",
    "#    gan = GAN(**params)\n",
    "#    gan.train()\n",
    "#    os.system(\"gfal-copy -p -f -t 4200 --verbose file://`pwd`/progress/%s gsiftp://gftp.t2.ucsd.edu/cms/store/user/bhashemi/GAN/%s --checksum ADLER32\" % (gan.tag, gan.tag))\n",
    "#    os.system(\"rm -rf progress/%s\" % gan.tag)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot masses\n",
    "plt.plot(gan.d_epochinfo[\"mass_sig\"])\n",
    "plt.plot(gan.d_epochinfo[\"mass_mu\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot losses\n",
    "plt.plot(gan.d_epochinfo[\"d_loss\"])\n",
    "plt.plot(gan.d_epochinfo[\"g_loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get noise, predict from it, and plot stuff. easy.\n",
    "\n",
    "# You must load the config of the model you want into the gan first by running the block of code with the\n",
    "# proper config settings or the loss function will be messed up here.\n",
    "\n",
    "#tag = gan.tag\n",
    "tag = \"v2_512_bgbd_mllANDwidth_NonTC\"\n",
    "gan.load_data()\n",
    "epoch=52000\n",
    "\n",
    "print(\"progress/%s/gen_%d.weights\" % (tag,epoch))\n",
    "model = load_model(\"progress/%s/gen_%d.weights\" % (tag,epoch), custom_objects={'loss_func': custom_loss(c=\"\", loss_type=gan.loss_type)})\n",
    "_, noise = gan.get_noise(50000)\n",
    "# print noise\n",
    "# print noise.shape\n",
    "# print noise\n",
    "preds = model.predict(noise)\n",
    "print (preds-noise[:,0:19]).mean(axis=0)\n",
    "cov_pred = np.cov(preds.T)\n",
    "cov_real = np.cov(noise[:,0:19].T)\n",
    "cov_diff = (cov_pred - cov_real)\n",
    "print(cov_real[2,2], cov_pred[2,2], cov_diff[2,2])\n",
    "print(cov_diff.shape)\n",
    "print(cov_real)\n",
    "print(cov_pred)\n",
    "print(cov_diff)\n",
    "# make_plots(noise,gan.data[:5000],title=\"epoch {}\".format(3000))\n",
    "make_plots(preds,gan.data[:5000],title=\"epoch {}\".format(3000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mfake = Minv(preds,nopy2=True)\n",
    "mreal = Minv(noise[:,0:19],nopy2=True)\n",
    "mreal = mreal[np.isfinite(mreal)]\n",
    "mfake = mfake[np.isfinite(mfake)]\n",
    "print mreal.mean(), mreal.std()\n",
    "print mfake.mean(), mfake.std()\n",
    "#print (mreal-mfake)[:100]\n",
    "print(np.mean(noise[:,0]))\n",
    "#_ = plt.hist(mreal-mfake,bins=np.linspace(-50,50,100))\n",
    "\n",
    "_ = plt.hist((preds-noise[:,0:19])[:,18],bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## I need to add a step that computes the covariance matrix elements between the variables, but for some reason I'm\n",
    "## struggling to understand how to write the function.\n",
    "import itertools\n",
    "\n",
    "mean = [0, 0]\n",
    "cov = [[4, 2], [2, 5]]  # diagonal covariance\n",
    "\n",
    "x, y = np.random.multivariate_normal(mean, cov, 5000).T\n",
    "#z = np.random.multivariate_normal(mean, cov, 5000).T\n",
    "\n",
    "#print(x,y)\n",
    "\n",
    "def get_covariance(row1, row2):\n",
    "    mean1 = np.mean(row1)\n",
    "    mean2 = np.mean(row2)\n",
    "    #print(\"row1: %s, \\n mean1: %d, sum1: %d \\n var1 = %d \" % (row1, mean1, np.mean(row1), np.sum((row1 - mean1)*(row1 - mean1)) ) )\n",
    "    #print(\"row2: %s, \\n mean2: %d, sum2: %d \\n var2 = %d \" % (row2, mean2, np.mean(row2), np.sum(row2 - mean2)) )\n",
    "    return (np.mean((row1 - mean1)*(row2-mean2)))\n",
    "\n",
    "print(get_covariance(x,y))\n",
    "np.cov(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(0,100,50)\n",
    "_ = plt.hist(gan.data[\"met\"][gan.data[\"nvtxs\"] < 18],bins=bins,histtype=\"step\", density=True, label=\"low PU\")\n",
    "_ = plt.hist(gan.data[\"met\"][gan.data[\"nvtxs\"] > 28],bins=bins,histtype=\"step\", density=True, label=\"high PU\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(0,100,50)\n",
    "_ = plt.hist(preds[:,12][preds[:,7] < 18],bins=bins,histtype=\"step\", density=True, label=\"low PU\")\n",
    "_ = plt.hist(preds[:,12][preds[:,7] > 28],bins=bins,histtype=\"step\", density=True, label=\"high PU\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

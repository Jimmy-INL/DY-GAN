{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import tensorflow\n",
      "import keras\n",
      "import matplotlib\n",
      "import sklearn\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "# running with non gpu singularity container, so commented out the next line to use CPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(42)\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "print \"import tensorflow\"\n",
    "           \n",
    "import keras.backend.tensorflow_backend as K\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Dense, Dropout, Flatten, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D, LeakyReLU, Lambda\n",
    "from keras.layers import Input, merge, Concatenate, concatenate, Add, Multiply\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.optimizers import RMSprop,Adadelta\n",
    "print \"import keras\"\n",
    "\n",
    "import numpy as np\n",
    "# from tqdm import tqdm\n",
    "import time\n",
    "import pickle\n",
    "import sys\n",
    "import glob\n",
    "import hashlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "print \"import matplotlib\"\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from scipy.stats import binned_statistic_2d\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "\n",
    "print \"import sklearn\"\n",
    "\n",
    "np.random.seed(42)\n",
    "cov_hash = None\n",
    "cov_ans = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Minv(cols,ptetaphi=False,nopy2=True):\n",
    "    \"\"\"\n",
    "    Computes M for two objects given the cartesian momentum projections\n",
    "    if `ptetaphi` is True, then assumes the 8 input columns are cylindrical eptetaphi\n",
    "    if `nopy2` is True, input is 7 columns with no py2\n",
    "    \"\"\"\n",
    "    if ptetaphi:\n",
    "        cols = ptetaphi_to_cartesian(cols)\n",
    "    if nopy2:\n",
    "        M2 = (cols[:,0]+cols[:,4])**2\n",
    "        M2 -= (cols[:,1]+cols[:,5])**2\n",
    "        M2 -= (cols[:,2]          )**2\n",
    "        M2 -= (cols[:,3]+cols[:,6])**2\n",
    "    else:\n",
    "        M2 = (cols[:,0]+cols[:,4])**2\n",
    "        M2 -= (cols[:,1]+cols[:,5])**2\n",
    "        M2 -= (cols[:,2]+cols[:,6])**2\n",
    "        M2 -= (cols[:,3]+cols[:,7])**2\n",
    "    return np.sqrt(M2)\n",
    "\n",
    "def cartesian_to_ptetaphi(eight_cartesian_cols):\n",
    "    \"\"\"\n",
    "    Takes 8 columns as cartesian e px py pz e px py pz\n",
    "    and converts to e pt eta phi e pt eta phi\n",
    "    \"\"\"\n",
    "    e1 =  eight_cartesian_cols[:,0]\n",
    "    e2 =  eight_cartesian_cols[:,4]\n",
    "    px1 = eight_cartesian_cols[:,1]\n",
    "    px2 = eight_cartesian_cols[:,5]\n",
    "    py1 = eight_cartesian_cols[:,2]\n",
    "    py2 = eight_cartesian_cols[:,6]\n",
    "    pz1 = eight_cartesian_cols[:,3]\n",
    "    pz2 = eight_cartesian_cols[:,7]\n",
    "    p1 = np.sqrt(px1**2+py1**2+pz1**2)\n",
    "    p2 = np.sqrt(px2**2+py2**2+pz2**2)\n",
    "    pt1 = np.sqrt(px1**2+py1**2)\n",
    "    pt2 = np.sqrt(px2**2+py2**2)\n",
    "    phi1 = np.arctan2(py1,px1)\n",
    "    phi2 = np.arctan2(py2,px2)\n",
    "    eta1 = np.arctanh(pz1/p1)\n",
    "    eta2 = np.arctanh(pz2/p2)\n",
    "    return np.c_[e1,pt1,eta1,phi1,e2,pt2,eta2,phi2]\n",
    "\n",
    "def ptetaphi_to_cartesian(eight_eptetaphi_cols):\n",
    "    \"\"\"\n",
    "    Takes 8 columns as e pt eta phi e pt eta phi\n",
    "    and converts to e px py pz e px py pz\n",
    "    \"\"\"\n",
    "    e1 =  eight_eptetaphi_cols[:,0]\n",
    "    e2 =  eight_eptetaphi_cols[:,4]\n",
    "    pt1 =  eight_eptetaphi_cols[:,1]\n",
    "    pt2 =  eight_eptetaphi_cols[:,5]\n",
    "    eta1 =  eight_eptetaphi_cols[:,2]\n",
    "    eta2 =  eight_eptetaphi_cols[:,6]\n",
    "    phi1 =  eight_eptetaphi_cols[:,3]\n",
    "    phi2 =  eight_eptetaphi_cols[:,7]\n",
    "    px1 = np.abs(pt1)*np.cos(phi1)\n",
    "    px2 = np.abs(pt2)*np.cos(phi2)\n",
    "    py1 = np.abs(pt1)*np.sin(phi1)\n",
    "    py2 = np.abs(pt2)*np.sin(phi2)\n",
    "    pz1 = np.abs(pt1)/np.tan(2.0*np.arctan(np.exp(-1.*eta1)))\n",
    "    pz2 = np.abs(pt2)/np.tan(2.0*np.arctan(np.exp(-1.*eta2)))\n",
    "    return np.c_[e1,px1,py1,pz1,e2,px2,py2,pz2]\n",
    "\n",
    "def get_dphi(px1,py1,px2,py2):\n",
    "    phi1 = np.arctan2(py1,px1)\n",
    "    phi2 = np.arctan2(py2,px2)\n",
    "    dphi = phi1-phi2\n",
    "    dphi[dphi>np.pi] -= 2*np.pi\n",
    "    dphi[dphi<-np.pi] += 2*np.pi \n",
    "    return dphi\n",
    "\n",
    "def M4(E,px,py,pz):\n",
    "    return np.sqrt(E*E - px*px - py*py - pz*pz)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invmass_from_8cartesian_nopy2(x):\n",
    "    \n",
    "    invmass = K.sqrt(\n",
    "                (x[:,0:1]+x[:,4:5])**2-\n",
    "                (x[:,1:2]+x[:,5:6])**2-\n",
    "                (x[:,2:3]         )**2-\n",
    "                (x[:,3:4]+x[:,6:7])**2\n",
    "                )\n",
    "    return invmass\n",
    "\n",
    "def lep1mass_from_8cartesian_nopy2(x):\n",
    "    \n",
    "    invmass = K.sqrt(\n",
    "                (x[:,0:1])**2-\n",
    "                (x[:,1:2])**2-\n",
    "                (x[:,2:3])**2-\n",
    "                (x[:,3:4])**2\n",
    "                )\n",
    "    return invmass\n",
    "\n",
    "def lep2mass_from_8cartesian_nopy2(x):\n",
    "    \n",
    "    invmass = K.sqrt(\n",
    "                (x[:,4:5])**2-\n",
    "                (x[:,5:6])**2-\n",
    "                (x[:,6:7])**2\n",
    "                )\n",
    "    return invmass\n",
    "\n",
    "\n",
    "# def getKS(real_data, predictions):\n",
    "#     return ks_2samp(real_data[\"mll\"], Minv(predictions))\n",
    "\n",
    "def getKS(real_data, predictions):\n",
    "    return {  \"mll\" : ks_2samp(real_data[\"mll\"], Minv(predictions))[0],\n",
    "         \"lep1_e\"   : ks_2samp(real_data[\"lep1_e\"], predictions[:,0])[0],\n",
    "         \"lep1_px\"  :  ks_2samp(real_data[\"lep1_px\"], predictions[:,1])[0],\n",
    "         \"lep1_py\"  :  ks_2samp(real_data[\"lep1_py\"], predictions[:,2])[0],\n",
    "         \"lep1_pz\"  :  ks_2samp(real_data[\"lep1_pz\"], predictions[:,3])[0],\n",
    "         \"lep2_e\"   :  ks_2samp(real_data[\"lep2_e\"], predictions[:,4])[0],\n",
    "         \"lep2_px\"  :  ks_2samp(real_data[\"lep2_px\"], predictions[:,5])[0],\n",
    "         \"lep2_pz\"  :  ks_2samp(real_data[\"lep2_pz\"], predictions[:,6])[0],\n",
    "         \"nvtxs\"    :  ks_2samp(real_data[\"nvtxs\"], np.rint(predictions[:,7]))[0],\n",
    "         \"lep1_iso\" :  ks_2samp(real_data[\"lep1_iso\"], predictions[:,8])[0],\n",
    "         \"lep2_iso\" :  ks_2samp(real_data[\"lep2_iso\"], predictions[:,9])[0],\n",
    "         \"metx\"     :  ks_2samp(real_data[\"met\"]*np.cos(real_data[\"metphi\"]), predictions[:,10])[0],\n",
    "         \"mety\"     :  ks_2samp(real_data[\"met\"]*np.sin(real_data[\"metphi\"]), predictions[:,11])[0],\n",
    "         \"met\"      :  ks_2samp(real_data[\"met\"], np.hypot(predictions[:,10],predictions[:,11]))[0],\n",
    "         \"metphi\"   :  ks_2samp(real_data[\"metphi\"], np.arctan2(predictions[:,11],predictions[:,10]))[0],\n",
    "         \"jet_pt1\"  :  ks_2samp(real_data[\"jet_pt1\"], predictions[:,12])[0],\n",
    "         \"jet_pt2\"  :  ks_2samp(real_data[\"jet_pt2\"], predictions[:,13])[0],\n",
    "         \"jet_pt3\"  :  ks_2samp(real_data[\"jet_pt3\"], predictions[:,14])[0],\n",
    "         \"jet_pt4\"  :  ks_2samp(real_data[\"jet_pt4\"], predictions[:,15])[0],\n",
    "         \"jet_pt5\"  :  ks_2samp(real_data[\"jet_pt5\"], predictions[:,16])[0] }\n",
    "\n",
    "def onetime(func):\n",
    "    \"\"\"stores the functions output, returns the output if called again on the same input, else computes new output\"\"\"\n",
    "    def decorated(*args, **kwargs):\n",
    "        global cov_ans\n",
    "        global cov_hash\n",
    "        new_hash=hashlib.md5(str(args)+str(kwargs)).hexdigest() \n",
    "        if new_hash != cov_hash:\n",
    "            #print(\"computing\")\n",
    "            cov_ans = func(*args, **kwargs)\n",
    "        cov_hash=new_hash\n",
    "        return cov_ans\n",
    "    return decorated\n",
    "    \n",
    "\n",
    "@onetime\n",
    "def covariance_metrics(real_data, predictions):\n",
    "    \"\"\"Takes in real_data matrix with real entries as rows and predictions matrix with generated events as rows and returns the covariance matricies for the two as well as the average, maximum, and std. dev of the difference between the entries in the coverance matrix as well as in the average of the variables.\"\"\"\n",
    "    \n",
    "    cov_pred = np.cov(predictions.T)\n",
    "    avg_pred = predictions.mean(axis=0)\n",
    "    cov_real = np.cov(real_data.T)\n",
    "    avg_real = real_data.mean(axis=0)\n",
    "    \n",
    "    #cov_diff = np.abs((cov_pred - cov_real)/np.sqrt(np.abs(np.outer(avg_real, avg_pred))))\n",
    "    cov_diff = np.abs((cov_pred - cov_real)/cov_real)\n",
    "    ar=avg_real\n",
    "    ar[ar == 0] = 1\n",
    "    avg_diff = np.abs((avg_pred - avg_real)/ar)\n",
    "    \n",
    "    return cov_diff, avg_diff\n",
    "\n",
    "def get_score(real_data, predictions, weight_cov = (1/361.), weight_avg = (1/19.)):\n",
    "    cov_diff, avg_diff = covariance_metrics(real_data, predictions)\n",
    "    return weight_cov*np.sum(cov_diff)+weight_avg*np.sum(avg_diff)\n",
    "\n",
    "# def get_first_N(x,N):\n",
    "#     return x[:,0:N]\n",
    "\n",
    "# def fix_outputs(x):\n",
    "#     \"\"\"\n",
    "#     Take nominal delphes format of 19 columns and fix some columns\n",
    "#     \"\"\"\n",
    "#     return K.concatenate([\n",
    "#         # x[:,0:21],\n",
    "#         x[:,0:7], # epxpypz for lep1,lep2 -1 for no py2\n",
    "#         x[:,7:8], # nvtx\n",
    "#         K.sign(x[:,8:10]), # q1 q2\n",
    "#         x[:,10:12], # iso1 iso2\n",
    "#         x[:,12:14], # met, metphi\n",
    "#         x[:,14:19], # jet pts\n",
    "#         ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_plots(preds,reals,title=\"\",fname=\"\",show_pred=True,show_real=True,wspace=0.1,hspace=0.3,tightlayout=True,visible=True):\n",
    "    nrows, ncols = 5,5\n",
    "    fig, axs = plt.subplots(nrows,ncols,figsize=(16,13))\n",
    "#     fig, axs = plt.subplots(nrows,ncols,figsize=(12,10))\n",
    "#     fig.subplots_adjust(wspace=0.1,hspace=0.3)\n",
    "    fig.subplots_adjust(wspace=wspace,hspace=hspace)\n",
    "\n",
    "\n",
    "    info = [\n",
    "        [\"lep1_e\",(0,250,50)],\n",
    "        [\"lep1_px\",(-100,100,50)],\n",
    "        [\"lep1_py\",(-100,100,50)],\n",
    "        [\"lep1_pz\",(-200,200,50)],\n",
    "        [\"lep2_e\",(0,250,50)],\n",
    "        [\"lep2_px\",(-100,100,50)],\n",
    "        [\"lep2_pz\",(-200,200,50)],\n",
    "        [\"nvtxs\",(0,50,350)],\n",
    "        [\"metx\",(-50,50,100)],\n",
    "        [\"mety\",(-50,50,100)],\n",
    "#         [\"lep1_charge\",(-7,7,30)],\n",
    "#         [\"lep2_charge\",(-7,7,30)],\n",
    "        [\"lep1_iso\",(0,2.0,30)],\n",
    "        [\"lep2_iso\",(0,2.0,30)],\n",
    "        [\"jet_pt1\",(0,100,50)],\n",
    "        [\"jet_pt2\",(0,100,50)],\n",
    "        [\"jet_pt3\",(0,100,50)],\n",
    "        [\"jet_pt4\",(0,100,50)],\n",
    "        [\"jet_pt5\",(0,100,50)],\n",
    "        # derived features\n",
    "        [\"dphi\",(-4,4,50)],\n",
    "        [\"met\",(0,150,50)],\n",
    "        [\"metphi\",(-6,6,50)],\n",
    "        [\"mll\",(60,120,50)],\n",
    "        [\"lep1_mass\",(0,10,50)],\n",
    "        [\"lep2_mass\",(0,10,50)],\n",
    "        [\"njets\",(0,7,7)],\n",
    "    ]\n",
    "    for axx in axs:\n",
    "        for ax in axx:\n",
    "            ax.get_yaxis().set_visible(False)\n",
    "            # turn off all axis borders, and turn them on below so they only show\n",
    "            # up for axes we've plotted in\n",
    "            ax.axis('off')\n",
    "    for ic,(cname,crange) in enumerate(info):\n",
    "        if cname == \"mll\":\n",
    "            real = reals[\"mll\"]\n",
    "            pred = Minv(preds)\n",
    "        elif cname == \"lep1_mass\": real, pred = M4(reals[\"lep1_e\"], reals[\"lep1_px\"], reals[\"lep1_py\"], reals[\"lep1_pz\"]), M4(preds[:,0], preds[:,1], preds[:,2], preds[:,3])\n",
    "        elif cname == \"lep2_mass\": real, pred = M4(reals[\"lep2_e\"], reals[\"lep2_px\"], 0, reals[\"lep2_pz\"]), M4(preds[:,4], preds[:,5], preds[:,6], preds[:,7])\n",
    "        elif cname == \"lep1_e\": real, pred = reals[cname], preds[:,0]\n",
    "        elif cname == \"lep1_pz\": real, pred = reals[cname], preds[:,3]\n",
    "        elif cname == \"lep2_e\": real, pred = reals[cname], preds[:,4]\n",
    "        elif cname == \"lep2_pz\": real, pred = reals[cname], preds[:,6]\n",
    "        elif cname == \"lep1_px\": \n",
    "            real = reals[cname]\n",
    "            pred = preds[:,1]\n",
    "        elif cname == \"lep1_py\":\n",
    "            real = reals[cname]\n",
    "            pred = preds[:,2]\n",
    "        elif cname == \"lep2_px\":\n",
    "            real = reals[cname]\n",
    "            pred = preds[:,5]\n",
    "        elif cname == \"dphi\":\n",
    "            real = get_dphi(reals[\"lep1_px\"], reals[\"lep1_py\"], reals[\"lep2_px\"], np.zeros(len(reals)))\n",
    "            pred = get_dphi(preds[:,1], preds[:,2], preds[:,5], np.zeros(len(preds)))\n",
    "        elif cname == \"nvtxs\": real, pred = reals[cname], np.round(preds[:,7])\n",
    "#         elif cname == \"lep1_charge\": real, pred = reals[cname], preds[:,8]\n",
    "#         elif cname == \"lep2_charge\": real, pred = reals[cname], preds[:,9]\n",
    "        elif cname == \"lep1_iso\": real, pred = reals[cname], preds[:,8]\n",
    "        elif cname == \"lep2_iso\": real, pred = reals[cname], preds[:,9]\n",
    "        elif cname == \"metx\": real, pred = reals[\"met\"]*np.cos(reals[\"metphi\"]), preds[:,10]\n",
    "        elif cname == \"mety\": real, pred = reals[\"met\"]*np.sin(reals[\"metphi\"]), preds[:,11]\n",
    "        elif cname == \"met\": real, pred = reals[\"met\"], np.hypot(preds[:,10],preds[:,11])\n",
    "        elif cname == \"metphi\": real, pred = reals[\"metphi\"], np.arctan2(preds[:,11],preds[:,10])\n",
    "        elif cname == \"jet_pt1\": real, pred = reals[cname], preds[:,12]\n",
    "        elif cname == \"jet_pt2\": real, pred = reals[cname], preds[:,13]\n",
    "        elif cname == \"jet_pt3\": real, pred = reals[cname], preds[:,14]\n",
    "        elif cname == \"jet_pt4\": real, pred = reals[cname], preds[:,15]\n",
    "        elif cname == \"jet_pt5\": real, pred = reals[cname], preds[:,16]\n",
    "        elif cname == \"njets\":\n",
    "            real = \\\n",
    "                1*(reals[\"jet_pt1\"] > 15) + \\\n",
    "                1*(reals[\"jet_pt2\"] > 15) + \\\n",
    "                1*(reals[\"jet_pt3\"] > 15) + \\\n",
    "                1*(reals[\"jet_pt4\"] > 15) + \\\n",
    "                1*(reals[\"jet_pt5\"] > 15)\n",
    "            pred = \\\n",
    "                1*(preds[:,12] > 15) + \\\n",
    "                1*(preds[:,13] > 15) + \\\n",
    "                1*(preds[:,14] > 15) + \\\n",
    "                1*(preds[:,15] > 15) + \\\n",
    "                1*(preds[:,16] > 15)\n",
    "        idx = ic // ncols, ic % ncols\n",
    "        if show_real:\n",
    "            bins_real = axs[idx].hist(real, range=crange[:2],bins=crange[-1], histtype=\"step\", lw=1.5,density=True)\n",
    "        if show_pred:\n",
    "            bins_pred = axs[idx].hist(pred, range=crange[:2],bins=crange[-1], histtype=\"step\", lw=1.5,density=True)\n",
    "        axs[idx].set_xlabel(\"{}\".format(cname),fontsize=14)\n",
    "        axs[idx].axis('on')\n",
    "        if cname in [\"mll\",\"lep1_mass\",\"lep2_mass\",\"dphi\",\"met\",\"metphi\",\"njets\"]:\n",
    "            axs[idx].xaxis.label.set_color('blue')\n",
    "    #     axs[idx].set_yscale(\"log\", nonposy='clip')\n",
    "    _ = axs[0,0].legend([\"True\",\"Pred\"], loc='upper right',fontsize=14)\n",
    "    _ = axs[0,0].set_title(title)\n",
    "    if tightlayout:\n",
    "        plt.tight_layout()\n",
    "    if fname:\n",
    "        fig.savefig(fname)\n",
    "    if not visible:\n",
    "        plt.close(fig)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recview(data, dtype=\"<f4\"):\n",
    "    \"\"\"\n",
    "    for example, can give it a matrix of values predicted by generator\n",
    "    and this gives a view of the same matrix with dtypes (for easier column selection)\n",
    "    if weird results, like 2 columns per field, use dtype=\"<f8\"\n",
    "    \"\"\"\n",
    "    cnames = [\n",
    "    \"lep1_e\",\n",
    "    \"lep1_px\",\n",
    "    \"lep1_py\",\n",
    "    \"lep1_pz\",\n",
    "    \"lep2_e\",\n",
    "    \"lep2_px\",\n",
    "    \"lep2_pz\",\n",
    "    \"nvtxs\",\n",
    "#     \"lep1_charge\",\n",
    "#     \"lep2_charge\",\n",
    "    \"lep1_iso\",\n",
    "    \"lep2_iso\",\n",
    "    \"metx\",\n",
    "    \"mety\",\n",
    "    \"jet_pt1\",\n",
    "    \"jet_pt2\",\n",
    "    \"jet_pt3\",\n",
    "    \"jet_pt4\",\n",
    "    \"jet_pt5\",\n",
    "    ]\n",
    "    cnames = [(cn,dtype) for cn in cnames]\n",
    "    return data.view(dtype=cnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN class\n",
    "Instantiate the GAN class, then add methods piece by piece (by making a \"new\" class inheriting from the original).\n",
    "I did this so that all these long functions could go into different cells, which makes it easier to navigate/read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN():\n",
    "    def __init__(self, **kwargs):\n",
    "\n",
    "        self.args = dict(kwargs)\n",
    "\n",
    "        self.verbose = kwargs.get(\"verbose\",True)\n",
    "        self.tag = kwargs[\"tag\"]\n",
    "        self.input_file = str(kwargs[\"input_file\"])\n",
    "        self.noise_shape = (int(kwargs[\"noise_size\"]),)\n",
    "        self.output_shape = (int(kwargs[\"output_size\"]),)\n",
    "        self.noise_type = int(kwargs[\"noise_type\"])\n",
    "        self.ntest_samples = int(kwargs[\"ntest_samples\"])\n",
    "        self.nepochs_dump_pred_metrics = int(kwargs[\"nepochs_dump_pred_metrics\"])\n",
    "        self.nepochs_dump_models = int(kwargs[\"nepochs_dump_models\"])\n",
    "        self.nepochs_dump_plots = int(kwargs[\"nepochs_dump_plots\"])\n",
    "        self.nepochs_max = int(kwargs[\"nepochs_max\"])\n",
    "        self.batch_size = int(kwargs[\"batch_size\"])\n",
    "        self.do_soft_labels = kwargs[\"do_soft_labels\"]\n",
    "        self.do_noisy_labels = kwargs[\"do_noisy_labels\"]\n",
    "        self.nepochs_decay_noisy_labels = int(kwargs[\"nepochs_decay_noisy_labels\"])\n",
    "        self.optimizer_gen = kwargs[\"optimizer_gen\"]\n",
    "        self.optimizer_disc = kwargs[\"optimizer_disc\"]\n",
    "        self.depth_disc = kwargs[\"depth_disc\"]\n",
    "        self.width_disc = kwargs[\"width_disc\"]\n",
    "        self.depth_gen = kwargs[\"depth_gen\"]\n",
    "        self.width_gen = kwargs[\"width_gen\"]\n",
    "        self.beefy_generator = kwargs[\"beefy_generator\"]\n",
    "        self.beefy_discriminator = kwargs[\"beefy_discriminator\"]\n",
    "#         self.fix_delphes_outputs = kwargs[\"fix_delphes_outputs\"]\n",
    "        self.use_mll_loss = kwargs[\"use_mll_loss\"]\n",
    "        self.loss_mll_weight = kwargs[\"loss_mll_weight\"]\n",
    "        self.terminate_early = kwargs[\"terminate_early\"]\n",
    "        self.loss_type = kwargs[\"loss_type\"]\n",
    "        self.dropout_discriminator = kwargs[\"dropout_discriminator\"]\n",
    "        self.frac_true = kwargs[\"frac_true\"]\n",
    "        self.outdir = kwargs[\"outdir\"]\n",
    "\n",
    "        self.scaler_type = kwargs[\"scaler_type\"]\n",
    "        self.scaler = None\n",
    "        self.jetscaler = None\n",
    "        self.isoscaler = None\n",
    "        \n",
    "        self.initialize()\n",
    "        \n",
    "    def initialize(self):\n",
    "        if self.scaler_type.lower() == \"minmax\":\n",
    "            self.scaler = MinMaxScaler(feature_range=(-1.,1.))\n",
    "        elif self.scaler_type.lower() == \"robust\":\n",
    "            self.scaler = RobustScaler()\n",
    "        elif self.scaler_type.lower() == \"standard\":\n",
    "            self.scaler = StandardScaler()\n",
    "        else:\n",
    "            if \"jet\" in self.scaler_type.lower():\n",
    "                print \"Scaling jet pts\"\n",
    "                #self.jetscaler = MinMaxScaler(feature_range=(-1.,1.))\n",
    "                self.jetscaler = RobustScaler()\n",
    "            if \"iso\" in self.scaler_type.lower():\n",
    "                print \"Scaling lep isos\"\n",
    "                #self.isoscaler = MinMaxScaler(feature_range=(-1.,1.))\n",
    "                self.isoscaler = RobustScaler()\n",
    "\n",
    "        self.data = None\n",
    "        self.data_ref = None\n",
    "        self.d_epochinfo = {}\n",
    "        self.X_train = None\n",
    "\n",
    "        optimizer_d = self.optimizer_disc\n",
    "        optimizer_g = self.optimizer_gen\n",
    "        \n",
    "        def dosplit(x):\n",
    "            return float(x.split(\"=\")[1].split(\")\")[0].strip())\n",
    "        if \"lr\" in self.optimizer_disc: optimizer_d = Adadelta(lr=dosplit(self.optimizer_disc))\n",
    "        if \"lr\" in self.optimizer_gen: optimizer_g = Adadelta(lr=dosplit(self.optimizer_gen))\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        if self.use_mll_loss:\n",
    "            loss = self.custom_loss(c=self.loss_mll_weight, loss_type=self.loss_type)\n",
    "        else:\n",
    "            loss = \"binary_crossentropy\"\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss=self.custom_loss(0, \"disc\"),\n",
    "            optimizer=optimizer_d,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build and compile the generator\n",
    "        self.generator = self.build_generator()\n",
    "        self.generator.compile(loss=loss, optimizer=optimizer_g)\n",
    "\n",
    "        # The generator takes noise as input and generated imgs\n",
    "        z = Input(shape=self.noise_shape)\n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The valid takes generated images as input and determines validity\n",
    "        valid = self.discriminator(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator) takes\n",
    "        # noise as input => generates images => determines validity\n",
    "        self.combined = Model(z, valid)\n",
    "        self.combined.compile(loss=loss, optimizer=optimizer_g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(GAN):\n",
    "    \n",
    "    def custom_loss(self, c, loss_type = \"force_mll\"):\n",
    "        mu_z, sig_z = 89.6, 7.73\n",
    "        mu_l, sig_l = 0.105, 0.019\n",
    "        if loss_type == \"disc\":\n",
    "            def loss_func(y_true, y_pred_mll):\n",
    "                y_true = y_true[:,0]\n",
    "                y_pred = y_pred_mll[:,0]\n",
    "                return binary_crossentropy(y_true, y_pred)\n",
    "            return loss_func\n",
    "        elif loss_type == \"force_mll\":\n",
    "            def loss_func(y_true, y_pred_mll):\n",
    "                y_true = y_true[:,0]\n",
    "                y_pred = y_pred_mll[:,0]\n",
    "                mll_pred = y_pred_mll[:,1]\n",
    "                mll_loss = K.mean(K.abs(mll_pred - mu_z))\n",
    "                return binary_crossentropy(y_true, y_pred) + c*mll_loss\n",
    "            return loss_func\n",
    "        elif loss_type == \"force_z_width\":\n",
    "            def loss_func(y_true, y_pred_mll):\n",
    "                y_true = y_true[:,0]\n",
    "                y_pred = y_pred_mll[:,0]\n",
    "                mll_pred = y_pred_mll[:,1]\n",
    "                mll_loss = K.mean((mll_pred - mu_z)**2)\n",
    "                mll_sigma_loss = (K.std(mll_pred)-sig_z)**2\n",
    "\n",
    "                return binary_crossentropy(y_true, y_pred) + c*mll_loss + c*mll_sigma_loss\n",
    "            return loss_func\n",
    "        elif loss_type == \"lepmass\":\n",
    "            def loss_func(y_true, y_pred_mll):\n",
    "                y_true = y_true[:,0]\n",
    "                y_pred = y_pred_mll[:,0]\n",
    "                \n",
    "                mll_pred = y_pred_mll[:,1]\n",
    "                l1m_pred = y_pred_mll[:,2]\n",
    "                l2m_pred = y_pred_mll[:,3]\n",
    "                \n",
    "                mll_loss = K.mean((mll_pred - mu_z)**2)\n",
    "                mll_sigma_loss = (K.std(mll_pred)-sig_z)**2\n",
    "                l1_loss = K.mean((l1m_pred - mu_l)**2)\n",
    "                l1_sigma_loss = (K.std(l1m_pred)-sig_l)**2\n",
    "                l2_loss = K.mean((l2m_pred - mu_l)**2)\n",
    "                l2_sigma_loss = (K.std(l2m_pred)-sig_l)**2\n",
    "\n",
    "                return binary_crossentropy(y_true, y_pred) + c*mll_loss + c*mll_sigma_loss + c*c*K.log(l1_loss) + c*c*K.log(l1_sigma_loss) + c*c*K.log(l2_loss) + c*c*K.log(l2_sigma_loss)\n",
    "            return loss_func\n",
    "        else:\n",
    "            raise ValueError(\"Can not make loss function of type %s\" % loss_type)\n",
    "        \n",
    "    def build_generator(self):\n",
    "\n",
    "        inputs = Input(shape=self.noise_shape)\n",
    "\n",
    "        ## Head\n",
    "        x = Dense(64)(inputs)\n",
    "        x = LeakyReLU(alpha=0.2)(x)\n",
    "        if self.depth_gen > 0 and self.width_gen > 0:\n",
    "            for level in xrange(0,self.depth_gen):\n",
    "                x = Dense(width_gen/(2**level))(x) #Triangle with width halved at each level\n",
    "                x = LeakyReLU(alpha=0.2)(x)\n",
    "        elif self.beefy_generator:\n",
    "            for size in [128,256,512,256,128]:\n",
    "                x = Dense(size)(x)\n",
    "                x = LeakyReLU(alpha=0.2)(x)\n",
    "        else:\n",
    "            for size in [128,128,128,64,32]:\n",
    "                x = Dense(size)(x)\n",
    "                x = LeakyReLU(alpha=0.2)(x)\n",
    " \n",
    "    \n",
    "        x = Dense(self.output_shape[0], activation=\"linear\")(x)\n",
    "            \n",
    "#         if self.fix_delphes_outputs:\n",
    "#             x = Lambda(fix_outputs,\n",
    "#                 input_shape=self.output_shape,\n",
    "#                 output_shape=self.output_shape\n",
    "#                 )(x)\n",
    "            \n",
    "        model = Model(inputs=inputs, outputs=[x])\n",
    "        \n",
    "        print \"Generator params: {}\".format(model.count_params())\n",
    "        if self.verbose:\n",
    "            model.summary()\n",
    "        \n",
    "        return model\n",
    "    \n",
    "class GAN(GAN):\n",
    "    \n",
    "    def build_discriminator(self):\n",
    "\n",
    "        inputs = Input(self.output_shape)\n",
    "        mll = Lambda(invmass_from_8cartesian_nopy2)(inputs)\n",
    "        lep1m = Lambda(lep1mass_from_8cartesian_nopy2)(inputs)\n",
    "        lep2m = Lambda(lep2mass_from_8cartesian_nopy2)(inputs)\n",
    "        x = Dense(128)(inputs)\n",
    "        x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "        ## Main Body\n",
    "        if self.depth_disc > 0 and self.width_disc > 0:\n",
    "            for level in xrange(0,self.depth_disc):\n",
    "                x = Dense(self.width_disc/(2**level))(x) #Triangle with width halved at each level\n",
    "                x = LeakyReLU(alpha=0.2)(x)\n",
    "        elif self.beefy_discriminator:\n",
    "            for size in [128,256,256,128,64,32,16,8]:\n",
    "                x = Dense(size)(x)\n",
    "                if self.dropout_discriminator:\n",
    "                    x = Dropout(0.1)(x)\n",
    "                x = LeakyReLU(alpha=0.2)(x)\n",
    "        else:\n",
    "            for size in [128]*5 + [64,32,16,8]:\n",
    "                x = Dense(size)(x)\n",
    "                x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "        ## Tail\n",
    "        out = Dense(1,activation='sigmoid')(x)\n",
    "        \n",
    "        if self.use_mll_loss:\n",
    "            if self.loss_type == \"lepmass\":\n",
    "                model = Model(inputs=inputs, outputs=concatenate([out,mll,lep1m,lep2m]))\n",
    "            else:\n",
    "                model = Model(inputs=inputs, outputs=concatenate([out,mll]))\n",
    "        else:\n",
    "            model = Model(inputs=inputs, outputs=out)\n",
    "#         print model.output_shape\n",
    "        if self.verbose:\n",
    "            model.summary()\n",
    "        print \"Discriminator params: {}\".format(model.count_params())\n",
    "        \n",
    "        return model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(GAN):\n",
    "    \n",
    "    def load_data(self):\n",
    "        if self.data is not None: return\n",
    "        \n",
    "        self.data = np.load(self.input_file)\n",
    "            \n",
    "        # make sure we drop low mass resonances\n",
    "        self.data = self.data[self.data[\"genmll\"] > 50.]\n",
    "        \n",
    "    def make_flat_array(self):\n",
    "        \"\"\"Builds X_train array which is a flat version of the self.data that has any scaling or modifications applied\"\"\"\n",
    "        \n",
    "        lepcoords = np.c_[\n",
    "            self.data[\"lep1_e\"],\n",
    "            self.data[\"lep1_px\"],\n",
    "            self.data[\"lep1_py\"],\n",
    "            self.data[\"lep1_pz\"],\n",
    "            self.data[\"lep2_e\"],\n",
    "            self.data[\"lep2_px\"],\n",
    "            self.data[\"lep2_pz\"],\n",
    "        ]\n",
    "\n",
    "        nvtx_smeared = np.round(np.random.normal(self.data[\"nvtxs\"],0.5))\n",
    "        \n",
    "        isocoords = np.c_[self.data[\"lep1_iso\"], \n",
    "                          self.data[\"lep2_iso\"]]\n",
    "        if self.isoscaler:\n",
    "            print \"scaling lepton isolations\"\n",
    "            self.isoscaler.fit(isocoords)\n",
    "            isocoords = self.isoscaler.transform(isocoords).astype(np.float32)\n",
    "            pickle.dump(self.isoscaler, open(\"{}/progress/{}/isoscaler.pkl\".format(self.outdir,self.tag),'w'))\n",
    "        \n",
    "        jetcoords = np.c_[self.data[\"jet_pt1\"],\n",
    "                          self.data[\"jet_pt2\"],\n",
    "                          self.data[\"jet_pt3\"],\n",
    "                          self.data[\"jet_pt4\"],\n",
    "                          self.data[\"jet_pt5\"]]\n",
    "        \n",
    "        if self.jetscaler:\n",
    "            print \"scaling jet pts\"\n",
    "            self.jetscaler.fit(jetcoords)\n",
    "            jetcoords = self.jetscaler.transform(jetcoords).astype(np.float32)\n",
    "            pickle.dump(self.jetscaler, open(\"{}/progress/{}/jetscaler.pkl\".format(self.outdir,self.tag),'w'))\n",
    "        \n",
    "        self.X_train = np.c_[\n",
    "            lepcoords, # 7 columns\n",
    "            nvtx_smeared, # 1 column\n",
    "            isocoords,\n",
    "            self.data[\"met\"]*np.cos(self.data[\"metphi\"]), # metx\n",
    "            self.data[\"met\"]*np.sin(self.data[\"metphi\"]), # mety\n",
    "            jetcoords\n",
    "        ].astype(np.float32)\n",
    "\n",
    "        # # NOTE. StandardScaler should be fit on training set\n",
    "        # # and applied the same to train and test, otherwise we\n",
    "        # # introduce a bias\n",
    "        if self.scaler:\n",
    "            self.scaler.fit(self.X_train)\n",
    "            self.X_train = self.scaler.transform(self.X_train).astype(np.float32)\n",
    "            pickle.dump(self.scaler, open(\"{}/progress/{}/scaler.pkl\".format(self.outdir,self.tag),'w'))\n",
    "        \n",
    "\n",
    "    def get_noise(self, amount=1024, max_true_samples=-1, max_true_samples_frac=-1):\n",
    "        \"\"\"\n",
    "        `amount` specifies number of noise vectors\n",
    "        `max_true_samples` applies only to truth conditioned noise type\n",
    "            if > 0, then the true samples are sampled from the first \n",
    "            `max_true_samples` of real events (by default, all are allowed)\n",
    "        `max_true_samples_frac` same desc as `max_true_samples`, but specified\n",
    "            instead as fraction of `amount`\n",
    "        \"\"\"\n",
    "        # nominal\n",
    "        if self.noise_type == 1:\n",
    "            noise_half = np.random.normal(0, 1, (amount//2, self.noise_shape[0]))\n",
    "            noise_full = np.random.normal(0, 1, (amount, self.noise_shape[0]))\n",
    "\n",
    "        elif self.noise_type == 2: # random soup, 4,2,2 have to be modified to sum to noise_shape[0]\n",
    "            ngaus = self.noise_shape[0] // 2\n",
    "            nflat = (self.noise_shape[0] - ngaus) // 2\n",
    "            nexpo = self.noise_shape[0] - nflat - ngaus\n",
    "            noise_gaus = np.random.normal( 0, 1, (amount//2+amount, ngaus))\n",
    "            noise_flat = np.random.uniform(-1, 1, (amount//2+amount, nflat))\n",
    "            noise_expo = np.random.exponential( 1,    (amount//2+amount, nexpo))\n",
    "            noise = np.c_[ noise_gaus,noise_flat,noise_expo ]\n",
    "            noise_half = noise[:amount//2]\n",
    "            noise_full = noise[-amount:]\n",
    "        elif self.noise_type == 3: #Flat noise between 0-1, last 4 units are flipped negative\n",
    "            noise_half = np.random.uniform(0, 1, (amount//2, self.noise_shape[0]))\n",
    "            noise_half[:,-4:] *= -1\n",
    "            noise_full = np.random.uniform(0, 1, (amount, self.noise_shape[0]))\n",
    "            noise_full[:,-4:] *= -1\n",
    "            \n",
    "            \n",
    "        return noise_half, noise_full\n",
    "    \n",
    "    def getTrialNums(self):\n",
    "        \"\"\"Looks for directories with the same tag but different _num at the end, returns a list of the nums\"\"\"\n",
    "        tag_nonum = self.tag[:self.tag.rfind('_')]\n",
    "        dirs = []\n",
    "        dirnums = []\n",
    "        for d in os.listdir(\"%s/progress/\" % (self.outdir)):\n",
    "            if tag_nonum in d:\n",
    "                dirnums.append(int(d[d.rfind('_')+1:]))\n",
    "        return dirnums\n",
    "    \n",
    "    def get_available_checkpoints(self, trial=None):\n",
    "        # check the output folder for all weights files and return list of epochs for existing files\n",
    "        if self.tag[-1].isdigit():\n",
    "            tag_regex = self.tag[:self.tag.rfind('_')]+\"*\"\n",
    "            fnames = glob.glob(\"{}/progress/{}/gen_*.weights\".format(self.outdir,tag_regex))\n",
    "        elif trial:\n",
    "            tag_trail = self.tag[:self.tag.rfind('_')]+\"_%d\" % trial\n",
    "            fnames = glob.glob(\"{}/progress/{}/gen_*.weights\".format(self.outdir,tag_trail))\n",
    "        else:\n",
    "            fnames = glob.glob(\"{}/progress/{}/gen_*.weights\".format(self.outdir,self.tag))\n",
    "        return np.array(sorted(map(lambda x: int(x.rsplit(\"_\",1)[1].split(\".\")[0]), fnames)))\n",
    "\n",
    "    def load_checkpoint(self, epoch, trial=None):\n",
    "        \"\"\"given an epoch number, load the disc/gen files, overriding self.discriminator/self.generator\n",
    "        need to give custom_objects to load_model because keras doesn't know what the loss function is otherwise\"\"\"\n",
    "        custom = {\"loss_func\": gan.custom_loss(c=self.loss_mll_weight,loss_type=self.loss_type)}\n",
    "        # make sure data is loaded if we just want to make a gan, load a checkpoint, and predict + pull real samples\n",
    "        self.load_data()\n",
    "        \n",
    "        if trial:\n",
    "            tag = self.tag[:self.tag.rfind('_')]+\"_%d\" % trial\n",
    "            self.tag=tag\n",
    "        else:\n",
    "            tag = self.tag\n",
    "            \n",
    "        self.discriminator = load_model(\"{}/progress/{}/disc_{}.weights\".format(self.outdir,tag,epoch),custom_objects=custom)\n",
    "        self.generator = load_model(\"{}/progress/{}/gen_{}.weights\".format(self.outdir,tag,epoch),custom_objects=custom)\n",
    "        self.d_epochinfo = pickle.load(open(\"{}/progress/{}/history.pkl\".format(self.outdir,tag),'r'))\n",
    "\n",
    "        \n",
    "    def load_last_checkpoint(self,which=-1):\n",
    "        # convenience function to get last available checkpoint and load it (or `which`th from last)\n",
    "        lastepoch = self.get_available_checkpoints()[which]\n",
    "        print \"Loading last checkpoint for tag {}: epoch {}\".format(self.tag, lastepoch)\n",
    "        self.load_checkpoint(lastepoch)\n",
    "        \n",
    "    def predict(self, N, frac):\n",
    "        \"\"\"Gets prediction from trained generator and undoes any scaling.\"\"\"\n",
    "        _, noise = self.get_noise(N,max_true_samples_frac=frac)\n",
    "        # print preds.shape\n",
    "        preds = self.generator.predict(noise,verbose=1)\n",
    "        \n",
    "        if self.scaler:\n",
    "            preds = self.scaler.inverse_transform(preds)\n",
    "            return preds\n",
    "        \n",
    "        lepcoords = preds[:,0:7]\n",
    "        nvtx = preds[:,7]\n",
    "        isocoords = preds[:,8:10]\n",
    "        metcoords = preds[:,10:12]\n",
    "        jetcoords = preds[:,12:17]\n",
    "        if self.isoscaler:\n",
    "            isocoords = self.isoscaler.inverse_transform(isocoords)\n",
    "        if self.jetscaler:\n",
    "            jetcoords = self.jetscaler.inverse_transform(jetcoords)\n",
    "            \n",
    "        return  np.c_[\n",
    "                lepcoords, # 7 columns\n",
    "                nvtx, # 1 column\n",
    "                isocoords,\n",
    "                metcoords,\n",
    "                jetcoords\n",
    "            ].astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Training Metrics\n",
    "\n",
    "In this block we impliment running through the batch training metrics for all the models in the current tag and finding the best models based on those metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(GAN):\n",
    "    \n",
    "    def findBest(self, metrics=[\"mll\", \"metphi\", \"lep1_iso\"]):\n",
    "        trial_info = []\n",
    "        tag_nonum = self.tag[:self.tag.rfind('_')]\n",
    "        for i in self.getTrialNums():\n",
    "            trial_log = pickle.load( open(\"%s/progress/%s_%d/history.pkl\" % (self.outdir, tag_nonum, i)) )\n",
    "            trial_info += zip(np.zeros(len(trial_log[\"epoch\"]))+i, trial_log[\"epoch\"], trial_log[\"ks\"])\n",
    "\n",
    "        #print trial_info   \n",
    "\n",
    "        sorted_trials = {}\n",
    "\n",
    "        #Get dict of epoch lists, each list is sorted by the metric 'm'\n",
    "        for m in metrics:\n",
    "            sorted_trials[m] = list(enumerate(np.array(sorted(trial_info, key = lambda x: 2 if math.isnan(x[2][m]) else x[2][m]))[:,0:2]))\n",
    "\n",
    "        #print sorted_trials[\"g_loss\"]\n",
    "\n",
    "        epoch_score = {}\n",
    "        for row in sorted_trials[metrics[0]]:\n",
    "            epoch_score[\"%d_%d\" % (row[1][0], row[1][1])] = [row[0], {metrics[0]: row[0]}]\n",
    "\n",
    "        for m in metrics[1:]:\n",
    "            for row in sorted_trials[m]:\n",
    "                epoch_score[\"%d_%d\" % (row[1][0], row[1][1])][0] += row[0]\n",
    "                epoch_score[\"%d_%d\" % (row[1][0], row[1][1])][1][m] = row[0]\n",
    "\n",
    "        return sorted(epoch_score.items(), key=lambda k: k[1][0])\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(GAN):\n",
    "            \n",
    "    def train(self, trial=1):\n",
    "\n",
    "        self.load_data()\n",
    "        self.make_flat_array()\n",
    "        \n",
    "        # make an alias to save typing\n",
    "        X_train = self.X_train\n",
    "        \n",
    "        half_batch = int(self.batch_size / 2)\n",
    "\n",
    "        prev_gen_loss = -1\n",
    "        prev_disc_loss = -1\n",
    "        n_loss_same_gen = 0  # number of epochs for which generator loss has remained ~same (within 0.01%)\n",
    "        n_loss_same_disc = 0  # number of epochs for which discriminator loss has remained ~same (within 0.01%)\n",
    "        old_info = -1, -1\n",
    "        ks_score = 999.\n",
    "        best_ks_score = 999.\n",
    "        \n",
    "        os.system(\"mkdir -p {}/progress/{}/\".format(self.outdir,self.tag))\n",
    "        os.system(\"cp gan_reco_Sep12_bobak.ipynb {}/progress/{}/\".format(self.outdir,self.tag))\n",
    "        logfile = open(\"{}/progress/{}/log.txt\".format(self.outdir,self.tag),'w+')\n",
    "        logfile.write(\"Getting Started! Copied notebook into progress directory.\\n\")\n",
    "        \n",
    "        for epoch in range(self.nepochs_max):\n",
    "\n",
    "            if self.terminate_early:\n",
    "                if n_loss_same_gen > 1000 or n_loss_same_disc > 1000:\n",
    "                    print \"BREAKING because disc/gen loss has remained the same for {}/{} epochs!\".format(n_loss_same_disc,n_loss_same_gen)\n",
    "                    break\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random half batch of images\n",
    "            idx = np.random.randint(0, X_train.shape[0], half_batch)\n",
    "            imgs = X_train[idx]\n",
    "            \n",
    "            noise_half, noise_full = self.get_noise(self.batch_size, max_true_samples_frac=self.frac_true)\n",
    "            \n",
    "            # Generate a half batch of new images\n",
    "            gen_imgs = self.generator.predict(noise_half)\n",
    "\n",
    "            # Train the discriminator\n",
    "            ones = np.ones((half_batch, 1))\n",
    "            zeros = np.zeros((half_batch, 1))\n",
    "\n",
    "            if self.do_soft_labels:\n",
    "                ones *= 0.9\n",
    "\n",
    "            if self.do_noisy_labels:\n",
    "                frac = 0.3*np.exp(-epoch/self.nepochs_decay_noisy_labels)\n",
    "                if frac > 0.005:\n",
    "                    ones[np.random.randint(0, len(ones), int(frac*len(ones)))] = 0\n",
    "                    zeros[np.random.randint(0, len(zeros), int(frac*len(zeros)))] = 1\n",
    "\n",
    "            d_loss = self.discriminator.train_on_batch(np.concatenate([imgs, gen_imgs[:half_batch]]), np.concatenate([ones, zeros]))\n",
    "            \n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "            # The generator wants the discriminator to label the generated samples\n",
    "            # as valid (ones)\n",
    "            valid_y = np.array([1] * self.batch_size)\n",
    "\n",
    "            # Train the generator\n",
    "            g_loss = self.combined.train_on_batch(noise_full, valid_y)\n",
    "\n",
    "            if (g_loss - prev_gen_loss) < 0.0001: n_loss_same_gen += 1\n",
    "            else: n_loss_same_gen = 0\n",
    "            prev_gen_loss = g_loss\n",
    "\n",
    "            if (d_loss[0] - prev_disc_loss) < 0.0001: n_loss_same_disc += 1\n",
    "            else: n_loss_same_disc = 0\n",
    "            prev_disc_loss = d_loss[0]\n",
    "\n",
    "            # Plot the progress\n",
    "#             print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "            sys.stdout.write(\"\\r{} [D loss: {}, acc.: {:.2f}%] [G loss: {}] [mll={:.3f}+-{:.3f}] [ks={:.3f}]\".format(epoch, d_loss[0], 100.0*d_loss[1], g_loss, old_info[0], old_info[1],ks_score))\n",
    "\n",
    "            if epoch % self.nepochs_dump_pred_metrics == 0 and epoch > 0:\n",
    "            \n",
    "                _, noise_test = self.get_noise(self.ntest_samples,  max_true_samples_frac=self.frac_true)\n",
    "            \n",
    "                sys.stdout.write(\"\\n\") # break up the stream of text\n",
    "\n",
    "                gen_imgs = self.generator.predict(noise_test)\n",
    "\n",
    "                if self.scaler:\n",
    "                    gen_imgs = self.scaler.inverse_transform(gen_imgs)\n",
    "\n",
    "                masses = Minv(gen_imgs)\n",
    "                masses = masses[np.isfinite(masses)]\n",
    "                old_info = masses.mean(), masses.std()\n",
    "                ks_scores = getKS(self.data[:15000],gen_imgs)\n",
    "                ks_scores[\"stat_score\"] = get_score(X_train, gen_imgs)\n",
    "                ks_score = ks_scores[\"mll\"]\n",
    "\n",
    "                if \"epoch\" not in self.d_epochinfo:\n",
    "                    self.d_epochinfo[\"epoch\"] = []\n",
    "                    self.d_epochinfo[\"d_acc\"] = []\n",
    "                    self.d_epochinfo[\"d_loss\"] = []\n",
    "                    self.d_epochinfo[\"g_loss\"] = []\n",
    "                    self.d_epochinfo[\"mass_mu\"] = []\n",
    "                    self.d_epochinfo[\"mass_sig\"] = []\n",
    "                    self.d_epochinfo[\"ks\"] = []\n",
    "                    self.d_epochinfo[\"time\"] = []\n",
    "                    self.d_epochinfo[\"args\"] = self.args\n",
    "                else:\n",
    "                    self.d_epochinfo[\"epoch\"].append(epoch)\n",
    "                    self.d_epochinfo[\"d_acc\"].append(100*d_loss[1])\n",
    "                    self.d_epochinfo[\"d_loss\"].append(d_loss[0])\n",
    "                    self.d_epochinfo[\"g_loss\"].append(g_loss)\n",
    "                    self.d_epochinfo[\"mass_mu\"].append(masses.mean())\n",
    "                    self.d_epochinfo[\"mass_sig\"].append(masses.std())\n",
    "                    self.d_epochinfo[\"ks\"].append(ks_scores)\n",
    "                    self.d_epochinfo[\"time\"].append(time.time())\n",
    "\n",
    "                pickle.dump(self.d_epochinfo, open(\"{}/progress/{}/history.pkl\".format(self.outdir,self.tag),'w'))\n",
    "\n",
    "                # note, nested within nepochs_dump_pred_metrics, so below value must be multiple of that\n",
    "                if epoch % self.nepochs_dump_plots == 0 and epoch > 0:\n",
    "                    preds = self.predict(self.ntest_samples,self.frac_true)\n",
    "                    reals = self.data[:15000]\n",
    "                    _ = make_plots(preds,reals,title=\"{}: epoch {}\".format(self.tag,epoch),\n",
    "                                   fname=\"{}/progress/{}/plots_{:06d}.png\".format(self.outdir,self.tag,epoch),visible=False)\n",
    "\n",
    "                # note, nested within nepochs_dump_pred_metrics, so below value must be multiple of that\n",
    "                if epoch % self.nepochs_dump_models == 0 and epoch > 0:                    \n",
    "                    dfname = \"{}/progress/{}/disc_{}.weights\".format(self.outdir,self.tag,epoch)\n",
    "                    gfname = \"{}/progress/{}/gen_{}.weights\".format(self.outdir,self.tag,epoch)\n",
    "                    self.discriminator.save(dfname)\n",
    "                    self.generator.save(gfname)\n",
    "                    mll_ks_score = ks_scores[\"mll\"]\n",
    "                    MetPhi_ks_score = ks_scores[\"metphi\"]\n",
    "                    Lep1Iso_ks_score = ks_scores[\"lep1_iso\"]\n",
    "                    logfile.write(\"epoch %d trial %s StatsScore %f MLLKSStatistic %f METPhiKSStatistic %f Lep1IsoKSStatistic %f \\n\" % (epoch, trial, ks_scores[\"stat_score\"], ks_scores[\"mll\"], ks_scores[\"metphi\"], ks_scores[\"lep1_iso\"]))\n",
    "                    logfile.write(\"\\r{} [D loss: {}, acc.: {:.2f}%] [G loss: {}] [mll={:.3f}+-{:.3f}]\\n\".format(epoch, d_loss[0], 100.0*d_loss[1], g_loss, old_info[0], old_info[1]))\n",
    "        \n",
    "        logfile.close()\n",
    "    \n",
    "    def batchTrain(self, tag, ntrials=10):\n",
    "        \"\"\"Trains the same config ntrials times, does book-keeping for you\"\"\"\n",
    "        trial = 1\n",
    "        for i in range(1,ntrials):\n",
    "            tag+=\"_%d\" % i\n",
    "            self.tag=tag\n",
    "            self.initialize()\n",
    "            try:\n",
    "                self.train(i)\n",
    "            except Exception as e:\n",
    "                print(\"Encountered Error %s\" % e)\n",
    "                continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set parameters and instantiate\n",
    "Parameters, plots, epoch metrics, get saved to folder `progress/<tag>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'do_skip_connection': False, 'width_disc': 0, 'ntest_samples': 10000, 'optimizer_disc': 'adadelta', 'nepochs_dump_models': 100, 'loss_type': 'lepmass', 'input_file': '/home/users/bhashemi/Projects/GIT/DY-GAN/delphes/total_Zmumu_13TeV_PU20_v2.npa', 'dropout_discriminator': False, 'terminate_early': True, 'do_batch_normalization_disc': False, 'scaler_type': 'None', 'nepochs_dump_plots': 500, 'batch_size': 512, 'do_concatenate_disc': False, 'do_noisy_labels': False, 'do_soft_labels': False, 'depth_gen': 0, 'nepochs_dump_pred_metrics': 100, 'verbose': True, 'noise_size': 17, 'do_batch_normalization_gen': False, 'outdir': '/nfs-7/userdata/bhashemi/DY-GAN/models', 'output_size': 17, 'frac_true': -1, 'loss_mll_weight': 0.0001, 'nepochs_max': 40001, 'beefy_discriminator': True, 'width_gen': 0, 'depth_disc': 0, 'do_tanh_gen': False, 'do_concatenate_gen': False, 'use_mll_loss': True, 'beefy_generator': True, 'nepochs_decay_noisy_labels': 1000, 'optimizer_gen': 'adadelta', 'noise_type': 3}\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_50 (InputLayer)           (None, 17)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_296 (Dense)               (None, 128)          2304        input_50[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_262 (LeakyReLU)     (None, 128)          0           dense_296[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_297 (Dense)               (None, 128)          16512       leaky_re_lu_262[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_263 (LeakyReLU)     (None, 128)          0           dense_297[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_298 (Dense)               (None, 256)          33024       leaky_re_lu_263[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_264 (LeakyReLU)     (None, 256)          0           dense_298[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_299 (Dense)               (None, 256)          65792       leaky_re_lu_264[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_265 (LeakyReLU)     (None, 256)          0           dense_299[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_300 (Dense)               (None, 128)          32896       leaky_re_lu_265[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_266 (LeakyReLU)     (None, 128)          0           dense_300[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_301 (Dense)               (None, 64)           8256        leaky_re_lu_266[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_267 (LeakyReLU)     (None, 64)           0           dense_301[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_302 (Dense)               (None, 32)           2080        leaky_re_lu_267[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_268 (LeakyReLU)     (None, 32)           0           dense_302[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_303 (Dense)               (None, 16)           528         leaky_re_lu_268[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_269 (LeakyReLU)     (None, 16)           0           dense_303[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_304 (Dense)               (None, 8)            136         leaky_re_lu_269[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_270 (LeakyReLU)     (None, 8)            0           dense_304[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_305 (Dense)               (None, 1)            9           leaky_re_lu_270[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_58 (Lambda)              (None, 1)            0           input_50[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_59 (Lambda)              (None, 1)            0           input_50[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_60 (Lambda)              (None, 1)            0           input_50[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_20 (Concatenate)    (None, 4)            0           dense_305[0][0]                  \n",
      "                                                                 lambda_58[0][0]                  \n",
      "                                                                 lambda_59[0][0]                  \n",
      "                                                                 lambda_60[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 161,537\n",
      "Trainable params: 161,537\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Discriminator params: 161537\n",
      "Generator params: 340497\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_51 (InputLayer)        (None, 17)                0         \n",
      "_________________________________________________________________\n",
      "dense_306 (Dense)            (None, 64)                1152      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_271 (LeakyReLU)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_307 (Dense)            (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_272 (LeakyReLU)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_308 (Dense)            (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_273 (LeakyReLU)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_309 (Dense)            (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_274 (LeakyReLU)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_310 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_275 (LeakyReLU)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_311 (Dense)            (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_276 (LeakyReLU)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_312 (Dense)            (None, 17)                2193      \n",
      "=================================================================\n",
      "Total params: 340,497\n",
      "Trainable params: 340,497\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# defaults\n",
    "params = {\n",
    "        \"batch_size\": 512,\n",
    "        \"beefy_discriminator\": True,\n",
    "        \"beefy_generator\": True,\n",
    "        \"depth_disc\": 0,\n",
    "        \"depth_gen\": 0,\n",
    "        \"do_batch_normalization_disc\": False,\n",
    "        \"do_batch_normalization_gen\": False,\n",
    "        \"do_concatenate_disc\": False,\n",
    "        \"do_concatenate_gen\": False,\n",
    "        \"do_noisy_labels\": False,\n",
    "        \"do_soft_labels\": False,\n",
    "#         \"do_noisy_labels\": True,\n",
    "#         \"do_soft_labels\": True,\n",
    "        \"do_skip_connection\": False,\n",
    "        \"do_tanh_gen\": False,\n",
    "        \"dropout_discriminator\": False,\n",
    "        \"frac_true\": -1,\n",
    "        \"input_file\": \"/home/users/bhashemi/Projects/GIT/DY-GAN/delphes/total_Zmumu_13TeV_PU20_v2.npa\",\n",
    "        \"loss_mll_weight\": 0.0001,\n",
    "#         \"loss_type\": \"force_mll\",\n",
    "        #\"loss_type\": \"force_z_width\",\n",
    "        \"loss_type\": \"lepmass\",\n",
    "        \"nepochs_decay_noisy_labels\": 1000,\n",
    "#         \"nepochs_dump_models\": 9999999, # can't dump models? FIXME\n",
    "        \"nepochs_dump_models\": 100, # can't dump models? FIXME\n",
    "        \"nepochs_dump_plots\": 500,\n",
    "        \"nepochs_dump_pred_metrics\": 100,\n",
    "        #\"nepochs_max\": 100001,\n",
    "        \"nepochs_max\": 40001,\n",
    "        \"noise_size\": 17,\n",
    "        \"noise_type\": 3,\n",
    "        \"ntest_samples\": 10000,\n",
    "        \"optimizer_disc\": \"adadelta\",\n",
    "        \"optimizer_gen\": \"adadelta\",\n",
    "#         \"optimizer_disc\": \"adadelta(lr=0.1)\",\n",
    "#         \"optimizer_gen\": \"adadelta(lr=0.1)\",\n",
    "        \"output_size\": 17,\n",
    "        #\"scaler_type\": \"jetiso\",\n",
    "        \"scaler_type\": \"None\",\n",
    "        \"terminate_early\": True,\n",
    "        \"use_mll_loss\": True,\n",
    "        \"width_disc\": 0,\n",
    "        \"width_gen\": 0,\n",
    "        \"verbose\": True,\n",
    "        \"outdir\": \"/nfs-7/userdata/bhashemi/DY-GAN/models\",\n",
    "        #\"outdir\": \"/home/users/bhashemi/Projects/GIT/DY-GAN/gan/notebooks\"\n",
    "        }\n",
    "params.update({\n",
    "})\n",
    "print params\n",
    "    \n",
    "# change tag for provenance\n",
    "params[\"tag\"] = \"v3_test\"\n",
    "\n",
    "gan = GAN(**params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_53 (InputLayer)           (None, 17)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_313 (Dense)               (None, 128)          2304        input_53[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_277 (LeakyReLU)     (None, 128)          0           dense_313[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_314 (Dense)               (None, 128)          16512       leaky_re_lu_277[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_278 (LeakyReLU)     (None, 128)          0           dense_314[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_315 (Dense)               (None, 256)          33024       leaky_re_lu_278[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_279 (LeakyReLU)     (None, 256)          0           dense_315[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_316 (Dense)               (None, 256)          65792       leaky_re_lu_279[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_280 (LeakyReLU)     (None, 256)          0           dense_316[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_317 (Dense)               (None, 128)          32896       leaky_re_lu_280[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_281 (LeakyReLU)     (None, 128)          0           dense_317[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_318 (Dense)               (None, 64)           8256        leaky_re_lu_281[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_282 (LeakyReLU)     (None, 64)           0           dense_318[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_319 (Dense)               (None, 32)           2080        leaky_re_lu_282[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_283 (LeakyReLU)     (None, 32)           0           dense_319[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_320 (Dense)               (None, 16)           528         leaky_re_lu_283[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_284 (LeakyReLU)     (None, 16)           0           dense_320[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_321 (Dense)               (None, 8)            136         leaky_re_lu_284[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_285 (LeakyReLU)     (None, 8)            0           dense_321[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_322 (Dense)               (None, 1)            9           leaky_re_lu_285[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_61 (Lambda)              (None, 1)            0           input_53[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_62 (Lambda)              (None, 1)            0           input_53[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_63 (Lambda)              (None, 1)            0           input_53[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_21 (Concatenate)    (None, 4)            0           dense_322[0][0]                  \n",
      "                                                                 lambda_61[0][0]                  \n",
      "                                                                 lambda_62[0][0]                  \n",
      "                                                                 lambda_63[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 161,537\n",
      "Trainable params: 161,537\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Discriminator params: 161537\n",
      "Generator params: 340497\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_54 (InputLayer)        (None, 17)                0         \n",
      "_________________________________________________________________\n",
      "dense_323 (Dense)            (None, 64)                1152      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_286 (LeakyReLU)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_324 (Dense)            (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_287 (LeakyReLU)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_325 (Dense)            (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_288 (LeakyReLU)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_326 (Dense)            (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_289 (LeakyReLU)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_327 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_290 (LeakyReLU)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_328 (Dense)            (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_291 (LeakyReLU)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_329 (Dense)            (None, 17)                2193      \n",
      "=================================================================\n",
      "Total params: 340,497\n",
      "Trainable params: 340,497\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_56 (InputLayer)           (None, 17)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_330 (Dense)               (None, 128)          2304        input_56[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_292 (LeakyReLU)     (None, 128)          0           dense_330[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_331 (Dense)               (None, 128)          16512       leaky_re_lu_292[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_293 (LeakyReLU)     (None, 128)          0           dense_331[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_332 (Dense)               (None, 256)          33024       leaky_re_lu_293[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_294 (LeakyReLU)     (None, 256)          0           dense_332[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_333 (Dense)               (None, 256)          65792       leaky_re_lu_294[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_295 (LeakyReLU)     (None, 256)          0           dense_333[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_334 (Dense)               (None, 128)          32896       leaky_re_lu_295[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_296 (LeakyReLU)     (None, 128)          0           dense_334[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_335 (Dense)               (None, 64)           8256        leaky_re_lu_296[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_297 (LeakyReLU)     (None, 64)           0           dense_335[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_336 (Dense)               (None, 32)           2080        leaky_re_lu_297[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_298 (LeakyReLU)     (None, 32)           0           dense_336[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_337 (Dense)               (None, 16)           528         leaky_re_lu_298[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_299 (LeakyReLU)     (None, 16)           0           dense_337[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_338 (Dense)               (None, 8)            136         leaky_re_lu_299[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_300 (LeakyReLU)     (None, 8)            0           dense_338[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_339 (Dense)               (None, 1)            9           leaky_re_lu_300[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_64 (Lambda)              (None, 1)            0           input_56[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_65 (Lambda)              (None, 1)            0           input_56[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_66 (Lambda)              (None, 1)            0           input_56[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_22 (Concatenate)    (None, 4)            0           dense_339[0][0]                  \n",
      "                                                                 lambda_64[0][0]                  \n",
      "                                                                 lambda_65[0][0]                  \n",
      "                                                                 lambda_66[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 161,537\n",
      "Trainable params: 161,537\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Discriminator params: 161537\n",
      "Generator params: 340497\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_57 (InputLayer)        (None, 17)                0         \n",
      "_________________________________________________________________\n",
      "dense_340 (Dense)            (None, 64)                1152      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_301 (LeakyReLU)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_341 (Dense)            (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_302 (LeakyReLU)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_342 (Dense)            (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_303 (LeakyReLU)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_343 (Dense)            (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_304 (LeakyReLU)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_344 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_305 (LeakyReLU)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_345 (Dense)            (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_306 (LeakyReLU)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_346 (Dense)            (None, 17)                2193      \n",
      "=================================================================\n",
      "Total params: 340,497\n",
      "Trainable params: 340,497\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 [D loss: 0.116265542805, acc.: 0.00%] [G loss: 2.40036559105] [mll=-1.000+--1.000] [ks=999.000]]\n",
      "200 [D loss: 0.0954831838608, acc.: 0.00%] [G loss: 3.25717401505] [mll=12.644+-1.420] [ks=1.000]]\n",
      "300 [D loss: 0.0738756284118, acc.: 0.00%] [G loss: 4.16508150101] [mll=53.538+-6.320] [ks=0.945]\n",
      "400 [D loss: 0.0491074770689, acc.: 0.00%] [G loss: 4.18707227707] [mll=45.934+-5.358] [ks=0.966]\n",
      "500 [D loss: 0.0971423089504, acc.: 0.00%] [G loss: 5.27033567429] [mll=64.386+-7.432] [ks=0.884]\n",
      "10000/10000 [==============================] - 1s 61us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:74: RuntimeWarning: invalid value encountered in sqrt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600 [D loss: 0.0187243316323, acc.: 0.00%] [G loss: 4.96369743347] [mll=52.974+-6.027] [ks=0.947]]\n",
      "700 [D loss: 0.0545892640948, acc.: 0.00%] [G loss: 4.74828863144] [mll=89.933+-10.453] [ks=0.226]\n",
      "800 [D loss: 0.0946172550321, acc.: 0.00%] [G loss: 5.60989332199] [mll=74.750+-8.444] [ks=0.734]\n",
      "900 [D loss: 0.0566033162177, acc.: 0.00%] [G loss: 3.97165679932] [mll=64.787+-7.027] [ks=0.890]\n",
      "1000 [D loss: 0.0271218121052, acc.: 0.00%] [G loss: 3.47482657433] [mll=70.592+-7.661] [ks=0.826]\n",
      "10000/10000 [==============================] - 1s 60us/step\n",
      "1100 [D loss: 0.0385638810694, acc.: 0.00%] [G loss: 4.28246545792] [mll=80.107+-8.336] [ks=0.587]\n",
      "1200 [D loss: 0.0885897353292, acc.: 0.00%] [G loss: 3.11744523048] [mll=77.228+-7.846] [ks=0.692]\n",
      "1300 [D loss: 0.0722847878933, acc.: 0.00%] [G loss: 3.03892612457] [mll=79.509+-8.075] [ks=0.616]\n",
      "1400 [D loss: 0.192727297544, acc.: 0.00%] [G loss: 8.73898601532] [mll=77.054+-7.140] [ks=0.720]]\n",
      "1500 [D loss: 0.0604627989233, acc.: 0.00%] [G loss: 3.06627297401] [mll=74.002+-6.948] [ks=0.790]\n",
      "10000/10000 [==============================] - 1s 69us/step\n",
      "1600 [D loss: 0.0805656835437, acc.: 0.00%] [G loss: 3.85432076454] [mll=57.130+-4.882] [ks=0.944]\n",
      "1700 [D loss: 0.184171885252, acc.: 0.00%] [G loss: 2.93529939651] [mll=68.995+-6.172] [ks=0.872]]\n",
      "1800 [D loss: 0.0927197635174, acc.: 0.00%] [G loss: 2.8370244503] [mll=90.934+-8.331] [ks=0.231]]\n",
      "1900 [D loss: 0.0769556760788, acc.: 0.00%] [G loss: 3.20711874962] [mll=82.855+-6.980] [ks=0.522]\n",
      "2000 [D loss: 0.0595183707774, acc.: 0.00%] [G loss: 4.68848228455] [mll=93.020+-7.652] [ks=0.331]\n",
      "10000/10000 [==============================] - 1s 77us/step\n",
      "2100 [D loss: 0.217591077089, acc.: 0.00%] [G loss: 3.51219344139] [mll=55.892+-4.774] [ks=0.948]]]\n",
      "2200 [D loss: 0.0969184115529, acc.: 0.00%] [G loss: 3.0980091095] [mll=89.905+-7.346] [ks=0.167]]\n",
      "2300 [D loss: 0.112144522369, acc.: 0.00%] [G loss: 4.80711698532] [mll=80.939+-5.888] [ks=0.641]]\n",
      "2400 [D loss: 7.97128772736, acc.: 0.00%] [G loss: 0.00372176570818] [mll=67.177+-4.868] [ks=0.903]\n",
      "2500 [D loss: 7.97119188309, acc.: 0.00%] [G loss: 0.00442462367937] [mll=89.265+-6.003] [ks=0.171]\n",
      "10000/10000 [==============================] - 1s 68us/step\n",
      "2600 [D loss: 7.97119188309, acc.: 0.00%] [G loss: 0.00690241809934] [mll=93.403+-5.496] [ks=0.349]\n",
      "2700 [D loss: 7.97119188309, acc.: 0.00%] [G loss: 0.00403500162065] [mll=94.999+-4.674] [ks=0.486]\n",
      "2800 [D loss: 7.97119426727, acc.: 0.00%] [G loss: 0.00535294041038] [mll=92.425+-3.954] [ks=0.274]\n",
      "2900 [D loss: 7.97119188309, acc.: 0.00%] [G loss: 0.0044239484705] [mll=94.243+-3.928] [ks=0.448]]\n",
      "3000 [D loss: 7.97119188309, acc.: 0.00%] [G loss: 0.00319514214061] [mll=93.043+-3.812] [ks=0.332]\n",
      "10000/10000 [==============================] - 1s 66us/step\n",
      "3100 [D loss: 7.97119188309, acc.: 0.00%] [G loss: 0.00459322007373] [mll=91.071+-3.706] [ks=0.118]\n",
      "3200 [D loss: 7.97119188309, acc.: 0.00%] [G loss: 0.00455856928602] [mll=93.442+-3.975] [ks=0.350]\n",
      "3300 [D loss: 7.97119188309, acc.: 0.00%] [G loss: 0.0044883787632] [mll=93.282+-3.971] [ks=0.336]]\n",
      "3400 [D loss: 7.97119188309, acc.: 0.00%] [G loss: 0.00430591823533] [mll=93.254+-4.034] [ks=0.327]\n",
      "3500 [D loss: 7.97119188309, acc.: 0.00%] [G loss: 0.00417475728318] [mll=92.873+-3.959] [ks=0.286]\n",
      "10000/10000 [==============================] - 1s 68us/step\n",
      "3533 [D loss: 7.97119188309, acc.: 0.00%] [G loss: 0.00345643959008] [mll=92.670+-3.963] [ks=0.263]BREAKING because disc/gen loss has remained the same for 1001/9 epochs!\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_59 (InputLayer)           (None, 17)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_347 (Dense)               (None, 128)          2304        input_59[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_307 (LeakyReLU)     (None, 128)          0           dense_347[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_348 (Dense)               (None, 128)          16512       leaky_re_lu_307[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_308 (LeakyReLU)     (None, 128)          0           dense_348[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_349 (Dense)               (None, 256)          33024       leaky_re_lu_308[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_309 (LeakyReLU)     (None, 256)          0           dense_349[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_350 (Dense)               (None, 256)          65792       leaky_re_lu_309[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_310 (LeakyReLU)     (None, 256)          0           dense_350[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_351 (Dense)               (None, 128)          32896       leaky_re_lu_310[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_311 (LeakyReLU)     (None, 128)          0           dense_351[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_352 (Dense)               (None, 64)           8256        leaky_re_lu_311[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_312 (LeakyReLU)     (None, 64)           0           dense_352[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_353 (Dense)               (None, 32)           2080        leaky_re_lu_312[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_313 (LeakyReLU)     (None, 32)           0           dense_353[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_354 (Dense)               (None, 16)           528         leaky_re_lu_313[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_314 (LeakyReLU)     (None, 16)           0           dense_354[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_355 (Dense)               (None, 8)            136         leaky_re_lu_314[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_315 (LeakyReLU)     (None, 8)            0           dense_355[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_356 (Dense)               (None, 1)            9           leaky_re_lu_315[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_67 (Lambda)              (None, 1)            0           input_59[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_68 (Lambda)              (None, 1)            0           input_59[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_69 (Lambda)              (None, 1)            0           input_59[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_23 (Concatenate)    (None, 4)            0           dense_356[0][0]                  \n",
      "                                                                 lambda_67[0][0]                  \n",
      "                                                                 lambda_68[0][0]                  \n",
      "                                                                 lambda_69[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 161,537\n",
      "Trainable params: 161,537\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Discriminator params: 161537\n",
      "Generator params: 340497\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_60 (InputLayer)        (None, 17)                0         \n",
      "_________________________________________________________________\n",
      "dense_357 (Dense)            (None, 64)                1152      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_316 (LeakyReLU)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_358 (Dense)            (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_317 (LeakyReLU)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_359 (Dense)            (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_318 (LeakyReLU)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_360 (Dense)            (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_319 (LeakyReLU)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_361 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_320 (LeakyReLU)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_362 (Dense)            (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_321 (LeakyReLU)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_363 (Dense)            (None, 17)                2193      \n",
      "=================================================================\n",
      "Total params: 340,497\n",
      "Trainable params: 340,497\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 [D loss: 0.0504452846944, acc.: 0.00%] [G loss: 6.09108781815] [mll=-1.000+--1.000] [ks=999.000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in sqrt\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:96: RuntimeWarning: Mean of empty slice.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 [D loss: 0.0793833658099, acc.: 0.00%] [G loss: 2.95337057114] [mll=nan+-nan] [ks=1.000]\n",
      "300 [D loss: 0.365017414093, acc.: 0.00%] [G loss: 2.02338695526] [mll=45.733+-4.872] [ks=0.969]]\n",
      "400 [D loss: 0.167869433761, acc.: 0.00%] [G loss: 4.09248876572] [mll=49.877+-4.988] [ks=0.961]1]\n",
      "500 [D loss: 0.188357919455, acc.: 0.00%] [G loss: 2.56744074821] [mll=39.999+-3.834] [ks=0.988]]\n",
      "10000/10000 [==============================] - 1s 66us/step\n",
      "600 [D loss: 0.0592795871198, acc.: 0.00%] [G loss: 4.38813066483] [mll=71.149+-6.789] [ks=0.837]\n",
      "700 [D loss: 1.3364238739, acc.: 0.00%] [G loss: 4.48352241516] [mll=22.705+-2.214] [ks=0.999]]9]\n",
      "800 [D loss: 0.0311002135277, acc.: 0.00%] [G loss: 4.18136739731] [mll=82.042+-7.188] [ks=0.552]\n",
      "900 [D loss: 0.0391888618469, acc.: 0.00%] [G loss: 4.20669126511] [mll=59.913+-4.975] [ks=0.934]\n",
      "1000 [D loss: 0.137930646539, acc.: 0.00%] [G loss: 4.93250322342] [mll=64.792+-5.292] [ks=0.911]\n",
      "10000/10000 [==============================] - 1s 72us/step\n",
      "1100 [D loss: 0.101556777954, acc.: 0.00%] [G loss: 3.11634993553] [mll=56.750+-5.193] [ks=0.943]]]\n",
      "1200 [D loss: 0.10331749171, acc.: 0.00%] [G loss: 2.56210589409] [mll=75.757+-7.628] [ks=0.733]3]\n",
      "1300 [D loss: 0.0297594368458, acc.: 0.00%] [G loss: 3.85203194618] [mll=108.884+-9.081] [ks=0.845]\n",
      "1400 [D loss: 0.0844576209784, acc.: 0.00%] [G loss: 3.28847551346] [mll=64.397+-6.052] [ks=0.905]\n",
      "1500 [D loss: 0.0788353607059, acc.: 0.00%] [G loss: 3.34702515602] [mll=86.899+-7.533] [ks=0.305]\n",
      "10000/10000 [==============================] - 1s 64us/step\n",
      "1600 [D loss: 0.0934630706906, acc.: 0.00%] [G loss: 3.13041830063] [mll=72.993+-6.351] [ks=0.820]\n",
      "1700 [D loss: 0.571282565594, acc.: 0.00%] [G loss: 1.4595015049] [mll=43.060+-3.047] [ks=0.986]]]\n",
      "1800 [D loss: 0.19614315033, acc.: 0.00%] [G loss: 2.41295599937] [mll=73.712+-4.888] [ks=0.846]]]\n",
      "1900 [D loss: 0.149623855948, acc.: 0.00%] [G loss: 2.3316628933] [mll=64.469+-4.619] [ks=0.912]]]]\n",
      "2000 [D loss: 0.297016113997, acc.: 0.00%] [G loss: 4.43688726425] [mll=86.745+-4.876] [ks=0.374]]\n",
      "10000/10000 [==============================] - 1s 70us/step\n",
      "2100 [D loss: 0.222697988153, acc.: 0.00%] [G loss: 4.66337203979] [mll=49.886+-30.096] [ks=0.689]\n",
      "2200 [D loss: 0.296020030975, acc.: 0.00%] [G loss: 1.95358085632] [mll=82.797+-12.622] [ks=0.346]]\n",
      "2300 [D loss: 0.323462933302, acc.: 0.00%] [G loss: 2.5163629055] [mll=101.157+-13.408] [ks=0.564]]]\n",
      "2400 [D loss: 0.232484102249, acc.: 0.00%] [G loss: 2.00094676018] [mll=26.553+-11.520] [ks=0.944]]\n",
      "2500 [D loss: 0.292543023825, acc.: 0.00%] [G loss: 1.88720619678] [mll=88.479+-9.799] [ks=0.306]]\n",
      "10000/10000 [==============================] - 1s 58us/step\n",
      "2600 [D loss: 0.172767400742, acc.: 0.00%] [G loss: 2.08962130547] [mll=81.068+-7.216] [ks=0.570]\n",
      "2700 [D loss: 0.286209821701, acc.: 0.00%] [G loss: 2.03474783897] [mll=90.500+-6.334] [ks=0.175]\n",
      "2800 [D loss: 0.168811097741, acc.: 0.00%] [G loss: 2.2887532711] [mll=94.796+-11.086] [ks=0.405]]\n",
      "2900 [D loss: 0.334235161543, acc.: 0.00%] [G loss: 2.01508522034] [mll=81.374+-10.146] [ks=0.457]]\n",
      "3000 [D loss: 0.359547585249, acc.: 0.00%] [G loss: 1.82072281837] [mll=85.524+-11.528] [ks=0.430]\n",
      "10000/10000 [==============================] - 1s 67us/step\n",
      "3100 [D loss: 0.3188624084, acc.: 0.00%] [G loss: 2.79878401756] [mll=99.660+-10.640] [ks=0.607]7]\n",
      "3200 [D loss: 0.376759022474, acc.: 0.00%] [G loss: 3.33702683449] [mll=64.112+-11.530] [ks=0.491]]\n",
      "3300 [D loss: 0.540000021458, acc.: 0.00%] [G loss: 2.09305357933] [mll=82.545+-8.941] [ks=0.542]]\n",
      "3400 [D loss: 0.278130084276, acc.: 0.00%] [G loss: 1.97216880322] [mll=84.079+-8.435] [ks=0.476]\n",
      "3500 [D loss: 0.26201698184, acc.: 1.17%] [G loss: 3.27072572708] [mll=82.980+-12.610] [ks=0.552]]]\n",
      "10000/10000 [==============================] - 1s 71us/step\n",
      "3600 [D loss: 0.523655176163, acc.: 0.00%] [G loss: 1.60249495506] [mll=122.417+-25.833] [ks=0.838]\n",
      "3700 [D loss: 0.434806644917, acc.: 0.00%] [G loss: 1.63066768646] [mll=63.782+-27.778] [ks=0.478]\n",
      "3800 [D loss: 0.433179557323, acc.: 0.00%] [G loss: 1.77367973328] [mll=81.230+-11.580] [ks=0.478]\n",
      "3900 [D loss: 0.397945523262, acc.: 0.00%] [G loss: 2.09857487679] [mll=90.547+-10.025] [ks=0.218]]\n",
      "4000 [D loss: 0.370897591114, acc.: 0.00%] [G loss: 1.66031932831] [mll=80.581+-9.509] [ks=0.591]]\n",
      "10000/10000 [==============================] - 1s 56us/step\n",
      "4100 [D loss: 0.464315921068, acc.: 0.00%] [G loss: 1.70247745514] [mll=87.740+-9.665] [ks=0.260]]\n",
      "4200 [D loss: 0.582535684109, acc.: 0.00%] [G loss: 1.51273059845] [mll=88.302+-9.797] [ks=0.225]]\n",
      "4300 [D loss: 0.483391702175, acc.: 0.00%] [G loss: 1.54813683033] [mll=94.605+-9.833] [ks=0.362]]\n",
      "4400 [D loss: 0.450546056032, acc.: 0.00%] [G loss: 1.63094604015] [mll=87.707+-10.571] [ks=0.271]]\n",
      "4500 [D loss: 0.593685209751, acc.: 0.00%] [G loss: 1.64474081993] [mll=92.810+-10.498] [ks=0.270]]\n",
      "10000/10000 [==============================] - 1s 75us/step\n",
      "4600 [D loss: 0.532686293125, acc.: 0.00%] [G loss: 1.53721535206] [mll=92.257+-7.265] [ks=0.231]]\n",
      "4700 [D loss: 0.567753255367, acc.: 0.00%] [G loss: 1.00797498226] [mll=78.152+-18.408] [ks=0.453]]\n",
      "4800 [D loss: 0.718884408474, acc.: 0.00%] [G loss: 1.10706341267] [mll=84.048+-10.042] [ks=0.427]]\n",
      "4900 [D loss: 0.622775018215, acc.: 0.00%] [G loss: 1.0255433321] [mll=91.721+-9.816] [ks=0.287]]]\n",
      "5000 [D loss: 0.556991279125, acc.: 0.00%] [G loss: 1.12957334518] [mll=95.294+-7.883] [ks=0.411]]\n",
      "10000/10000 [==============================] - 1s 66us/step\n",
      "5100 [D loss: 0.790053963661, acc.: 0.00%] [G loss: 1.54181075096] [mll=93.319+-7.849] [ks=0.273]]\n",
      "5200 [D loss: 0.607528746128, acc.: 0.00%] [G loss: 1.09546124935] [mll=80.267+-13.817] [ks=0.453]]\n",
      "5300 [D loss: 0.527245044708, acc.: 0.00%] [G loss: 1.05374825001] [mll=94.295+-8.272] [ks=0.352]]\n",
      "5400 [D loss: 0.546691358089, acc.: 0.00%] [G loss: 1.14851796627] [mll=96.025+-9.755] [ks=0.424]]\n",
      "5500 [D loss: 0.59296900034, acc.: 0.00%] [G loss: 1.17832767963] [mll=94.418+-10.509] [ks=0.344]]]\n",
      "10000/10000 [==============================] - 1s 68us/step\n",
      "5600 [D loss: 0.65874171257, acc.: 0.00%] [G loss: 1.15774047375] [mll=93.469+-8.461] [ks=0.308]8]\n",
      "5700 [D loss: 0.673692345619, acc.: 0.00%] [G loss: 1.16926085949] [mll=89.880+-6.848] [ks=0.172]]\n",
      "5800 [D loss: 0.690068662167, acc.: 0.00%] [G loss: 1.00553119183] [mll=87.762+-7.052] [ks=0.280]]\n",
      "5900 [D loss: 0.657738268375, acc.: 0.00%] [G loss: 1.08483731747] [mll=98.856+-13.759] [ks=0.493]]\n",
      "6000 [D loss: 0.681610167027, acc.: 0.00%] [G loss: 0.762902796268] [mll=96.584+-7.748] [ks=0.515]\n",
      "10000/10000 [==============================] - 1s 71us/step\n",
      "6100 [D loss: 0.615307629108, acc.: 0.00%] [G loss: 0.981242239475] [mll=94.196+-9.688] [ks=0.379]\n",
      "6200 [D loss: 0.65023547411, acc.: 0.00%] [G loss: 0.877409398556] [mll=97.324+-7.855] [ks=0.518]]\n",
      "6300 [D loss: 0.641703903675, acc.: 0.00%] [G loss: 0.80201870203] [mll=85.949+-9.444] [ks=0.324]]\n",
      "6400 [D loss: 0.633475482464, acc.: 0.00%] [G loss: 0.774331986904] [mll=98.136+-8.456] [ks=0.566]\n",
      "6500 [D loss: 0.627217829227, acc.: 0.00%] [G loss: 0.954428315163] [mll=98.485+-11.007] [ks=0.508]\n",
      "10000/10000 [==============================] - 1s 67us/step\n",
      "6600 [D loss: 0.605366945267, acc.: 0.00%] [G loss: 0.790049493313] [mll=91.095+-12.279] [ks=0.329]\n",
      "6700 [D loss: 0.621037423611, acc.: 0.00%] [G loss: 0.943916857243] [mll=95.457+-6.012] [ks=0.481]\n",
      "6800 [D loss: 0.656402170658, acc.: 0.00%] [G loss: 0.913144052029] [mll=88.807+-9.690] [ks=0.198]\n",
      "6900 [D loss: 0.581184446812, acc.: 0.00%] [G loss: 0.79833137989] [mll=79.331+-18.784] [ks=0.373]]\n",
      "7000 [D loss: 0.654708147049, acc.: 0.00%] [G loss: 1.09271728992] [mll=92.246+-6.850] [ks=0.266]]\n",
      "10000/10000 [==============================] - 1s 75us/step\n",
      "7100 [D loss: 0.536071360111, acc.: 0.00%] [G loss: 1.0523301363] [mll=94.728+-7.322] [ks=0.409]9]\n",
      "7200 [D loss: 0.539049804211, acc.: 0.00%] [G loss: 0.998930990696] [mll=90.484+-6.751] [ks=0.133]\n",
      "7300 [D loss: 0.54767036438, acc.: 0.00%] [G loss: 0.905533254147] [mll=90.255+-9.048] [ks=0.227]]\n",
      "7400 [D loss: 0.554215013981, acc.: 0.00%] [G loss: 1.78690326214] [mll=93.140+-9.386] [ks=0.386]]\n",
      "7500 [D loss: 0.515777111053, acc.: 0.39%] [G loss: 1.28506290913] [mll=94.993+-10.634] [ks=0.381]]\n",
      "10000/10000 [==============================] - 1s 68us/step\n",
      "7600 [D loss: 0.472477376461, acc.: 0.00%] [G loss: 1.0404253006] [mll=98.445+-23.569] [ks=0.343]]]\n",
      "7700 [D loss: 0.586065351963, acc.: 0.00%] [G loss: 1.07223069668] [mll=93.565+-9.021] [ks=0.351]]\n",
      "7800 [D loss: 0.570640087128, acc.: 0.00%] [G loss: 1.08487451077] [mll=94.336+-9.619] [ks=0.385]]\n",
      "7900 [D loss: 0.523158609867, acc.: 0.00%] [G loss: 1.5798060894] [mll=94.842+-10.205] [ks=0.412]]]\n",
      "8000 [D loss: 0.535959541798, acc.: 0.00%] [G loss: 1.11380660534] [mll=102.900+-17.149] [ks=0.538]]\n",
      "10000/10000 [==============================] - 1s 66us/step\n",
      "8100 [D loss: 0.548588335514, acc.: 0.00%] [G loss: 1.34421169758] [mll=101.853+-14.616] [ks=0.542]]\n",
      "8200 [D loss: 0.619704246521, acc.: 0.00%] [G loss: 1.26554763317] [mll=96.494+-8.837] [ks=0.472]]\n",
      "8300 [D loss: 0.517880916595, acc.: 0.00%] [G loss: 1.01509201527] [mll=96.416+-8.097] [ks=0.503]]\n",
      "8400 [D loss: 0.480951040983, acc.: 0.00%] [G loss: 1.06943500042] [mll=92.266+-6.547] [ks=0.264]]\n",
      "8500 [D loss: 0.514145851135, acc.: 0.00%] [G loss: 1.01611864567] [mll=85.904+-12.769] [ks=0.286]]\n",
      "10000/10000 [==============================] - 1s 65us/step\n",
      "8600 [D loss: 0.496464937925, acc.: 0.00%] [G loss: 1.03820598125] [mll=96.418+-9.431] [ks=0.548]]\n",
      "8700 [D loss: 0.504763543606, acc.: 0.00%] [G loss: 1.08857643604] [mll=89.008+-8.935] [ks=0.163]]\n",
      "8800 [D loss: 0.497523576021, acc.: 0.00%] [G loss: 1.05350887775] [mll=93.614+-8.238] [ks=0.277]]\n",
      "8900 [D loss: 0.541633546352, acc.: 0.00%] [G loss: 1.13564634323] [mll=91.988+-7.459] [ks=0.236]]\n",
      "9000 [D loss: 0.473088949919, acc.: 0.00%] [G loss: 1.04946386814] [mll=94.339+-8.165] [ks=0.361]]\n",
      "10000/10000 [==============================] - 1s 74us/step\n",
      "9100 [D loss: 0.48606389761, acc.: 0.00%] [G loss: 0.953188955784] [mll=92.187+-8.091] [ks=0.253]]\n",
      "9200 [D loss: 0.521269261837, acc.: 0.59%] [G loss: 1.37755084038] [mll=94.838+-8.190] [ks=0.394]]\n",
      "9300 [D loss: 0.480201214552, acc.: 0.00%] [G loss: 1.17304241657] [mll=107.844+-27.213] [ks=0.545]]\n",
      "9400 [D loss: 0.487068235874, acc.: 0.00%] [G loss: 1.17835009098] [mll=88.867+-9.433] [ks=0.230]]\n",
      "9500 [D loss: 0.499278277159, acc.: 0.00%] [G loss: 1.06400978565] [mll=92.828+-7.437] [ks=0.279]]\n",
      "10000/10000 [==============================] - 1s 80us/step\n",
      "9600 [D loss: 0.496438086033, acc.: 0.00%] [G loss: 1.08258831501] [mll=92.975+-6.785] [ks=0.315]]\n",
      "9700 [D loss: 0.429472953081, acc.: 0.00%] [G loss: 1.08058893681] [mll=98.263+-15.861] [ks=0.420]]\n",
      "9800 [D loss: 0.348125189543, acc.: 0.00%] [G loss: 1.38637530804] [mll=91.897+-9.592] [ks=0.290]]\n",
      "9900 [D loss: 0.326496571302, acc.: 0.00%] [G loss: 1.77377998829] [mll=87.419+-7.196] [ks=0.293]\n",
      "10000 [D loss: 0.271506100893, acc.: 0.00%] [G loss: 1.60127162933] [mll=78.659+-10.575] [ks=0.626]\n",
      "10000/10000 [==============================] - 1s 61us/step\n",
      "10100 [D loss: 0.302722364664, acc.: 0.00%] [G loss: 1.47970867157] [mll=84.233+-6.218] [ks=0.523]\n",
      "10200 [D loss: 0.321805864573, acc.: 0.00%] [G loss: 1.56376338005] [mll=89.914+-8.897] [ks=0.163]\n",
      "10300 [D loss: 0.262882351875, acc.: 0.00%] [G loss: 1.69316494465] [mll=88.409+-11.674] [ks=0.323]\n",
      "10400 [D loss: 0.267262816429, acc.: 0.00%] [G loss: 1.7961204052] [mll=87.308+-8.762] [ks=0.279]]\n",
      "10500 [D loss: 0.268256932497, acc.: 0.00%] [G loss: 1.88493323326] [mll=88.592+-8.795] [ks=0.274]\n",
      "10000/10000 [==============================] - 1s 73us/step\n",
      "10600 [D loss: 0.262538164854, acc.: 0.20%] [G loss: 1.81142067909] [mll=89.569+-6.494] [ks=0.173]\n",
      "10700 [D loss: 0.298074841499, acc.: 0.00%] [G loss: 1.73033308983] [mll=83.740+-9.604] [ks=0.426]\n",
      "10800 [D loss: 0.315149575472, acc.: 0.00%] [G loss: 1.85302078724] [mll=89.281+-6.642] [ks=0.212]\n",
      "10900 [D loss: 0.309931755066, acc.: 0.00%] [G loss: 1.83825194836] [mll=91.258+-23.542] [ks=0.271]\n",
      "11000 [D loss: 0.298057436943, acc.: 0.00%] [G loss: 1.91546463966] [mll=91.275+-7.522] [ks=0.217]\n",
      "10000/10000 [==============================] - 1s 68us/step\n",
      "11100 [D loss: 0.31968781352, acc.: 0.00%] [G loss: 1.59831798077] [mll=83.802+-9.087] [ks=0.405]]\n",
      "11200 [D loss: 0.261583209038, acc.: 0.00%] [G loss: 1.96433269978] [mll=92.549+-8.113] [ks=0.318]\n",
      "11300 [D loss: 0.278968453407, acc.: 0.00%] [G loss: 1.86377537251] [mll=93.981+-10.880] [ks=0.291]\n",
      "11400 [D loss: 0.298701971769, acc.: 0.00%] [G loss: 1.65484154224] [mll=104.606+-24.208] [ks=0.500]\n",
      "11500 [D loss: 0.284047156572, acc.: 0.00%] [G loss: 1.75620377064] [mll=96.334+-5.030] [ks=0.578]\n",
      "10000/10000 [==============================] - 1s 71us/step\n",
      "11600 [D loss: 0.306767612696, acc.: 0.00%] [G loss: 1.72235572338] [mll=94.689+-7.798] [ks=0.460]\n",
      "11700 [D loss: 0.294254779816, acc.: 0.00%] [G loss: 1.83647453785] [mll=86.614+-18.504] [ks=0.271]\n",
      "11800 [D loss: 0.248862743378, acc.: 0.00%] [G loss: 1.92158675194] [mll=95.283+-9.870] [ks=0.436]\n",
      "11900 [D loss: 0.263189941645, acc.: 0.39%] [G loss: 1.7822457552] [mll=88.597+-6.056] [ks=0.251]]\n",
      "12000 [D loss: 0.276924878359, acc.: 0.00%] [G loss: 1.8725720644] [mll=98.683+-12.637] [ks=0.474]]\n",
      "10000/10000 [==============================] - 1s 75us/step\n",
      "12100 [D loss: 0.344426214695, acc.: 0.00%] [G loss: 1.71529793739] [mll=86.037+-12.543] [ks=0.256]\n",
      "12200 [D loss: 0.288292557001, acc.: 0.00%] [G loss: 1.73100876808] [mll=91.911+-8.250] [ks=0.205]\n",
      "12300 [D loss: 0.397515416145, acc.: 0.00%] [G loss: 2.08100438118] [mll=109.405+-24.343] [ks=0.621]\n",
      "12400 [D loss: 0.271045833826, acc.: 0.00%] [G loss: 1.77879345417] [mll=99.929+-19.630] [ks=0.426]\n",
      "12500 [D loss: 0.312352091074, acc.: 0.00%] [G loss: 1.77141487598] [mll=99.263+-14.309] [ks=0.467]\n",
      "10000/10000 [==============================] - 1s 65us/step\n",
      "12600 [D loss: 0.299500256777, acc.: 0.00%] [G loss: 1.73047542572] [mll=89.007+-7.183] [ks=0.248]\n",
      "12700 [D loss: 0.333731919527, acc.: 0.00%] [G loss: 1.73054027557] [mll=91.263+-7.157] [ks=0.190]\n",
      "12800 [D loss: 0.328506231308, acc.: 0.00%] [G loss: 1.63977241516] [mll=91.632+-7.285] [ks=0.177]\n",
      "12900 [D loss: 0.285591334105, acc.: 0.00%] [G loss: 1.50444495678] [mll=98.497+-7.597] [ks=0.595]\n",
      "13000 [D loss: 0.259700745344, acc.: 0.39%] [G loss: 1.80197155476] [mll=96.623+-7.285] [ks=0.526]\n",
      "10000/10000 [==============================] - 1s 53us/step\n",
      "13100 [D loss: 0.285496026278, acc.: 0.00%] [G loss: 1.78816330433] [mll=96.433+-11.465] [ks=0.381]\n",
      "13200 [D loss: 0.294338434935, acc.: 1.17%] [G loss: 1.76825225353] [mll=88.375+-7.889] [ks=0.271]\n",
      "13300 [D loss: 0.2519749403, acc.: 0.00%] [G loss: 1.76004230976] [mll=102.790+-19.490] [ks=0.503]3]\n",
      "13400 [D loss: 0.270391732454, acc.: 0.00%] [G loss: 1.71956419945] [mll=89.921+-6.039] [ks=0.133]\n",
      "13500 [D loss: 0.270496994257, acc.: 0.00%] [G loss: 1.89423823357] [mll=87.875+-7.044] [ks=0.276]\n",
      "10000/10000 [==============================] - 1s 71us/step\n",
      "13600 [D loss: 0.28363135457, acc.: 0.00%] [G loss: 1.72928798199] [mll=90.275+-6.732] [ks=0.114]]\n",
      "13700 [D loss: 0.255271941423, acc.: 0.00%] [G loss: 1.98971354961] [mll=89.764+-6.892] [ks=0.159]\n",
      "13800 [D loss: 0.281166821718, acc.: 0.00%] [G loss: 1.70952212811] [mll=92.993+-8.991] [ks=0.232]\n",
      "13900 [D loss: 0.290975064039, acc.: 0.00%] [G loss: 1.76755154133] [mll=91.153+-5.953] [ks=0.168]\n",
      "14000 [D loss: 0.280923724174, acc.: 0.00%] [G loss: 1.81473350525] [mll=92.364+-6.935] [ks=0.260]\n",
      "10000/10000 [==============================] - 1s 58us/step\n",
      "14100 [D loss: 0.271594256163, acc.: 0.00%] [G loss: 1.73974013329] [mll=89.571+-7.305] [ks=0.131]\n",
      "14200 [D loss: 0.238629072905, acc.: 0.00%] [G loss: 1.97655856609] [mll=91.032+-6.418] [ks=0.176]\n",
      "14300 [D loss: 0.284886389971, acc.: 2.54%] [G loss: 1.96638095379] [mll=89.583+-6.469] [ks=0.162]\n",
      "14400 [D loss: 0.380798310041, acc.: 0.00%] [G loss: 1.74898135662] [mll=98.174+-16.175] [ks=0.417]\n",
      "14500 [D loss: 0.299926072359, acc.: 0.00%] [G loss: 1.71338832378] [mll=94.246+-9.134] [ks=0.389]\n",
      "10000/10000 [==============================] - 1s 71us/step\n",
      "14600 [D loss: 0.246062785387, acc.: 0.00%] [G loss: 1.90648651123] [mll=84.574+-11.900] [ks=0.296]\n",
      "14700 [D loss: 0.267650991678, acc.: 0.00%] [G loss: 1.8086616993] [mll=88.613+-7.099] [ks=0.243]]\n",
      "14800 [D loss: 0.286030650139, acc.: 0.00%] [G loss: 1.66802811623] [mll=92.266+-5.729] [ks=0.211]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14900 [D loss: 0.275162696838, acc.: 0.00%] [G loss: 1.79077708721] [mll=90.087+-9.451] [ks=0.232]\n",
      "15000 [D loss: 0.313216656446, acc.: 0.00%] [G loss: 1.75499618053] [mll=89.958+-6.242] [ks=0.156]\n",
      "10000/10000 [==============================] - 1s 58us/step\n",
      "15100 [D loss: 0.256868153811, acc.: 0.00%] [G loss: 1.81139397621] [mll=90.070+-7.342] [ks=0.158]\n",
      "15200 [D loss: 0.330031365156, acc.: 0.00%] [G loss: 1.64417266846] [mll=87.861+-4.897] [ks=0.280]\n",
      "15300 [D loss: 0.291591435671, acc.: 0.00%] [G loss: 1.63956642151] [mll=89.972+-6.742] [ks=0.137]\n",
      "15400 [D loss: 0.280439019203, acc.: 0.00%] [G loss: 1.88303291798] [mll=90.417+-10.629] [ks=0.196]\n",
      "15500 [D loss: 7.97119188309, acc.: 0.00%] [G loss: nan] [mll=100.580+-12.687] [ks=0.592] [ks=0.592]\n",
      "  768/10000 [=>............................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:116: RuntimeWarning: Mean of empty slice.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 67us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:69: RuntimeWarning: invalid value encountered in greater\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:70: RuntimeWarning: invalid value encountered in less\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15600 [D loss: 7.97119188309, acc.: 0.00%] [G loss: nan] [mll=nan+-nan] [ks=1.000]\n",
      "15700 [D loss: 7.97119188309, acc.: 0.00%] [G loss: nan] [mll=nan+-nan] [ks=1.000]\n",
      "15800 [D loss: 7.97119188309, acc.: 0.00%] [G loss: nan] [mll=nan+-nan] [ks=1.000]\n",
      "15900 [D loss: 7.97119188309, acc.: 0.00%] [G loss: nan] [mll=nan+-nan] [ks=1.000]\n",
      "16000 [D loss: 7.97119188309, acc.: 0.00%] [G loss: nan] [mll=nan+-nan] [ks=1.000]\n",
      "10000/10000 [==============================] - 1s 76us/step\n",
      "16100 [D loss: 7.97119188309, acc.: 0.00%] [G loss: nan] [mll=nan+-nan] [ks=1.000]\n",
      "16200 [D loss: 7.97119188309, acc.: 0.00%] [G loss: nan] [mll=nan+-nan] [ks=1.000]\n",
      "16300 [D loss: 7.97119188309, acc.: 0.00%] [G loss: nan] [mll=nan+-nan] [ks=1.000]\n",
      "16400 [D loss: 7.97119188309, acc.: 0.00%] [G loss: nan] [mll=nan+-nan] [ks=1.000]\n",
      "16480 [D loss: 7.97119188309, acc.: 0.00%] [G loss: nan] [mll=nan+-nan] [ks=1.000]BREAKING because disc/gen loss has remained the same for 1001/0 epochs!\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_62 (InputLayer)           (None, 17)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_364 (Dense)               (None, 128)          2304        input_62[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_322 (LeakyReLU)     (None, 128)          0           dense_364[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_365 (Dense)               (None, 128)          16512       leaky_re_lu_322[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_323 (LeakyReLU)     (None, 128)          0           dense_365[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_366 (Dense)               (None, 256)          33024       leaky_re_lu_323[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_324 (LeakyReLU)     (None, 256)          0           dense_366[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_367 (Dense)               (None, 256)          65792       leaky_re_lu_324[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_325 (LeakyReLU)     (None, 256)          0           dense_367[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_368 (Dense)               (None, 128)          32896       leaky_re_lu_325[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_326 (LeakyReLU)     (None, 128)          0           dense_368[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_369 (Dense)               (None, 64)           8256        leaky_re_lu_326[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_327 (LeakyReLU)     (None, 64)           0           dense_369[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_370 (Dense)               (None, 32)           2080        leaky_re_lu_327[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_328 (LeakyReLU)     (None, 32)           0           dense_370[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_371 (Dense)               (None, 16)           528         leaky_re_lu_328[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_329 (LeakyReLU)     (None, 16)           0           dense_371[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_372 (Dense)               (None, 8)            136         leaky_re_lu_329[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_330 (LeakyReLU)     (None, 8)            0           dense_372[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_373 (Dense)               (None, 1)            9           leaky_re_lu_330[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_70 (Lambda)              (None, 1)            0           input_62[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_71 (Lambda)              (None, 1)            0           input_62[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_72 (Lambda)              (None, 1)            0           input_62[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_24 (Concatenate)    (None, 4)            0           dense_373[0][0]                  \n",
      "                                                                 lambda_70[0][0]                  \n",
      "                                                                 lambda_71[0][0]                  \n",
      "                                                                 lambda_72[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 161,537\n",
      "Trainable params: 161,537\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Discriminator params: 161537\n",
      "Generator params: 340497\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_63 (InputLayer)        (None, 17)                0         \n",
      "_________________________________________________________________\n",
      "dense_374 (Dense)            (None, 64)                1152      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_331 (LeakyReLU)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_375 (Dense)            (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_332 (LeakyReLU)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_376 (Dense)            (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_333 (LeakyReLU)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_377 (Dense)            (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_334 (LeakyReLU)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_378 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_335 (LeakyReLU)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_379 (Dense)            (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_336 (LeakyReLU)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_380 (Dense)            (None, 17)                2193      \n",
      "=================================================================\n",
      "Total params: 340,497\n",
      "Trainable params: 340,497\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 [D loss: 0.342079669237, acc.: 50.00%] [G loss: 1.60910403728] [mll=-1.000+--1.000] [ks=999.000]\n",
      "200 [D loss: 0.327333569527, acc.: 0.00%] [G loss: 4.07549858093] [mll=nan+-nan] [ks=1.000]]\n",
      "300 [D loss: 0.111453585327, acc.: 0.00%] [G loss: 4.31909942627] [mll=49.129+-5.171] [ks=0.961]]\n",
      "400 [D loss: 0.0891220867634, acc.: 0.00%] [G loss: 3.59400606155] [mll=51.197+-5.003] [ks=0.958]]\n",
      "500 [D loss: 0.51011043787, acc.: 0.00%] [G loss: 3.16450285912] [mll=77.084+-7.398] [ks=0.710]0]\n",
      "10000/10000 [==============================] - 1s 67us/step\n",
      "600 [D loss: 0.0769529864192, acc.: 0.00%] [G loss: 3.99172997475] [mll=65.458+-5.854] [ks=0.901]]\n",
      "700 [D loss: 0.785475194454, acc.: 0.00%] [G loss: 1.62477850914] [mll=107.212+-8.609] [ks=0.827]]\n",
      "800 [D loss: 0.180782049894, acc.: 0.00%] [G loss: 2.41401934624] [mll=130.057+-11.044] [ks=0.964]]\n",
      "900 [D loss: 0.121985562146, acc.: 0.00%] [G loss: 3.21455597878] [mll=31.076+-1.979] [ks=0.998]]\n",
      "1000 [D loss: 0.156403288245, acc.: 0.00%] [G loss: 2.86421823502] [mll=40.775+-2.543] [ks=0.993]]\n",
      "10000/10000 [==============================] - 1s 76us/step\n",
      "1100 [D loss: 0.0865856483579, acc.: 0.00%] [G loss: 3.35996031761] [mll=71.295+-5.720] [ks=0.857]\n",
      "1200 [D loss: 0.0510685741901, acc.: 0.00%] [G loss: 3.41428756714] [mll=70.827+-5.174] [ks=0.874]\n",
      "1300 [D loss: 0.046125151217, acc.: 0.00%] [G loss: 3.94144964218] [mll=68.248+-4.973] [ks=0.895]]\n",
      "1400 [D loss: 0.0511827208102, acc.: 0.00%] [G loss: 3.76405930519] [mll=83.791+-6.157] [ks=0.502]]\n",
      "1500 [D loss: 0.0280660670251, acc.: 0.00%] [G loss: 4.57260465622] [mll=93.808+-6.489] [ks=0.367]\n",
      "10000/10000 [==============================] - 1s 64us/step\n",
      "1600 [D loss: 0.1689607054, acc.: 0.00%] [G loss: 2.95702719688] [mll=83.301+-5.644] [ks=0.549]9]]\n",
      "1700 [D loss: 0.0830387696624, acc.: 0.00%] [G loss: 3.13949418068] [mll=52.082+-2.444] [ks=0.970]\n",
      "1800 [D loss: 0.230772480369, acc.: 0.00%] [G loss: 3.14357805252] [mll=87.001+-4.925] [ks=0.343]]]\n",
      "1900 [D loss: 0.174476996064, acc.: 0.00%] [G loss: 3.81453490257] [mll=6.458+-3.385] [ks=0.937]]\n",
      "2000 [D loss: 0.20155775547, acc.: 0.00%] [G loss: 2.15982270241] [mll=44.546+-11.282] [ks=0.887]]]\n",
      "10000/10000 [==============================] - 1s 80us/step\n",
      "2100 [D loss: 0.191528171301, acc.: 0.00%] [G loss: 2.15134334564] [mll=96.245+-6.799] [ks=0.502]\n",
      "2200 [D loss: 0.119921565056, acc.: 0.00%] [G loss: 3.07562017441] [mll=88.610+-8.716] [ks=0.196]]\n",
      "2300 [D loss: 0.170889943838, acc.: 0.00%] [G loss: 3.43758749962] [mll=81.897+-7.272] [ks=0.554]]\n",
      "2400 [D loss: 0.185292243958, acc.: 0.00%] [G loss: 4.84441232681] [mll=77.279+-17.917] [ks=0.440]]\n",
      "2500 [D loss: 0.163168296218, acc.: 0.00%] [G loss: 2.44790244102] [mll=82.376+-20.110] [ks=0.651]]\n",
      "10000/10000 [==============================] - ETA:  - 1s 71us/step\n",
      "2600 [D loss: 0.246031150222, acc.: 0.00%] [G loss: 2.79996299744] [mll=69.079+-15.122] [ks=0.673]]\n",
      "2700 [D loss: 0.39600867033, acc.: 0.00%] [G loss: 2.40192317963] [mll=85.158+-32.879] [ks=0.457]]]\n",
      "2800 [D loss: 0.190156593919, acc.: 0.00%] [G loss: 2.32823014259] [mll=112.326+-21.587] [ks=0.683]]\n",
      "2900 [D loss: 0.168288588524, acc.: 0.00%] [G loss: 2.54788684845] [mll=83.836+-12.789] [ks=0.313]\n",
      "3000 [D loss: 7.97119188309, acc.: 0.00%] [G loss: nan] [mll=81.044+-8.580] [ks=0.589] [ks=0.589]\n",
      "10000/10000 [==============================] - 1s 75us/step\n",
      "3100 [D loss: 7.97119188309, acc.: 0.00%] [G loss: nan] [mll=nan+-nan] [ks=1.000]\n",
      "3200 [D loss: 7.97119188309, acc.: 0.00%] [G loss: nan] [mll=nan+-nan] [ks=1.000]\n",
      "3300 [D loss: 7.97119188309, acc.: 0.00%] [G loss: nan] [mll=nan+-nan] [ks=1.000]\n",
      "3400 [D loss: 7.97119188309, acc.: 0.00%] [G loss: nan] [mll=nan+-nan] [ks=1.000]\n",
      "3500 [D loss: 7.97119188309, acc.: 0.00%] [G loss: nan] [mll=nan+-nan] [ks=1.000]\n",
      "10000/10000 [==============================] - 1s 54us/step\n",
      "3600 [D loss: 7.97119188309, acc.: 0.00%] [G loss: nan] [mll=nan+-nan] [ks=1.000]\n",
      "3700 [D loss: 7.97119188309, acc.: 0.00%] [G loss: nan] [mll=nan+-nan] [ks=1.000]\n",
      "3800 [D loss: 7.97119188309, acc.: 0.00%] [G loss: nan] [mll=nan+-nan] [ks=1.000]\n",
      "3900 [D loss: 7.97119188309, acc.: 0.00%] [G loss: nan] [mll=nan+-nan] [ks=1.000]\n",
      "3943 [D loss: 7.97119188309, acc.: 0.00%] [G loss: nan] [mll=nan+-nan] [ks=1.000]BREAKING because disc/gen loss has remained the same for 1001/0 epochs!\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_65 (InputLayer)           (None, 17)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_381 (Dense)               (None, 128)          2304        input_65[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_337 (LeakyReLU)     (None, 128)          0           dense_381[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_382 (Dense)               (None, 128)          16512       leaky_re_lu_337[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_338 (LeakyReLU)     (None, 128)          0           dense_382[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_383 (Dense)               (None, 256)          33024       leaky_re_lu_338[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_339 (LeakyReLU)     (None, 256)          0           dense_383[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_384 (Dense)               (None, 256)          65792       leaky_re_lu_339[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_340 (LeakyReLU)     (None, 256)          0           dense_384[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_385 (Dense)               (None, 128)          32896       leaky_re_lu_340[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_341 (LeakyReLU)     (None, 128)          0           dense_385[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_386 (Dense)               (None, 64)           8256        leaky_re_lu_341[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_342 (LeakyReLU)     (None, 64)           0           dense_386[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_387 (Dense)               (None, 32)           2080        leaky_re_lu_342[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_343 (LeakyReLU)     (None, 32)           0           dense_387[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_388 (Dense)               (None, 16)           528         leaky_re_lu_343[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_344 (LeakyReLU)     (None, 16)           0           dense_388[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_389 (Dense)               (None, 8)            136         leaky_re_lu_344[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_345 (LeakyReLU)     (None, 8)            0           dense_389[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_390 (Dense)               (None, 1)            9           leaky_re_lu_345[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_73 (Lambda)              (None, 1)            0           input_65[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_74 (Lambda)              (None, 1)            0           input_65[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_75 (Lambda)              (None, 1)            0           input_65[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_25 (Concatenate)    (None, 4)            0           dense_390[0][0]                  \n",
      "                                                                 lambda_73[0][0]                  \n",
      "                                                                 lambda_74[0][0]                  \n",
      "                                                                 lambda_75[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 161,537\n",
      "Trainable params: 161,537\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Discriminator params: 161537\n",
      "Generator params: 340497\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_66 (InputLayer)        (None, 17)                0         \n",
      "_________________________________________________________________\n",
      "dense_391 (Dense)            (None, 64)                1152      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_346 (LeakyReLU)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_392 (Dense)            (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_347 (LeakyReLU)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_393 (Dense)            (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_348 (LeakyReLU)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_394 (Dense)            (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_349 (LeakyReLU)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_395 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_350 (LeakyReLU)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_396 (Dense)            (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_351 (LeakyReLU)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_397 (Dense)            (None, 17)                2193      \n",
      "=================================================================\n",
      "Total params: 340,497\n",
      "Trainable params: 340,497\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 [D loss: 0.0435382984579, acc.: 0.00%] [G loss: 3.42070221901] [mll=-1.000+--1.000] [ks=999.000]\n",
      "200 [D loss: 0.173529133201, acc.: 0.00%] [G loss: 3.26238465309] [mll=0.209+-0.030] [ks=1.000]]]\n",
      "300 [D loss: 0.157982960343, acc.: 0.00%] [G loss: 2.34672546387] [mll=22.374+-2.459] [ks=0.999]]\n",
      "400 [D loss: 0.061284031719, acc.: 0.00%] [G loss: 3.94566321373] [mll=40.808+-4.236] [ks=0.984]]]\n",
      "500 [D loss: 0.109905958176, acc.: 0.00%] [G loss: 2.78771209717] [mll=40.560+-3.715] [ks=0.987]]]\n",
      "10000/10000 [==============================] - 1s 64us/step\n",
      "600 [D loss: 0.0596599020064, acc.: 0.00%] [G loss: 3.81438994408] [mll=86.664+-8.588] [ks=0.312]\n",
      "700 [D loss: 0.0593498982489, acc.: 0.00%] [G loss: 3.7574429512] [mll=51.286+-4.556] [ks=0.960]]\n",
      "800 [D loss: 0.020041776821, acc.: 0.00%] [G loss: 4.29294538498] [mll=78.186+-7.124] [ks=0.689]]\n",
      "900 [D loss: 0.176859155297, acc.: 0.00%] [G loss: 7.33455276489] [mll=48.805+-4.267] [ks=0.967]]\n",
      "1000 [D loss: 0.104008808732, acc.: 0.00%] [G loss: 2.67991089821] [mll=72.647+-6.516] [ks=0.820]\n",
      "10000/10000 [==============================] - 1s 83us/step\n",
      "1100 [D loss: 0.140408158302, acc.: 0.00%] [G loss: 2.09627890587] [mll=71.024+-6.121] [ks=0.853]]\n",
      "1200 [D loss: 0.205920010805, acc.: 0.00%] [G loss: 3.56628775597] [mll=59.437+-5.085] [ks=0.933]]]\n",
      "1300 [D loss: 0.184653058648, acc.: 0.00%] [G loss: 2.55261063576] [mll=44.030+-3.889] [ks=0.979]]\n",
      "1400 [D loss: 0.0964005589485, acc.: 0.00%] [G loss: 3.79892396927] [mll=87.611+-9.048] [ks=0.272]\n",
      "1500 [D loss: 0.0755426213145, acc.: 0.00%] [G loss: 3.58896112442] [mll=68.700+-6.202] [ks=0.875]\n",
      "10000/10000 [==============================] - 1s 64us/step\n",
      "1600 [D loss: 0.0690885186195, acc.: 50.00%] [G loss: 3.82383394241] [mll=74.462+-6.416] [ks=0.797]\n",
      "1700 [D loss: 0.0723606720567, acc.: 0.00%] [G loss: 3.08135128021] [mll=3.266+-1.652] [ks=0.997]]\n",
      "1800 [D loss: 0.102238409221, acc.: 0.00%] [G loss: 5.91804456711] [mll=63.737+-4.553] [ks=0.924]]\n",
      "1900 [D loss: 0.115860648453, acc.: 0.00%] [G loss: 2.68983459473] [mll=59.389+-4.842] [ks=0.936]]\n",
      "2000 [D loss: 0.137822136283, acc.: 0.00%] [G loss: 3.53275537491] [mll=82.614+-10.950] [ks=0.455]]\n",
      "10000/10000 [==============================] - 1s 65us/step\n",
      "2100 [D loss: 0.138037368655, acc.: 0.00%] [G loss: 2.23408722878] [mll=42.728+-9.347] [ks=0.935]]\n",
      "2200 [D loss: 0.236678153276, acc.: 0.00%] [G loss: 2.14985203743] [mll=85.059+-7.702] [ks=0.437]\n",
      "2300 [D loss: 0.778613328934, acc.: 0.00%] [G loss: 2.38861966133] [mll=78.111+-6.979] [ks=0.704]\n",
      "2400 [D loss: 0.2989988029, acc.: 1.17%] [G loss: 2.69308257103] [mll=nan+-nan] [ks=1.000]0]]\n",
      "2500 [D loss: 0.23474702239, acc.: 0.00%] [G loss: 2.95119833946] [mll=124.967+-9.820] [ks=0.963]]]\n",
      "10000/10000 [==============================] - 1s 68us/step\n",
      "2600 [D loss: 0.193228662014, acc.: 0.00%] [G loss: 2.17396831512] [mll=86.290+-8.806] [ks=0.409]]\n",
      "2700 [D loss: 0.244014397264, acc.: 0.00%] [G loss: 1.91484439373] [mll=91.886+-6.363] [ks=0.249]\n",
      "2800 [D loss: 0.371264994144, acc.: 0.00%] [G loss: 1.70584559441] [mll=80.149+-8.525] [ks=0.602]]\n",
      "2900 [D loss: 0.479650199413, acc.: 0.59%] [G loss: 1.73661899567] [mll=71.047+-12.844] [ks=0.684]]\n",
      "3000 [D loss: 0.424012839794, acc.: 0.00%] [G loss: 1.77249634266] [mll=88.974+-28.254] [ks=0.479]]\n",
      "10000/10000 [==============================] - 1s 69us/step\n",
      "3100 [D loss: 0.322252780199, acc.: 0.00%] [G loss: 2.37532544136] [mll=85.273+-10.915] [ks=0.347]\n",
      "3200 [D loss: 0.422369211912, acc.: 0.00%] [G loss: 1.55792737007] [mll=109.311+-19.642] [ks=0.642]\n",
      "3300 [D loss: 0.432740002871, acc.: 0.00%] [G loss: 1.43484389782] [mll=83.435+-9.422] [ks=0.458]\n",
      "3400 [D loss: 0.350567370653, acc.: 0.00%] [G loss: 1.89475810528] [mll=89.732+-9.241] [ks=0.210]\n",
      "3500 [D loss: 0.48473495245, acc.: 0.59%] [G loss: 1.73511338234] [mll=84.937+-8.943] [ks=0.416]]\n",
      "10000/10000 [==============================] - 1s 57us/step\n",
      "3600 [D loss: 0.756511569023, acc.: 0.00%] [G loss: 1.86568725109] [mll=131.632+-49.623] [ks=0.694]]\n",
      "3700 [D loss: 0.518766880035, acc.: 0.00%] [G loss: 1.38021087646] [mll=81.126+-11.286] [ks=0.457]\n",
      "3800 [D loss: 0.442759543657, acc.: 0.00%] [G loss: 1.86928737164] [mll=85.681+-9.765] [ks=0.332]\n",
      "3900 [D loss: 0.507230758667, acc.: 0.00%] [G loss: 1.62796866894] [mll=92.927+-7.073] [ks=0.299]]\n",
      "4000 [D loss: 0.516744852066, acc.: 0.00%] [G loss: 1.27190124989] [mll=89.084+-11.698] [ks=0.266]\n",
      "10000/10000 [==============================] - 1s 78us/step\n",
      "4100 [D loss: 0.537034749985, acc.: 0.00%] [G loss: 1.35052120686] [mll=90.009+-12.153] [ks=0.246]]\n",
      "4200 [D loss: 0.642208635807, acc.: 0.00%] [G loss: 1.13339221478] [mll=95.938+-16.218] [ks=0.356]\n",
      "4300 [D loss: 0.490819722414, acc.: 0.00%] [G loss: 1.24309003353] [mll=97.788+-11.082] [ks=0.483]]\n",
      "4400 [D loss: 0.541103839874, acc.: 0.00%] [G loss: 1.44812917709] [mll=92.737+-7.146] [ks=0.268]]\n",
      "4500 [D loss: 0.609719574451, acc.: 0.00%] [G loss: 0.956616282463] [mll=83.862+-20.835] [ks=0.307]\n",
      "10000/10000 [==============================] - 1s 75us/step\n",
      "4600 [D loss: 0.414302557707, acc.: 0.00%] [G loss: 1.1743260622] [mll=100.271+-10.070] [ks=0.625]]]\n",
      "4700 [D loss: 0.596880972385, acc.: 0.00%] [G loss: 1.13569879532] [mll=89.282+-7.321] [ks=0.191]]\n",
      "4800 [D loss: 0.709198236465, acc.: 0.00%] [G loss: 0.878025949001] [mll=92.841+-10.728] [ks=0.347]\n",
      "4900 [D loss: 0.680205345154, acc.: 0.00%] [G loss: 1.47222697735] [mll=117.823+-25.625] [ks=0.803]]\n",
      "5000 [D loss: 0.651766240597, acc.: 0.00%] [G loss: 0.886491775513] [mll=91.176+-7.535] [ks=0.196]\n",
      "10000/10000 [==============================] - 1s 75us/step\n",
      "5100 [D loss: 0.637722730637, acc.: 0.00%] [G loss: 1.02861320972] [mll=95.670+-8.810] [ks=0.406]]\n",
      "5200 [D loss: 0.646030664444, acc.: 0.00%] [G loss: 0.996762216091] [mll=86.898+-8.405] [ks=0.292]\n",
      "5300 [D loss: 0.561964094639, acc.: 0.00%] [G loss: 0.944493293762] [mll=85.489+-8.536] [ks=0.365]\n",
      "5400 [D loss: 0.539624750614, acc.: 0.00%] [G loss: 0.89332729578] [mll=86.850+-7.059] [ks=0.308]]\n",
      "5500 [D loss: 0.598141133785, acc.: 0.00%] [G loss: 0.959523320198] [mll=94.811+-6.411] [ks=0.449]\n",
      "10000/10000 [==============================] - 1s 70us/step\n",
      "5600 [D loss: 0.62322974205, acc.: 0.00%] [G loss: 1.02116501331] [mll=95.384+-6.472] [ks=0.465]5]\n",
      "5700 [D loss: 0.63516664505, acc.: 0.00%] [G loss: 0.883281648159] [mll=93.588+-6.714] [ks=0.345]]\n",
      "5800 [D loss: 0.673293948174, acc.: 0.00%] [G loss: 1.07264268398] [mll=91.662+-7.445] [ks=0.199]]\n",
      "5900 [D loss: 0.678618192673, acc.: 0.00%] [G loss: 0.933366179466] [mll=87.876+-11.784] [ks=0.189]\n",
      "6000 [D loss: 0.639429807663, acc.: 0.00%] [G loss: 0.869183182716] [mll=93.512+-6.689] [ks=0.310]\n",
      "10000/10000 [==============================] - 1s 61us/step\n",
      "6100 [D loss: 0.686990320683, acc.: 0.00%] [G loss: 0.799593925476] [mll=93.562+-7.365] [ks=0.396]\n",
      "6200 [D loss: 0.804777681828, acc.: 0.00%] [G loss: 0.96150624752] [mll=92.185+-8.358] [ks=0.220]]\n",
      "6300 [D loss: 0.690047621727, acc.: 0.00%] [G loss: 0.83983695507] [mll=85.623+-7.292] [ks=0.415]]\n",
      "6400 [D loss: 0.636852681637, acc.: 0.00%] [G loss: 0.873325288296] [mll=94.910+-8.231] [ks=0.404]\n",
      "6500 [D loss: 0.631722152233, acc.: 0.00%] [G loss: 0.834936738014] [mll=92.253+-8.201] [ks=0.205]\n",
      "10000/10000 [==============================] - 1s 70us/step\n",
      "6600 [D loss: 0.661060750484, acc.: 0.00%] [G loss: 0.800534367561] [mll=90.420+-6.833] [ks=0.176]\n",
      "6700 [D loss: 0.647250115871, acc.: 0.00%] [G loss: 0.875992894173] [mll=107.529+-12.997] [ks=0.780]\n",
      "6800 [D loss: 0.696500301361, acc.: 0.00%] [G loss: 0.944686949253] [mll=86.441+-13.180] [ks=0.291]\n",
      "6900 [D loss: 0.59815788269, acc.: 0.00%] [G loss: 0.97212266922] [mll=80.036+-6.751] [ks=0.662]2]\n",
      "7000 [D loss: 0.55116802454, acc.: 0.00%] [G loss: 1.00016105175] [mll=88.580+-6.026] [ks=0.251]1]\n",
      "10000/10000 [==============================] - 1s 66us/step\n",
      "7100 [D loss: 0.589907705784, acc.: 0.00%] [G loss: 0.997764527798] [mll=96.234+-7.774] [ks=0.461]\n",
      "7200 [D loss: 0.502277910709, acc.: 0.00%] [G loss: 0.983861088753] [mll=95.458+-8.339] [ks=0.413]\n",
      "7300 [D loss: 0.476888120174, acc.: 0.00%] [G loss: 1.09938538074] [mll=88.610+-8.461] [ks=0.277]]\n",
      "7400 [D loss: 0.569671154022, acc.: 0.00%] [G loss: 0.90537917614] [mll=99.308+-14.749] [ks=0.422]]\n",
      "7500 [D loss: 0.499561756849, acc.: 0.00%] [G loss: 1.25973784924] [mll=91.589+-7.456] [ks=0.267]]\n",
      "10000/10000 [==============================] - 1s 74us/step\n",
      "7600 [D loss: 0.506973266602, acc.: 0.00%] [G loss: 1.06897068024] [mll=92.393+-6.527] [ks=0.259]]\n",
      "7700 [D loss: 0.505785942078, acc.: 0.00%] [G loss: 1.09711408615] [mll=105.754+-16.349] [ks=0.610]]\n",
      "7800 [D loss: 0.476281374693, acc.: 0.00%] [G loss: 1.09213638306] [mll=91.230+-7.834] [ks=0.239]]\n",
      "7900 [D loss: 0.567868769169, acc.: 0.00%] [G loss: 1.26714742184] [mll=96.189+-7.163] [ks=0.531]]\n",
      "8000 [D loss: 0.491501033306, acc.: 0.00%] [G loss: 1.05359017849] [mll=84.543+-8.555] [ks=0.415]]\n",
      "10000/10000 [==============================] - 1s 58us/step\n",
      "8100 [D loss: 0.529035568237, acc.: 0.00%] [G loss: 0.991418778896] [mll=92.152+-7.517] [ks=0.273]\n",
      "8200 [D loss: 0.46938508749, acc.: 0.00%] [G loss: 1.25655758381] [mll=92.089+-6.754] [ks=0.236]]]\n",
      "8300 [D loss: 0.532364666462, acc.: 0.00%] [G loss: 1.10416555405] [mll=108.811+-17.129] [ks=0.683]]\n",
      "8400 [D loss: 0.492182463408, acc.: 0.00%] [G loss: 1.17956888676] [mll=94.115+-10.149] [ks=0.406]]\n",
      "8500 [D loss: 0.528766572475, acc.: 0.00%] [G loss: 0.985602319241] [mll=94.458+-6.770] [ks=0.340]\n",
      "10000/10000 [==============================] - 1s 68us/step\n",
      "8600 [D loss: 0.428864508867, acc.: 0.00%] [G loss: 1.22315037251] [mll=101.101+-14.435] [ks=0.545]]\n",
      "8700 [D loss: 0.500163018703, acc.: 0.00%] [G loss: 1.05141794682] [mll=91.272+-6.925] [ks=0.182]]\n",
      "8800 [D loss: 0.487069159746, acc.: 0.00%] [G loss: 1.17482805252] [mll=91.475+-7.088] [ks=0.249]]\n",
      "8900 [D loss: 0.489271908998, acc.: 0.00%] [G loss: 1.13217532635] [mll=93.693+-6.678] [ks=0.312]]\n",
      "9000 [D loss: 0.452869445086, acc.: 0.00%] [G loss: 1.05626857281] [mll=90.451+-8.647] [ks=0.205]]\n",
      "10000/10000 [==============================] - 1s 63us/step\n",
      "9100 [D loss: 0.451246321201, acc.: 0.00%] [G loss: 1.13041198254] [mll=88.030+-11.837] [ks=0.183]]\n",
      "9200 [D loss: 0.468594968319, acc.: 0.00%] [G loss: 1.2059649229] [mll=91.113+-6.541] [ks=0.183]]]\n",
      "9300 [D loss: 0.486537188292, acc.: 0.00%] [G loss: 1.2514218092] [mll=96.247+-8.173] [ks=0.468]]]\n",
      "9400 [D loss: 0.511888742447, acc.: 0.00%] [G loss: 1.27869069576] [mll=87.321+-10.402] [ks=0.266]]\n",
      "9500 [D loss: 0.362328737974, acc.: 0.00%] [G loss: 1.45865905285] [mll=97.837+-6.517] [ks=0.626]\n",
      "10000/10000 [==============================] - 1s 60us/step\n",
      "9600 [D loss: 0.306161254644, acc.: 0.00%] [G loss: 1.53841567039] [mll=91.509+-7.488] [ks=0.187]\n",
      "9700 [D loss: 0.333960086107, acc.: 0.00%] [G loss: 1.75937473774] [mll=87.594+-7.948] [ks=0.287]\n",
      "9800 [D loss: 0.366978943348, acc.: 0.00%] [G loss: 1.60163283348] [mll=87.915+-9.136] [ks=0.256]\n",
      "9900 [D loss: 0.305990427732, acc.: 0.00%] [G loss: 1.55973064899] [mll=84.129+-9.664] [ks=0.408]\n",
      "10000 [D loss: 0.343558102846, acc.: 0.00%] [G loss: 1.6520127058] [mll=87.537+-8.974] [ks=0.225]\n",
      "10000/10000 [==============================] - 1s 67us/step\n",
      "10100 [D loss: 0.304830819368, acc.: 0.00%] [G loss: 1.72379899025] [mll=90.249+-7.745] [ks=0.145]\n",
      "10200 [D loss: 0.317961484194, acc.: 0.00%] [G loss: 1.77919495106] [mll=87.554+-8.370] [ks=0.229]\n",
      "10300 [D loss: 0.313395351171, acc.: 0.00%] [G loss: 1.80658888817] [mll=90.854+-7.502] [ks=0.145]\n",
      "10400 [D loss: 0.302913874388, acc.: 0.00%] [G loss: 1.73167610168] [mll=86.747+-6.300] [ks=0.335]\n",
      "10500 [D loss: 0.253609776497, acc.: 0.00%] [G loss: 1.84759151936] [mll=83.060+-7.696] [ks=0.531]\n",
      "10000/10000 [==============================] - 1s 68us/step\n",
      "10600 [D loss: 0.381758719683, acc.: 0.00%] [G loss: 1.78725230694] [mll=88.653+-6.071] [ks=0.214]\n",
      "10700 [D loss: 0.272982388735, acc.: 0.00%] [G loss: 1.81612420082] [mll=90.972+-9.854] [ks=0.213]\n",
      "10800 [D loss: 0.317494720221, acc.: 0.00%] [G loss: 1.76123821735] [mll=92.052+-8.606] [ks=0.199]\n",
      "10900 [D loss: 0.288374692202, acc.: 0.00%] [G loss: 1.91299068928] [mll=92.368+-7.373] [ks=0.232]\n",
      "11000 [D loss: 0.285838872194, acc.: 0.00%] [G loss: 1.64155983925] [mll=89.618+-8.117] [ks=0.175]\n",
      "10000/10000 [==============================] - 1s 68us/step\n",
      "11100 [D loss: 0.261492818594, acc.: 0.00%] [G loss: 1.9994982481] [mll=93.060+-7.655] [ks=0.346]]\n",
      "11200 [D loss: 0.243009239435, acc.: 0.00%] [G loss: 2.0979783535] [mll=89.771+-7.648] [ks=0.168]]\n",
      "11300 [D loss: 0.267397850752, acc.: 0.00%] [G loss: 1.86283326149] [mll=86.984+-20.154] [ks=0.294]\n",
      "11400 [D loss: 0.296913594007, acc.: 0.00%] [G loss: 1.72275257111] [mll=88.857+-6.302] [ks=0.212]\n",
      "11500 [D loss: 0.345512032509, acc.: 0.00%] [G loss: 1.62329220772] [mll=89.510+-6.625] [ks=0.164]\n",
      "10000/10000 [==============================] - 1s 70us/step\n",
      "11600 [D loss: 0.248894244432, acc.: 0.00%] [G loss: 1.80966067314] [mll=86.572+-10.493] [ks=0.231]\n",
      "11700 [D loss: 0.241219878197, acc.: 0.00%] [G loss: 1.89847648144] [mll=91.088+-7.628] [ks=0.170]\n",
      "11800 [D loss: 0.291971057653, acc.: 0.00%] [G loss: 1.6432363987] [mll=88.103+-7.661] [ks=0.316]]\n",
      "11900 [D loss: 0.317754745483, acc.: 0.00%] [G loss: 1.81312918663] [mll=87.429+-7.308] [ks=0.335]\n",
      "12000 [D loss: 0.291215747595, acc.: 0.00%] [G loss: 1.72418701649] [mll=101.844+-19.175] [ks=0.485]\n",
      "10000/10000 [==============================] - 1s 77us/step\n",
      "12100 [D loss: 0.287009745836, acc.: 0.00%] [G loss: 1.78921425343] [mll=92.412+-6.195] [ks=0.278]\n",
      "12200 [D loss: 0.288615733385, acc.: 0.00%] [G loss: 1.73445320129] [mll=91.631+-6.672] [ks=0.181]\n",
      "12300 [D loss: 0.292246222496, acc.: 0.00%] [G loss: 1.6431568861] [mll=91.723+-5.784] [ks=0.216]]\n",
      "12400 [D loss: 0.247563019395, acc.: 0.00%] [G loss: 1.76816630363] [mll=91.166+-9.854] [ks=0.222]\n",
      "12500 [D loss: 0.292219281197, acc.: 0.00%] [G loss: 1.7409286499] [mll=90.587+-8.656] [ks=0.161]]\n",
      "10000/10000 [==============================] - 1s 68us/step\n",
      "12600 [D loss: 0.276425629854, acc.: 0.00%] [G loss: 1.74199318886] [mll=90.937+-7.190] [ks=0.197]\n",
      "12700 [D loss: 0.262628346682, acc.: 0.00%] [G loss: 1.90336251259] [mll=87.898+-7.275] [ks=0.242]\n",
      "12800 [D loss: 0.288486868143, acc.: 0.00%] [G loss: 1.77699065208] [mll=90.558+-7.047] [ks=0.170]\n",
      "12900 [D loss: 0.283688873053, acc.: 0.00%] [G loss: 1.68995118141] [mll=90.316+-6.604] [ks=0.120]\n",
      "13000 [D loss: 0.264869332314, acc.: 0.00%] [G loss: 1.75023627281] [mll=90.523+-6.313] [ks=0.155]\n",
      "10000/10000 [==============================] - 1s 68us/step\n",
      "13100 [D loss: 0.2608551085, acc.: 0.00%] [G loss: 1.93385148048] [mll=90.236+-7.124] [ks=0.147]7]\n",
      "13200 [D loss: 0.246787413955, acc.: 0.00%] [G loss: 1.90646529198] [mll=86.132+-14.845] [ks=0.217]\n",
      "13300 [D loss: 0.332156389952, acc.: 0.00%] [G loss: 1.926684618] [mll=92.628+-9.430] [ks=0.311]1]\n",
      "13400 [D loss: 0.296863555908, acc.: 0.00%] [G loss: 1.76688027382] [mll=88.926+-7.289] [ks=0.240]\n",
      "13500 [D loss: 0.261453747749, acc.: 0.00%] [G loss: 1.89778053761] [mll=89.662+-7.406] [ks=0.162]\n",
      "10000/10000 [==============================] - 1s 70us/step\n",
      "13600 [D loss: 0.274797230959, acc.: 0.00%] [G loss: 1.75968909264] [mll=90.036+-8.659] [ks=0.222]\n",
      "13700 [D loss: 0.280518084764, acc.: 0.00%] [G loss: 1.63473153114] [mll=89.505+-6.632] [ks=0.180]\n",
      "13800 [D loss: 0.252593159676, acc.: 0.00%] [G loss: 1.80617833138] [mll=90.745+-7.422] [ks=0.170]\n",
      "13900 [D loss: 0.320808053017, acc.: 0.00%] [G loss: 1.72252106667] [mll=89.882+-8.275] [ks=0.184]\n",
      "14000 [D loss: 0.351752310991, acc.: 0.00%] [G loss: 1.70216965675] [mll=94.981+-10.381] [ks=0.314]\n",
      "10000/10000 [==============================] - 1s 74us/step\n",
      "14100 [D loss: 0.301325410604, acc.: 0.00%] [G loss: 1.79190421104] [mll=91.324+-6.687] [ks=0.185]\n",
      "14200 [D loss: 0.235567316413, acc.: 0.00%] [G loss: 1.85463154316] [mll=91.900+-9.436] [ks=0.254]\n",
      "14300 [D loss: 0.277236908674, acc.: 0.00%] [G loss: 1.93643081188] [mll=91.707+-7.574] [ks=0.293]\n",
      "14400 [D loss: 0.266274333, acc.: 0.00%] [G loss: 1.94228732586] [mll=97.333+-12.603] [ks=0.399]99]\n",
      "14500 [D loss: 0.299474239349, acc.: 0.00%] [G loss: 1.70366275311] [mll=93.846+-8.960] [ks=0.331]\n",
      "10000/10000 [==============================] - 1s 71us/step\n",
      "14600 [D loss: 0.29099419713, acc.: 0.00%] [G loss: 1.82945907116] [mll=89.199+-6.574] [ks=0.168]]\n",
      "14700 [D loss: 0.281951278448, acc.: 0.00%] [G loss: 1.87764573097] [mll=90.482+-6.085] [ks=0.151]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14800 [D loss: 0.287393778563, acc.: 0.00%] [G loss: 1.81486880779] [mll=105.805+-18.854] [ks=0.588]\n",
      "14900 [D loss: 0.299758940935, acc.: 0.00%] [G loss: 1.95523619652] [mll=91.810+-8.857] [ks=0.258]\n",
      "15000 [D loss: 0.262353152037, acc.: 0.00%] [G loss: 1.94784939289] [mll=84.936+-10.398] [ks=0.413]\n",
      "10000/10000 [==============================] - 1s 72us/step\n",
      "15100 [D loss: 0.253222554922, acc.: 0.00%] [G loss: 1.82469928265] [mll=94.284+-7.727] [ks=0.357]\n",
      "15200 [D loss: 0.311540693045, acc.: 0.00%] [G loss: 1.8434458971] [mll=87.354+-7.617] [ks=0.234]]\n",
      "15300 [D loss: 0.266329616308, acc.: 0.00%] [G loss: 1.77889358997] [mll=90.803+-5.426] [ks=0.116]\n",
      "15400 [D loss: 0.285477846861, acc.: 0.00%] [G loss: 1.72245693207] [mll=89.082+-6.761] [ks=0.227]\n",
      "15500 [D loss: 0.290999054909, acc.: 0.00%] [G loss: 1.90340411663] [mll=90.029+-6.111] [ks=0.150]\n",
      "10000/10000 [==============================] - 1s 64us/step\n",
      "15600 [D loss: 0.320180684328, acc.: 0.00%] [G loss: 1.7220441103] [mll=90.972+-6.082] [ks=0.144]]\n",
      "15700 [D loss: 0.369150429964, acc.: 0.00%] [G loss: 1.98793125153] [mll=95.449+-6.718] [ks=0.443]\n",
      "15800 [D loss: 0.264443427324, acc.: 0.00%] [G loss: 1.79948019981] [mll=87.998+-7.721] [ks=0.236]\n",
      "15900 [D loss: 0.261878222227, acc.: 0.00%] [G loss: 1.92167210579] [mll=89.191+-6.664] [ks=0.240]\n",
      "16000 [D loss: 0.28501188755, acc.: 0.00%] [G loss: 1.51788032055] [mll=87.763+-6.561] [ks=0.300]]\n",
      "10000/10000 [==============================] - 1s 61us/step\n",
      "16100 [D loss: 0.315611869097, acc.: 0.00%] [G loss: 1.88621246815] [mll=95.091+-9.049] [ks=0.408]\n",
      "16200 [D loss: 0.249355196953, acc.: 0.00%] [G loss: 1.76624405384] [mll=89.247+-8.614] [ks=0.206]\n",
      "16300 [D loss: 0.262916296721, acc.: 0.00%] [G loss: 1.78938364983] [mll=89.475+-7.211] [ks=0.172]\n",
      "16400 [D loss: 0.259668141603, acc.: 0.00%] [G loss: 1.80745148659] [mll=90.808+-6.932] [ks=0.143]\n",
      "16500 [D loss: 0.287922054529, acc.: 0.00%] [G loss: 1.74717926979] [mll=92.110+-9.843] [ks=0.254]\n",
      "10000/10000 [==============================] - 1s 62us/step\n",
      "16600 [D loss: 0.253576487303, acc.: 0.00%] [G loss: 1.79514706135] [mll=92.225+-8.810] [ks=0.266]\n",
      "16700 [D loss: 0.249166235328, acc.: 0.00%] [G loss: 1.83645534515] [mll=93.577+-8.077] [ks=0.320]\n",
      "16800 [D loss: 0.271773070097, acc.: 0.00%] [G loss: 2.00879192352] [mll=91.458+-7.109] [ks=0.219]\n",
      "16900 [D loss: 0.29475876689, acc.: 0.00%] [G loss: 1.71447455883] [mll=91.609+-7.667] [ks=0.217]]\n",
      "17000 [D loss: 0.317086845636, acc.: 0.00%] [G loss: 1.70532393456] [mll=90.674+-7.308] [ks=0.152]\n",
      "10000/10000 [==============================] - 1s 62us/step\n",
      "17100 [D loss: 0.293195217848, acc.: 0.00%] [G loss: 1.71960234642] [mll=92.893+-6.588] [ks=0.273]\n",
      "17200 [D loss: 0.276827841997, acc.: 0.00%] [G loss: 1.78069698811] [mll=87.475+-7.764] [ks=0.318]\n",
      "17300 [D loss: 0.230865105987, acc.: 0.00%] [G loss: 1.85144603252] [mll=90.358+-8.143] [ks=0.165]\n",
      "17400 [D loss: 0.286805182695, acc.: 0.00%] [G loss: 1.63794314861] [mll=90.568+-7.431] [ks=0.169]\n",
      "17500 [D loss: 0.276988714933, acc.: 0.00%] [G loss: 1.84096980095] [mll=90.440+-6.259] [ks=0.132]\n",
      "10000/10000 [==============================] - 1s 70us/step\n",
      "17600 [D loss: 0.292542904615, acc.: 0.00%] [G loss: 1.69201827049] [mll=91.705+-5.338] [ks=0.156]\n",
      "17700 [D loss: 0.28852507472, acc.: 0.00%] [G loss: 1.80617260933] [mll=89.390+-7.890] [ks=0.194]]\n",
      "17800 [D loss: 0.265225321054, acc.: 0.00%] [G loss: 1.8054074049] [mll=98.798+-12.676] [ks=0.494]]\n",
      "17900 [D loss: 0.253186136484, acc.: 1.37%] [G loss: 1.99124360085] [mll=91.610+-10.082] [ks=0.236]\n",
      "18000 [D loss: 0.26055893302, acc.: 0.00%] [G loss: 1.92211711407] [mll=98.787+-20.674] [ks=0.382]]\n",
      "10000/10000 [==============================] - 1s 61us/step\n",
      "18100 [D loss: 0.247241765261, acc.: 0.00%] [G loss: 1.96899902821] [mll=93.456+-6.146] [ks=0.358]\n",
      "18200 [D loss: 0.284936279058, acc.: 0.00%] [G loss: 1.77408206463] [mll=87.960+-6.366] [ks=0.283]\n",
      "18300 [D loss: 0.305531531572, acc.: 0.00%] [G loss: 1.93356978893] [mll=99.083+-16.891] [ks=0.367]\n",
      "18400 [D loss: 0.290592700243, acc.: 0.00%] [G loss: 1.83318138123] [mll=89.172+-8.097] [ks=0.239]\n",
      "18500 [D loss: 0.303612262011, acc.: 0.00%] [G loss: 1.70025753975] [mll=86.410+-9.011] [ks=0.252]\n",
      "10000/10000 [==============================] - 1s 68us/step\n",
      "18600 [D loss: 0.272361963987, acc.: 0.00%] [G loss: 2.31170868874] [mll=89.949+-8.851] [ks=0.167]\n",
      "18700 [D loss: 0.273033469915, acc.: 0.00%] [G loss: 1.84801471233] [mll=92.709+-11.130] [ks=0.377]\n",
      "18800 [D loss: 0.299080252647, acc.: 0.00%] [G loss: 1.7637540102] [mll=91.147+-8.279] [ks=0.162]]\n",
      "18900 [D loss: 0.271159887314, acc.: 0.00%] [G loss: 1.95704174042] [mll=90.750+-7.665] [ks=0.162]\n",
      "19000 [D loss: 0.325106590986, acc.: 0.00%] [G loss: 1.65928065777] [mll=91.226+-8.900] [ks=0.210]\n",
      "10000/10000 [==============================] - 1s 73us/step\n",
      "19100 [D loss: 0.260841459036, acc.: 0.00%] [G loss: 2.22112965584] [mll=91.397+-6.331] [ks=0.147]\n",
      "19200 [D loss: 0.277423888445, acc.: 0.00%] [G loss: 1.75117504597] [mll=103.246+-20.687] [ks=0.483]\n",
      "19300 [D loss: 0.305439263582, acc.: 0.00%] [G loss: 1.90904998779] [mll=91.497+-7.821] [ks=0.224]\n",
      "19400 [D loss: 0.250091642141, acc.: 0.00%] [G loss: 1.82995200157] [mll=90.678+-9.711] [ks=0.207]\n",
      "19500 [D loss: 7.97119188309, acc.: 0.00%] [G loss: nan] [mll=89.945+-8.831] [ks=0.182] [ks=0.182]\n",
      "10000/10000 [==============================] - 1s 78us/step\n",
      "19600 [D loss: 7.97119188309, acc.: 0.00%] [G loss: nan] [mll=nan+-nan] [ks=1.000]\n",
      "19700 [D loss: 7.97119188309, acc.: 0.00%] [G loss: nan] [mll=nan+-nan] [ks=1.000]\n",
      "19800 [D loss: 7.97119188309, acc.: 0.00%] [G loss: nan] [mll=nan+-nan] [ks=1.000]\n",
      "19900 [D loss: 7.97119188309, acc.: 0.00%] [G loss: nan] [mll=nan+-nan] [ks=1.000]\n",
      "20000 [D loss: 7.97119188309, acc.: 0.00%] [G loss: nan] [mll=nan+-nan] [ks=1.000]\n",
      "10000/10000 [==============================] - 1s 73us/step\n",
      "20100 [D loss: 7.97119188309, acc.: 0.00%] [G loss: nan] [mll=nan+-nan] [ks=1.000]\n",
      "20200 [D loss: 7.97119188309, acc.: 0.00%] [G loss: nan] [mll=nan+-nan] [ks=1.000]\n",
      "20300 [D loss: 7.97119188309, acc.: 0.00%] [G loss: nan] [mll=nan+-nan] [ks=1.000]\n",
      "20400 [D loss: 7.97119188309, acc.: 0.00%] [G loss: nan] [mll=nan+-nan] [ks=1.000]\n",
      "20479 [D loss: 7.97119188309, acc.: 0.00%] [G loss: nan] [mll=nan+-nan] [ks=1.000]BREAKING because disc/gen loss has remained the same for 1001/0 epochs!\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_68 (InputLayer)           (None, 17)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_398 (Dense)               (None, 128)          2304        input_68[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_352 (LeakyReLU)     (None, 128)          0           dense_398[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_399 (Dense)               (None, 128)          16512       leaky_re_lu_352[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_353 (LeakyReLU)     (None, 128)          0           dense_399[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_400 (Dense)               (None, 256)          33024       leaky_re_lu_353[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_354 (LeakyReLU)     (None, 256)          0           dense_400[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_401 (Dense)               (None, 256)          65792       leaky_re_lu_354[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_355 (LeakyReLU)     (None, 256)          0           dense_401[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_402 (Dense)               (None, 128)          32896       leaky_re_lu_355[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_356 (LeakyReLU)     (None, 128)          0           dense_402[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_403 (Dense)               (None, 64)           8256        leaky_re_lu_356[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_357 (LeakyReLU)     (None, 64)           0           dense_403[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_404 (Dense)               (None, 32)           2080        leaky_re_lu_357[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_358 (LeakyReLU)     (None, 32)           0           dense_404[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_405 (Dense)               (None, 16)           528         leaky_re_lu_358[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_359 (LeakyReLU)     (None, 16)           0           dense_405[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_406 (Dense)               (None, 8)            136         leaky_re_lu_359[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_360 (LeakyReLU)     (None, 8)            0           dense_406[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_407 (Dense)               (None, 1)            9           leaky_re_lu_360[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_76 (Lambda)              (None, 1)            0           input_68[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_77 (Lambda)              (None, 1)            0           input_68[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_78 (Lambda)              (None, 1)            0           input_68[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_26 (Concatenate)    (None, 4)            0           dense_407[0][0]                  \n",
      "                                                                 lambda_76[0][0]                  \n",
      "                                                                 lambda_77[0][0]                  \n",
      "                                                                 lambda_78[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 161,537\n",
      "Trainable params: 161,537\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Discriminator params: 161537\n",
      "Generator params: 340497\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_69 (InputLayer)        (None, 17)                0         \n",
      "_________________________________________________________________\n",
      "dense_408 (Dense)            (None, 64)                1152      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_361 (LeakyReLU)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_409 (Dense)            (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_362 (LeakyReLU)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_410 (Dense)            (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_363 (LeakyReLU)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_411 (Dense)            (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_364 (LeakyReLU)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_412 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_365 (LeakyReLU)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_413 (Dense)            (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_366 (LeakyReLU)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_414 (Dense)            (None, 17)                2193      \n",
      "=================================================================\n",
      "Total params: 340,497\n",
      "Trainable params: 340,497\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 [D loss: 0.178703799844, acc.: 0.00%] [G loss: 1.83906066418] [mll=-1.000+--1.000] [ks=999.000]]\n",
      "200 [D loss: 0.221349328756, acc.: 0.00%] [G loss: 1.66836082935] [mll=24.604+-3.069] [ks=0.999]]]\n",
      "300 [D loss: 0.132805600762, acc.: 0.00%] [G loss: 3.12910056114] [mll=83.481+-10.209] [ks=0.430]]\n",
      "400 [D loss: 0.10655515641, acc.: 0.00%] [G loss: 3.77343773842] [mll=100.794+-11.797] [ks=0.592]]]\n",
      "500 [D loss: 0.0459128469229, acc.: 0.00%] [G loss: 4.37460660934] [mll=70.682+-7.914] [ks=0.815]\n",
      "10000/10000 [==============================] - 1s 80us/step\n",
      "600 [D loss: 0.0852534770966, acc.: 0.00%] [G loss: 5.30476617813] [mll=72.021+-7.812] [ks=0.800]\n",
      "700 [D loss: 0.0359251052141, acc.: 0.00%] [G loss: 4.91199827194] [mll=72.393+-7.672] [ks=0.799]\n",
      "800 [D loss: 0.0370147563517, acc.: 0.00%] [G loss: 4.6242351532] [mll=69.851+-7.385] [ks=0.840]]\n",
      "900 [D loss: 0.0638529360294, acc.: 0.00%] [G loss: 3.53294038773] [mll=63.762+-6.337] [ks=0.904]\n",
      "1000 [D loss: 0.0338594950736, acc.: 0.00%] [G loss: 5.26432561874] [mll=53.847+-5.267] [ks=0.949]\n",
      "10000/10000 [==============================] - 1s 68us/step\n",
      "1100 [D loss: 0.0789986327291, acc.: 0.00%] [G loss: 4.51382637024] [mll=59.814+-5.539] [ks=0.929]\n",
      "1200 [D loss: 0.0243235267699, acc.: 0.00%] [G loss: 4.14619874954] [mll=nan+-nan] [ks=1.000]]\n",
      "1300 [D loss: 0.0934840515256, acc.: 50.00%] [G loss: 4.40708112717] [mll=87.201+-7.598] [ks=0.295]\n",
      "1400 [D loss: 0.0795511081815, acc.: 0.00%] [G loss: 4.31055641174] [mll=nan+-nan] [ks=1.000]]\n",
      "1500 [D loss: 0.0640507265925, acc.: 0.00%] [G loss: 3.08959531784] [mll=81.133+-6.631] [ks=0.607]\n",
      "10000/10000 [==============================] - 1s 69us/step\n",
      "1600 [D loss: 0.170715659857, acc.: 0.00%] [G loss: 4.03268957138] [mll=70.432+-5.500] [ks=0.870]]\n",
      "1700 [D loss: 0.021219547838, acc.: 0.00%] [G loss: 5.39496231079] [mll=88.809+-6.957] [ks=0.204]]]\n",
      "1800 [D loss: 0.15175151825, acc.: 0.00%] [G loss: 2.74095678329] [mll=55.239+-3.697] [ks=0.957]]]]\n",
      "1900 [D loss: 0.133102193475, acc.: 0.00%] [G loss: 4.46382856369] [mll=132.090+-10.787] [ks=0.969]]\n",
      "2000 [D loss: 0.0916328877211, acc.: 0.00%] [G loss: 3.61162805557] [mll=71.652+-5.292] [ks=0.864]\n",
      "10000/10000 [==============================] - 1s 64us/step\n",
      "2100 [D loss: 0.12773360312, acc.: 0.00%] [G loss: 2.95414638519] [mll=101.678+-7.942] [ks=0.708]]]]\n",
      "2200 [D loss: 0.0845153182745, acc.: 0.00%] [G loss: 3.19271254539] [mll=96.176+-6.767] [ks=0.507]\n",
      "2300 [D loss: 0.0949659347534, acc.: 0.00%] [G loss: 3.19757866859] [mll=94.660+-7.074] [ks=0.419]\n",
      "2400 [D loss: 0.0161567125469, acc.: 0.00%] [G loss: 5.15094327927] [mll=73.847+-4.276] [ks=0.863]\n",
      "2500 [D loss: 0.144243642688, acc.: 50.00%] [G loss: 4.89196062088] [mll=67.461+-3.802] [ks=0.916]]\n",
      "10000/10000 [==============================] - 1s 75us/step\n",
      "2600 [D loss: 0.0735405758023, acc.: 0.00%] [G loss: 3.37211751938] [mll=nan+-nan] [ks=1.000]]\n",
      "2700 [D loss: 0.0975899174809, acc.: 0.00%] [G loss: 3.38902997971] [mll=93.728+-5.892] [ks=0.373]\n",
      "2800 [D loss: 0.143359169364, acc.: 0.00%] [G loss: 3.96219778061] [mll=91.961+-6.125] [ks=0.254]]\n",
      "2900 [D loss: 7.97134828568, acc.: 0.00%] [G loss: 0.0034344070591] [mll=70.557+-3.928] [ks=0.896]]\n",
      "3000 [D loss: 7.97121143341, acc.: 0.00%] [G loss: 0.00754681276157] [mll=87.806+-4.526] [ks=0.286]\n",
      "10000/10000 [==============================] - 1s 65us/step\n",
      "3100 [D loss: 7.97119188309, acc.: 0.00%] [G loss: 0.00568407727405] [mll=82.405+-3.615] [ks=0.699]\n",
      "3200 [D loss: 7.97119522095, acc.: 0.00%] [G loss: 0.00460980879143] [mll=84.251+-3.434] [ks=0.619]\n",
      "3300 [D loss: 7.97119188309, acc.: 0.00%] [G loss: 0.00459234183654] [mll=85.192+-3.375] [ks=0.560]\n",
      "3400 [D loss: 7.97119235992, acc.: 0.00%] [G loss: 0.00424117734656] [mll=85.358+-3.473] [ks=0.544]\n",
      "3500 [D loss: 7.97119188309, acc.: 0.00%] [G loss: 0.00494888983667] [mll=86.011+-3.498] [ks=0.490]\n",
      "10000/10000 [==============================] - 1s 73us/step\n",
      "3600 [D loss: 7.97119188309, acc.: 0.00%] [G loss: 0.00386880361475] [mll=84.594+-3.459] [ks=0.591]\n",
      "3700 [D loss: 7.97119188309, acc.: 0.00%] [G loss: 0.00331087573431] [mll=86.581+-3.578] [ks=0.437]\n",
      "3800 [D loss: 7.97119188309, acc.: 0.00%] [G loss: 0.0031847411301] [mll=91.364+-3.954] [ks=0.154]]\n",
      "3900 [D loss: 7.97119188309, acc.: 0.00%] [G loss: 0.00298793939874] [mll=90.848+-3.929] [ks=0.112]\n",
      "4000 [D loss: 7.97119188309, acc.: 0.00%] [G loss: 0.00327805662528] [mll=89.815+-3.854] [ks=0.125]\n",
      "10000/10000 [==============================] - 1s 72us/step\n",
      "4017 [D loss: 7.97119188309, acc.: 0.00%] [G loss: 0.00299825752154] [mll=91.274+-3.891] [ks=0.144]BREAKING because disc/gen loss has remained the same for 1001/27 epochs!\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_71 (InputLayer)           (None, 17)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_415 (Dense)               (None, 128)          2304        input_71[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_367 (LeakyReLU)     (None, 128)          0           dense_415[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_416 (Dense)               (None, 128)          16512       leaky_re_lu_367[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_368 (LeakyReLU)     (None, 128)          0           dense_416[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_417 (Dense)               (None, 256)          33024       leaky_re_lu_368[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_369 (LeakyReLU)     (None, 256)          0           dense_417[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_418 (Dense)               (None, 256)          65792       leaky_re_lu_369[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_370 (LeakyReLU)     (None, 256)          0           dense_418[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_419 (Dense)               (None, 128)          32896       leaky_re_lu_370[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_371 (LeakyReLU)     (None, 128)          0           dense_419[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_420 (Dense)               (None, 64)           8256        leaky_re_lu_371[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_372 (LeakyReLU)     (None, 64)           0           dense_420[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_421 (Dense)               (None, 32)           2080        leaky_re_lu_372[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_373 (LeakyReLU)     (None, 32)           0           dense_421[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_422 (Dense)               (None, 16)           528         leaky_re_lu_373[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_374 (LeakyReLU)     (None, 16)           0           dense_422[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_423 (Dense)               (None, 8)            136         leaky_re_lu_374[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_375 (LeakyReLU)     (None, 8)            0           dense_423[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_424 (Dense)               (None, 1)            9           leaky_re_lu_375[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_79 (Lambda)              (None, 1)            0           input_71[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_80 (Lambda)              (None, 1)            0           input_71[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_81 (Lambda)              (None, 1)            0           input_71[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_27 (Concatenate)    (None, 4)            0           dense_424[0][0]                  \n",
      "                                                                 lambda_79[0][0]                  \n",
      "                                                                 lambda_80[0][0]                  \n",
      "                                                                 lambda_81[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 161,537\n",
      "Trainable params: 161,537\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Discriminator params: 161537\n",
      "Generator params: 340497\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_72 (InputLayer)        (None, 17)                0         \n",
      "_________________________________________________________________\n",
      "dense_425 (Dense)            (None, 64)                1152      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_376 (LeakyReLU)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_426 (Dense)            (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_377 (LeakyReLU)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_427 (Dense)            (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_378 (LeakyReLU)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_428 (Dense)            (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_379 (LeakyReLU)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_429 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_380 (LeakyReLU)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_430 (Dense)            (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_381 (LeakyReLU)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_431 (Dense)            (None, 17)                2193      \n",
      "=================================================================\n",
      "Total params: 340,497\n",
      "Trainable params: 340,497\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 [D loss: 0.360582083464, acc.: 0.00%] [G loss: 2.01881217957] [mll=-1.000+--1.000] [ks=999.000]\n",
      "200 [D loss: 0.0408596396446, acc.: 0.00%] [G loss: 3.97516822815] [mll=10.877+-1.200] [ks=1.000]]\n",
      "300 [D loss: 0.0450969040394, acc.: 0.00%] [G loss: 4.57662582397] [mll=13.610+-1.116] [ks=1.000]\n",
      "400 [D loss: 2.35735487938, acc.: 0.00%] [G loss: 0.815739512444] [mll=40.133+-3.486] [ks=0.990]]\n",
      "500 [D loss: 0.140237808228, acc.: 0.00%] [G loss: 3.47895073891] [mll=32.028+-1.564] [ks=0.998]]\n",
      "10000/10000 [==============================] - 1s 67us/step\n",
      "600 [D loss: 0.135052070022, acc.: 0.00%] [G loss: 3.15589880943] [mll=56.836+-3.755] [ks=0.953]]]\n",
      "700 [D loss: 0.134918883443, acc.: 0.00%] [G loss: 7.32303524017] [mll=51.290+-3.275] [ks=0.967]]\n",
      "800 [D loss: 0.037808354944, acc.: 0.00%] [G loss: 4.37560606003] [mll=40.350+-1.576] [ks=0.995]]\n",
      "900 [D loss: 0.0554063357413, acc.: 0.00%] [G loss: 3.51362395287] [mll=81.999+-5.105] [ks=0.632]]\n",
      "1000 [D loss: 0.148665860295, acc.: 0.00%] [G loss: 2.3539659977] [mll=83.622+-5.167] [ks=0.551]]\n",
      "10000/10000 [==============================] - 1s 77us/step\n",
      "1100 [D loss: 0.0446978248656, acc.: 0.00%] [G loss: 4.82795858383] [mll=77.826+-4.083] [ks=0.808]\n",
      "1200 [D loss: 0.0476259551942, acc.: 0.00%] [G loss: 4.07732391357] [mll=79.665+-4.060] [ks=0.762]\n",
      "1300 [D loss: 0.0614486597478, acc.: 0.00%] [G loss: 3.95105791092] [mll=69.333+-3.615] [ks=0.907]]\n",
      "1400 [D loss: 0.0586197078228, acc.: 0.00%] [G loss: 5.18857955933] [mll=77.453+-3.505] [ks=0.833]\n",
      "1500 [D loss: 0.0898877978325, acc.: 0.00%] [G loss: 5.35099363327] [mll=73.652+-3.797] [ks=0.873]]\n",
      "10000/10000 [==============================] - 1s 92us/step\n",
      "1600 [D loss: 0.0326789878309, acc.: 0.00%] [G loss: 4.80316591263] [mll=76.512+-3.286] [ks=0.853]\n",
      "1700 [D loss: 1.03137683868, acc.: 0.00%] [G loss: 0.772277951241] [mll=85.870+-3.935] [ks=0.473]]]\n",
      "1800 [D loss: 0.166367277503, acc.: 0.00%] [G loss: 3.80930733681] [mll=86.167+-10.438] [ks=0.323]]]\n",
      "1900 [D loss: 0.0851784497499, acc.: 0.00%] [G loss: 3.38056993484] [mll=82.011+-7.922] [ks=0.530]\n",
      "2000 [D loss: 0.127110764384, acc.: 0.00%] [G loss: 3.24310898781] [mll=40.106+-3.145] [ks=0.992]]\n",
      "10000/10000 [==============================] - 1s 79us/step\n",
      "2100 [D loss: 0.121880814433, acc.: 0.00%] [G loss: 4.68545150757] [mll=39.456+-1.302] [ks=0.995]]\n",
      "2200 [D loss: 0.137975439429, acc.: 0.00%] [G loss: 3.62331986427] [mll=11.035+-2.588] [ks=1.000]]\n",
      "2300 [D loss: 0.0238939076662, acc.: 0.00%] [G loss: 4.30526924133] [mll=73.827+-5.396] [ks=0.833]\n",
      "2400 [D loss: 0.067762054503, acc.: 0.00%] [G loss: 4.69736671448] [mll=89.258+-5.267] [ks=0.180]]\n",
      "2500 [D loss: 0.0647263899446, acc.: 0.00%] [G loss: 4.06360816956] [mll=62.879+-2.827] [ks=0.942]\n",
      "10000/10000 [==============================] - 1s 71us/step\n",
      "2600 [D loss: 0.163389325142, acc.: 0.00%] [G loss: 4.38442611694] [mll=55.285+-1.876] [ks=0.967]]\n",
      "2700 [D loss: 0.0361593998969, acc.: 0.00%] [G loss: 5.64791822433] [mll=75.955+-2.557] [ks=0.877]\n",
      "2800 [D loss: 0.145441427827, acc.: 0.00%] [G loss: 3.50795888901] [mll=65.130+-2.270] [ks=0.940]]\n",
      "2900 [D loss: 0.0643749311566, acc.: 0.00%] [G loss: 4.43990135193] [mll=91.429+-3.174] [ks=0.148]\n",
      "3000 [D loss: 0.0519474223256, acc.: 0.00%] [G loss: 4.53691148758] [mll=82.377+-2.910] [ks=0.731]\n",
      "10000/10000 [==============================] - 1s 84us/step\n",
      "3100 [D loss: 0.185534149408, acc.: 0.00%] [G loss: 2.31456851959] [mll=74.730+-2.951] [ks=0.875]]\n",
      "3200 [D loss: 0.189604744315, acc.: 3.32%] [G loss: 3.91066169739] [mll=69.461+-15.945] [ks=0.691]]\n",
      "3300 [D loss: 0.312194675207, acc.: 0.00%] [G loss: 3.58467531204] [mll=95.658+-20.931] [ks=0.544]]\n",
      "3400 [D loss: 0.224605426192, acc.: 0.00%] [G loss: 2.30250668526] [mll=52.593+-9.339] [ks=0.894]]\n",
      "3500 [D loss: 0.159015417099, acc.: 0.00%] [G loss: 2.77686858177] [mll=89.450+-8.692] [ks=0.239]]\n",
      "10000/10000 [==============================] - 1s 85us/step\n",
      "3600 [D loss: 0.22650757432, acc.: 0.00%] [G loss: 2.71377158165] [mll=93.202+-7.301] [ks=0.288]]]\n",
      "3700 [D loss: 0.302155226469, acc.: 0.00%] [G loss: 2.18339538574] [mll=80.988+-9.104] [ks=0.531]]\n",
      "3800 [D loss: 0.244936212897, acc.: 0.00%] [G loss: 2.01805400848] [mll=92.220+-6.546] [ks=0.242]\n",
      "3900 [D loss: 0.256621807814, acc.: 0.00%] [G loss: 1.91337001324] [mll=91.560+-7.844] [ks=0.216]]\n",
      "4000 [D loss: 0.303230673075, acc.: 0.00%] [G loss: 2.33988022804] [mll=101.951+-18.679] [ks=0.580]\n",
      "10000/10000 [==============================] - 1s 76us/step\n",
      "4100 [D loss: 0.253279566765, acc.: 0.00%] [G loss: 2.10931420326] [mll=82.245+-8.647] [ks=0.538]]\n",
      "4200 [D loss: 0.294299870729, acc.: 0.00%] [G loss: 1.94863975048] [mll=90.968+-6.308] [ks=0.204]\n",
      "4300 [D loss: 0.636395394802, acc.: 0.00%] [G loss: 2.53206419945] [mll=88.831+-14.576] [ks=0.270]]\n",
      "4400 [D loss: 0.435648739338, acc.: 0.00%] [G loss: 1.54968261719] [mll=108.129+-18.437] [ks=0.666]\n",
      "4500 [D loss: 0.486597508192, acc.: 0.00%] [G loss: 1.5542974472] [mll=88.732+-9.838] [ks=0.338]]\n",
      "10000/10000 [==============================] - 1s 65us/step\n",
      "4600 [D loss: 0.478807479143, acc.: 0.00%] [G loss: 1.41781437397] [mll=96.285+-11.218] [ks=0.494]]\n",
      "4700 [D loss: 0.635055303574, acc.: 0.00%] [G loss: 1.32880604267] [mll=87.989+-10.901] [ks=0.336]\n",
      "4800 [D loss: 0.457901209593, acc.: 0.00%] [G loss: 2.13481211662] [mll=87.850+-12.921] [ks=0.364]]\n",
      "4900 [D loss: 0.829264700413, acc.: 0.00%] [G loss: 1.44840073586] [mll=92.100+-8.239] [ks=0.221]\n",
      "5000 [D loss: 0.511022984982, acc.: 0.00%] [G loss: 1.70220780373] [mll=93.256+-11.443] [ks=0.339]\n",
      "10000/10000 [==============================] - 1s 79us/step\n",
      "5100 [D loss: 7.97119188309, acc.: 0.00%] [G loss: nan] [mll=87.634+-13.043] [ks=0.249] [ks=0.249]\n",
      "5200 [D loss: 7.97119188309, acc.: 0.00%] [G loss: nan] [mll=nan+-nan] [ks=1.000]\n",
      "5300 [D loss: 7.97119188309, acc.: 0.00%] [G loss: nan] [mll=nan+-nan] [ks=1.000]\n",
      "5400 [D loss: 7.97119188309, acc.: 0.00%] [G loss: nan] [mll=nan+-nan] [ks=1.000]\n",
      "5500 [D loss: 7.97119188309, acc.: 0.00%] [G loss: nan] [mll=nan+-nan] [ks=1.000]\n",
      "10000/10000 [==============================] - 1s 74us/step\n",
      "5600 [D loss: 7.97119188309, acc.: 0.00%] [G loss: nan] [mll=nan+-nan] [ks=1.000]\n",
      "5700 [D loss: 7.97119188309, acc.: 0.00%] [G loss: nan] [mll=nan+-nan] [ks=1.000]\n",
      "5800 [D loss: 7.97119188309, acc.: 0.00%] [G loss: nan] [mll=nan+-nan] [ks=1.000]\n",
      "5900 [D loss: 7.97119188309, acc.: 0.00%] [G loss: nan] [mll=nan+-nan] [ks=1.000]\n",
      "6000 [D loss: 7.97119188309, acc.: 0.00%] [G loss: nan] [mll=nan+-nan] [ks=1.000]\n",
      "10000/10000 [==============================] - 1s 66us/step\n",
      "6066 [D loss: 7.97119188309, acc.: 0.00%] [G loss: nan] [mll=nan+-nan] [ks=1.000]BREAKING because disc/gen loss has remained the same for 1001/0 epochs!\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_74 (InputLayer)           (None, 17)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_432 (Dense)               (None, 128)          2304        input_74[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_382 (LeakyReLU)     (None, 128)          0           dense_432[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_433 (Dense)               (None, 128)          16512       leaky_re_lu_382[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_383 (LeakyReLU)     (None, 128)          0           dense_433[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_434 (Dense)               (None, 256)          33024       leaky_re_lu_383[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_384 (LeakyReLU)     (None, 256)          0           dense_434[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_435 (Dense)               (None, 256)          65792       leaky_re_lu_384[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_385 (LeakyReLU)     (None, 256)          0           dense_435[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_436 (Dense)               (None, 128)          32896       leaky_re_lu_385[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_386 (LeakyReLU)     (None, 128)          0           dense_436[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_437 (Dense)               (None, 64)           8256        leaky_re_lu_386[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_387 (LeakyReLU)     (None, 64)           0           dense_437[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_438 (Dense)               (None, 32)           2080        leaky_re_lu_387[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_388 (LeakyReLU)     (None, 32)           0           dense_438[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_439 (Dense)               (None, 16)           528         leaky_re_lu_388[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_389 (LeakyReLU)     (None, 16)           0           dense_439[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_440 (Dense)               (None, 8)            136         leaky_re_lu_389[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_390 (LeakyReLU)     (None, 8)            0           dense_440[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_441 (Dense)               (None, 1)            9           leaky_re_lu_390[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_82 (Lambda)              (None, 1)            0           input_74[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_83 (Lambda)              (None, 1)            0           input_74[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_84 (Lambda)              (None, 1)            0           input_74[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_28 (Concatenate)    (None, 4)            0           dense_441[0][0]                  \n",
      "                                                                 lambda_82[0][0]                  \n",
      "                                                                 lambda_83[0][0]                  \n",
      "                                                                 lambda_84[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 161,537\n",
      "Trainable params: 161,537\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Discriminator params: 161537\n",
      "Generator params: 340497\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_75 (InputLayer)        (None, 17)                0         \n",
      "_________________________________________________________________\n",
      "dense_442 (Dense)            (None, 64)                1152      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_391 (LeakyReLU)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_443 (Dense)            (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_392 (LeakyReLU)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_444 (Dense)            (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_393 (LeakyReLU)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_445 (Dense)            (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_394 (LeakyReLU)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_446 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_395 (LeakyReLU)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_447 (Dense)            (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_396 (LeakyReLU)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_448 (Dense)            (None, 17)                2193      \n",
      "=================================================================\n",
      "Total params: 340,497\n",
      "Trainable params: 340,497\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 [D loss: 0.139959648252, acc.: 0.00%] [G loss: 2.24145627022] [mll=-1.000+--1.000] [ks=999.000]\n",
      "200 [D loss: 0.149433895946, acc.: 50.00%] [G loss: 2.69584488869] [mll=14.630+-1.901] [ks=1.000]\n",
      "300 [D loss: 0.192955613136, acc.: 0.00%] [G loss: 3.05870223045] [mll=nan+-nan] [ks=1.000]]]\n",
      "400 [D loss: 0.0637273862958, acc.: 0.00%] [G loss: 3.46544289589] [mll=35.275+-4.406] [ks=0.993]]\n",
      "500 [D loss: 0.0876080468297, acc.: 0.00%] [G loss: 3.55296373367] [mll=72.427+-8.756] [ks=0.769]]\n",
      "10000/10000 [==============================] - 1s 77us/step\n",
      "600 [D loss: 0.167273268104, acc.: 0.00%] [G loss: 6.25400114059] [mll=67.504+-8.149] [ks=0.851]]\n",
      "700 [D loss: 0.111655957997, acc.: 0.00%] [G loss: 4.66460752487] [mll=60.269+-7.186] [ks=0.911]]]\n",
      "800 [D loss: 0.0704508349299, acc.: 0.00%] [G loss: 3.45788216591] [mll=57.419+-6.699] [ks=0.928]\n",
      "900 [D loss: 0.144363820553, acc.: 0.00%] [G loss: 3.8423063755] [mll=76.520+-8.822] [ks=0.680]]]\n",
      "1000 [D loss: 0.216369614005, acc.: 0.00%] [G loss: 2.05936694145] [mll=nan+-nan] [ks=1.000]\n",
      "10000/10000 [==============================] - 1s 74us/step\n",
      "1100 [D loss: 0.110062904656, acc.: 0.00%] [G loss: 3.09983468056] [mll=53.315+-5.478] [ks=0.950]]\n",
      "1200 [D loss: 0.117667928338, acc.: 0.00%] [G loss: 2.46470093727] [mll=53.419+-5.167] [ks=0.951]]\n",
      "1300 [D loss: 0.119039483368, acc.: 0.00%] [G loss: 3.79777312279] [mll=74.532+-7.286] [ks=0.772]]]\n",
      "1400 [D loss: 0.0766304060817, acc.: 0.00%] [G loss: 3.03263497353] [mll=61.121+-5.875] [ks=0.922]]\n",
      "1500 [D loss: 0.0606397651136, acc.: 0.00%] [G loss: 4.19957733154] [mll=74.198+-7.100] [ks=0.783]]\n",
      "10000/10000 [==============================] - 1s 105us/step\n",
      "1600 [D loss: 0.176135763526, acc.: 0.00%] [G loss: 1.74341785908] [mll=37.643+-3.083] [ks=0.994]]]\n",
      "1700 [D loss: 0.141046985984, acc.: 0.00%] [G loss: 2.77574253082] [mll=84.512+-8.472] [ks=0.412]]\n",
      "1800 [D loss: 0.261139392853, acc.: 0.00%] [G loss: 1.8898717165] [mll=148.770+-14.517] [ks=0.979]]]\n",
      "1900 [D loss: 0.106810852885, acc.: 0.00%] [G loss: 2.62072205544] [mll=101.659+-8.880] [ks=0.678]]\n",
      "2000 [D loss: 0.0490443147719, acc.: 0.00%] [G loss: 3.21092796326] [mll=82.473+-6.491] [ks=0.553]\n",
      "10000/10000 [==============================] - 1s 78us/step\n",
      "2100 [D loss: 0.107878558338, acc.: 0.00%] [G loss: 4.32891082764] [mll=71.476+-5.740] [ks=0.856]]\n",
      "2200 [D loss: 0.0747052431107, acc.: 0.00%] [G loss: 2.92317914963] [mll=55.710+-4.016] [ks=0.953]\n",
      "2300 [D loss: 0.107319585979, acc.: 0.00%] [G loss: 2.78254890442] [mll=100.668+-7.834] [ks=0.672]]\n",
      "2400 [D loss: 0.179017439485, acc.: 0.00%] [G loss: 2.43615365028] [mll=61.611+-4.873] [ks=0.923]]\n",
      "2500 [D loss: 0.181035876274, acc.: 0.00%] [G loss: 3.40297174454] [mll=72.224+-10.611] [ks=0.715]]\n",
      "10000/10000 [==============================] - 1s 77us/step\n",
      "2600 [D loss: 0.231434494257, acc.: 0.00%] [G loss: 2.55976653099] [mll=84.460+-12.189] [ks=0.475]]\n",
      "2700 [D loss: 0.137814745307, acc.: 0.00%] [G loss: 2.59705519676] [mll=62.646+-17.488] [ks=0.480]]\n",
      "2800 [D loss: 0.290491014719, acc.: 0.00%] [G loss: 2.63386893272] [mll=87.693+-6.903] [ks=0.291]\n",
      "2900 [D loss: 0.186623871326, acc.: 0.00%] [G loss: 2.57101559639] [mll=87.762+-8.375] [ks=0.311]]\n",
      "3000 [D loss: 0.248488798738, acc.: 0.00%] [G loss: 2.69698524475] [mll=83.316+-6.020] [ks=0.561]\n",
      "10000/10000 [==============================] - 1s 77us/step\n",
      "3100 [D loss: 0.298791080713, acc.: 0.00%] [G loss: 1.99932694435] [mll=101.687+-8.997] [ks=0.700]\n",
      "3200 [D loss: 0.246996968985, acc.: 0.00%] [G loss: 3.08923316002] [mll=95.658+-6.318] [ks=0.475]\n",
      "3300 [D loss: 0.261712759733, acc.: 0.00%] [G loss: 2.32295823097] [mll=104.394+-13.782] [ks=0.657]\n",
      "3400 [D loss: 0.307568222284, acc.: 0.00%] [G loss: 2.04706478119] [mll=81.252+-10.532] [ks=0.564]\n",
      "3500 [D loss: 0.273175388575, acc.: 0.00%] [G loss: 2.11077857018] [mll=99.923+-9.765] [ks=0.586]]\n",
      "10000/10000 [==============================] - 1s 63us/step\n",
      "3600 [D loss: 0.270349115133, acc.: 0.00%] [G loss: 1.86516392231] [mll=108.943+-7.138] [ks=0.901]\n",
      "3700 [D loss: 0.209070682526, acc.: 0.00%] [G loss: 2.14318728447] [mll=90.089+-5.440] [ks=0.138]\n",
      "3800 [D loss: 0.205852612853, acc.: 0.00%] [G loss: 1.81797969341] [mll=85.175+-4.923] [ks=0.481]]\n",
      "3900 [D loss: 7.97119188309, acc.: 0.00%] [G loss: nan] [mll=89.322+-5.480] [ks=0.179] [ks=0.179]\n",
      "4000 [D loss: 7.97119188309, acc.: 0.00%] [G loss: nan] [mll=nan+-nan] [ks=1.000]\n",
      "10000/10000 [==============================] - 1s 68us/step\n",
      "4100 [D loss: 7.97119188309, acc.: 0.00%] [G loss: nan] [mll=nan+-nan] [ks=1.000]\n",
      "4200 [D loss: 7.97119188309, acc.: 0.00%] [G loss: nan] [mll=nan+-nan] [ks=1.000]\n",
      "4300 [D loss: 7.97119188309, acc.: 0.00%] [G loss: nan] [mll=nan+-nan] [ks=1.000]\n",
      "4400 [D loss: 7.97119188309, acc.: 0.00%] [G loss: nan] [mll=nan+-nan] [ks=1.000]\n",
      "4500 [D loss: 7.97119188309, acc.: 0.00%] [G loss: nan] [mll=nan+-nan] [ks=1.000]\n",
      "10000/10000 [==============================] - 1s 81us/step\n",
      "4600 [D loss: 7.97119188309, acc.: 0.00%] [G loss: nan] [mll=nan+-nan] [ks=1.000]\n",
      "4700 [D loss: 7.97119188309, acc.: 0.00%] [G loss: nan] [mll=nan+-nan] [ks=1.000]\n",
      "4800 [D loss: 7.97119188309, acc.: 0.00%] [G loss: nan] [mll=nan+-nan] [ks=1.000]\n",
      "4890 [D loss: 7.97119188309, acc.: 0.00%] [G loss: nan] [mll=nan+-nan] [ks=1.000]BREAKING because disc/gen loss has remained the same for 1001/0 epochs!\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_77 (InputLayer)           (None, 17)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_449 (Dense)               (None, 128)          2304        input_77[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_397 (LeakyReLU)     (None, 128)          0           dense_449[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_450 (Dense)               (None, 128)          16512       leaky_re_lu_397[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_398 (LeakyReLU)     (None, 128)          0           dense_450[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_451 (Dense)               (None, 256)          33024       leaky_re_lu_398[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_399 (LeakyReLU)     (None, 256)          0           dense_451[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_452 (Dense)               (None, 256)          65792       leaky_re_lu_399[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_400 (LeakyReLU)     (None, 256)          0           dense_452[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_453 (Dense)               (None, 128)          32896       leaky_re_lu_400[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_401 (LeakyReLU)     (None, 128)          0           dense_453[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_454 (Dense)               (None, 64)           8256        leaky_re_lu_401[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_402 (LeakyReLU)     (None, 64)           0           dense_454[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_455 (Dense)               (None, 32)           2080        leaky_re_lu_402[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_403 (LeakyReLU)     (None, 32)           0           dense_455[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_456 (Dense)               (None, 16)           528         leaky_re_lu_403[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_404 (LeakyReLU)     (None, 16)           0           dense_456[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_457 (Dense)               (None, 8)            136         leaky_re_lu_404[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_405 (LeakyReLU)     (None, 8)            0           dense_457[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_458 (Dense)               (None, 1)            9           leaky_re_lu_405[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_85 (Lambda)              (None, 1)            0           input_77[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_86 (Lambda)              (None, 1)            0           input_77[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_87 (Lambda)              (None, 1)            0           input_77[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_29 (Concatenate)    (None, 4)            0           dense_458[0][0]                  \n",
      "                                                                 lambda_85[0][0]                  \n",
      "                                                                 lambda_86[0][0]                  \n",
      "                                                                 lambda_87[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 161,537\n",
      "Trainable params: 161,537\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Discriminator params: 161537\n",
      "Generator params: 340497\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_78 (InputLayer)        (None, 17)                0         \n",
      "_________________________________________________________________\n",
      "dense_459 (Dense)            (None, 64)                1152      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_406 (LeakyReLU)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_460 (Dense)            (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_407 (LeakyReLU)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_461 (Dense)            (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_408 (LeakyReLU)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_462 (Dense)            (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_409 (LeakyReLU)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_463 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_410 (LeakyReLU)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_464 (Dense)            (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_411 (LeakyReLU)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_465 (Dense)            (None, 17)                2193      \n",
      "=================================================================\n",
      "Total params: 340,497\n",
      "Trainable params: 340,497\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 [D loss: 0.0309668499976, acc.: 50.00%] [G loss: 4.58363676071] [mll=-1.000+--1.000] [ks=999.000]\n",
      "200 [D loss: 0.15606893599, acc.: 0.00%] [G loss: 2.00979542732] [mll=nan+-nan] [ks=1.000]]]]\n",
      "300 [D loss: 0.156094834208, acc.: 0.00%] [G loss: 5.87005329132] [mll=50.957+-5.857] [ks=0.953]]]\n",
      "400 [D loss: 0.160056337714, acc.: 0.00%] [G loss: 4.79415464401] [mll=17.439+-1.747] [ks=1.000]]]\n",
      "500 [D loss: 0.0301999878138, acc.: 0.00%] [G loss: 6.07837486267] [mll=68.396+-6.616] [ks=0.869]\n",
      "10000/10000 [==============================] - 1s 68us/step\n",
      "600 [D loss: 0.148609101772, acc.: 0.00%] [G loss: 3.31045174599] [mll=65.118+-6.046] [ks=0.901]]\n",
      "700 [D loss: 0.0680643841624, acc.: 50.00%] [G loss: 3.89958143234] [mll=79.815+-7.027] [ks=0.639]\n",
      "800 [D loss: 0.327740460634, acc.: 0.00%] [G loss: 2.21188163757] [mll=nan+-nan] [ks=1.000]]]\n",
      "900 [D loss: 0.0563855804503, acc.: 0.00%] [G loss: 3.38314223289] [mll=87.052+-8.124] [ks=0.294]\n",
      "1000 [D loss: 0.132346794009, acc.: 0.00%] [G loss: 3.8292927742] [mll=89.473+-7.649] [ks=0.173]]\n",
      "10000/10000 [==============================] - 1s 71us/step\n",
      "1100 [D loss: 0.167651548982, acc.: 0.00%] [G loss: 8.79959487915] [mll=76.150+-6.330] [ks=0.767]]\n",
      "1200 [D loss: 0.143370226026, acc.: 0.00%] [G loss: 3.00942993164] [mll=60.763+-4.963] [ks=0.931]]\n",
      "1300 [D loss: 0.0657917931676, acc.: 0.00%] [G loss: 3.97913694382] [mll=65.193+-4.982] [ks=0.912]]\n",
      "1400 [D loss: 0.0366277955472, acc.: 0.00%] [G loss: 4.10659408569] [mll=43.178+-2.718] [ks=0.988]\n",
      "1500 [D loss: 0.0296541042626, acc.: 0.00%] [G loss: 4.81180000305] [mll=72.071+-4.511] [ks=0.873]]\n",
      "10000/10000 [==============================] - 1s 75us/step\n",
      "1600 [D loss: 0.101168103516, acc.: 0.00%] [G loss: 4.74967765808] [mll=71.967+-4.600] [ks=0.872]]\n",
      "1700 [D loss: 0.0320408008993, acc.: 0.00%] [G loss: 4.01673173904] [mll=75.702+-5.037] [ks=0.814]]\n",
      "1800 [D loss: 0.0523283407092, acc.: 0.00%] [G loss: 5.02663612366] [mll=49.474+-2.895] [ks=0.973]\n",
      "1900 [D loss: 0.0968950688839, acc.: 0.00%] [G loss: 4.73552846909] [mll=50.982+-2.953] [ks=0.969]]\n",
      "2000 [D loss: 0.10026025027, acc.: 0.00%] [G loss: 2.46952986717] [mll=46.354+-2.503] [ks=0.982]]]\n",
      "10000/10000 [==============================] - 1s 74us/step\n",
      "2100 [D loss: 0.174886107445, acc.: 0.00%] [G loss: 2.43037700653] [mll=87.054+-5.522] [ks=0.348]]\n",
      "2200 [D loss: 0.311175346375, acc.: 0.00%] [G loss: 1.79806876183] [mll=94.052+-5.931] [ks=0.372]\n",
      "2300 [D loss: 0.204287782311, acc.: 0.00%] [G loss: 2.271930933] [mll=159.478+-15.078] [ks=0.988]8]\n",
      "2400 [D loss: 0.171698704362, acc.: 0.00%] [G loss: 3.36497092247] [mll=84.786+-8.782] [ks=0.470]]\n",
      "2500 [D loss: 0.209669619799, acc.: 0.00%] [G loss: 2.34800934792] [mll=74.827+-9.274] [ks=0.698]]\n",
      "10000/10000 [==============================] - 1s 87us/step\n",
      "2600 [D loss: 0.173304691911, acc.: 0.00%] [G loss: 2.26230549812] [mll=79.991+-7.845] [ks=0.605]]\n",
      "2700 [D loss: 0.16129642725, acc.: 0.00%] [G loss: 2.72468400002] [mll=74.442+-5.350] [ks=0.816]]]\n",
      "2800 [D loss: 0.242324262857, acc.: 0.00%] [G loss: 2.07630705833] [mll=87.532+-9.917] [ks=0.264]]\n",
      "2900 [D loss: 0.331209331751, acc.: 0.00%] [G loss: 2.17172074318] [mll=78.947+-8.722] [ks=0.610]\n",
      "3000 [D loss: 0.361307680607, acc.: 0.00%] [G loss: 1.2895065546] [mll=95.268+-8.992] [ks=0.439]]\n",
      "10000/10000 [==============================] - 1s 73us/step\n",
      "3100 [D loss: 0.330292373896, acc.: 0.00%] [G loss: 1.78742706776] [mll=106.240+-12.994] [ks=0.741]\n",
      "3200 [D loss: 0.375309705734, acc.: 0.00%] [G loss: 1.97309803963] [mll=87.309+-11.950] [ks=0.353]\n",
      "3300 [D loss: 0.367497235537, acc.: 0.00%] [G loss: 1.82841420174] [mll=89.592+-10.861] [ks=0.216]\n",
      "3400 [D loss: 0.344367474318, acc.: 0.00%] [G loss: 1.74594390392] [mll=85.276+-10.516] [ks=0.372]\n",
      "3500 [D loss: 0.530257701874, acc.: 0.00%] [G loss: 1.75984966755] [mll=78.467+-8.305] [ks=0.661]\n",
      "10000/10000 [==============================] - 1s 71us/step\n",
      "3600 [D loss: 0.463340550661, acc.: 0.00%] [G loss: 1.73180985451] [mll=85.951+-11.498] [ks=0.344]]\n",
      "3700 [D loss: 0.655868530273, acc.: 0.00%] [G loss: 1.56047427654] [mll=80.489+-15.982] [ks=0.409]]\n",
      "3800 [D loss: 0.575147867203, acc.: 0.00%] [G loss: 2.11726379395] [mll=90.806+-19.735] [ks=0.356]\n",
      "3900 [D loss: 0.548701226711, acc.: 0.98%] [G loss: 1.82931399345] [mll=76.098+-14.119] [ks=0.626]]\n",
      "4000 [D loss: 0.811052203178, acc.: 0.00%] [G loss: 1.78716671467] [mll=139.443+-44.990] [ks=0.782]]\n",
      "10000/10000 [==============================] - 1s 69us/step\n",
      "4100 [D loss: 0.58629077673, acc.: 0.00%] [G loss: 1.3818924427] [mll=80.108+-24.416] [ks=0.418]8]]\n",
      "4200 [D loss: 0.641296684742, acc.: 0.00%] [G loss: 1.0891071558] [mll=99.740+-10.216] [ks=0.575]]]\n",
      "4300 [D loss: 0.564825534821, acc.: 0.00%] [G loss: 1.46703004837] [mll=91.760+-9.996] [ks=0.261]]\n",
      "4400 [D loss: 0.593397796154, acc.: 0.00%] [G loss: 1.0238751173] [mll=88.295+-10.620] [ks=0.266]]]\n",
      "4500 [D loss: 0.765493929386, acc.: 0.00%] [G loss: 1.99650657177] [mll=100.013+-10.465] [ks=0.587]]\n",
      "10000/10000 [==============================] - 1s 67us/step\n",
      "4600 [D loss: 0.54768371582, acc.: 0.00%] [G loss: 1.03055155277] [mll=96.090+-22.584] [ks=0.355]]]\n",
      "4700 [D loss: 0.60334533453, acc.: 0.00%] [G loss: 1.17228269577] [mll=95.656+-8.526] [ks=0.452]]]\n",
      "4800 [D loss: 0.614939391613, acc.: 0.00%] [G loss: 1.19336235523] [mll=94.125+-8.072] [ks=0.381]]\n",
      "4900 [D loss: 0.545192241669, acc.: 0.00%] [G loss: 0.956982076168] [mll=83.264+-14.376] [ks=0.411]\n",
      "5000 [D loss: 0.59972602129, acc.: 0.00%] [G loss: 1.1758852005] [mll=92.129+-9.601] [ks=0.259]]]]\n",
      "10000/10000 [==============================] - 1s 90us/step\n",
      "5100 [D loss: 0.776469230652, acc.: 0.00%] [G loss: 1.1832703352] [mll=104.610+-15.437] [ks=0.608]]]\n",
      "5200 [D loss: 0.555750131607, acc.: 0.00%] [G loss: 1.11927521229] [mll=87.371+-9.073] [ks=0.270]]\n",
      "5300 [D loss: 0.563210070133, acc.: 0.00%] [G loss: 1.10394620895] [mll=98.384+-14.620] [ks=0.475]]\n",
      "5400 [D loss: 0.536752283573, acc.: 0.00%] [G loss: 1.84310817719] [mll=87.820+-9.841] [ks=0.266]]\n",
      "5500 [D loss: 0.489241361618, acc.: 0.00%] [G loss: 1.76103305817] [mll=86.720+-10.719] [ks=0.360]]\n",
      "10000/10000 [==============================] - 1s 95us/step\n",
      "5600 [D loss: 0.461089670658, acc.: 0.00%] [G loss: 1.12678432465] [mll=87.744+-9.155] [ks=0.231]]\n",
      "5700 [D loss: 0.568319737911, acc.: 0.00%] [G loss: 1.38096547127] [mll=96.633+-7.999] [ks=0.514]]\n",
      "5800 [D loss: 0.492128372192, acc.: 0.00%] [G loss: 1.28640365601] [mll=90.188+-8.308] [ks=0.185]]\n",
      "5900 [D loss: 0.628445863724, acc.: 0.00%] [G loss: 1.00521910191] [mll=126.633+-36.807] [ks=0.733]]\n",
      "6000 [D loss: 0.551016271114, acc.: 0.00%] [G loss: 1.53928422928] [mll=95.526+-9.273] [ks=0.480]]\n",
      "10000/10000 [==============================] - 1s 73us/step\n",
      "6100 [D loss: 0.486181735992, acc.: 0.00%] [G loss: 1.09736335278] [mll=86.268+-12.455] [ks=0.348]]\n",
      "6200 [D loss: 0.492650002241, acc.: 0.00%] [G loss: 1.24517726898] [mll=89.499+-7.811] [ks=0.173]]\n",
      "6300 [D loss: 0.540690362453, acc.: 0.00%] [G loss: 1.00088834763] [mll=87.440+-17.572] [ks=0.313]\n",
      "6400 [D loss: 0.498706400394, acc.: 0.00%] [G loss: 1.253952384] [mll=92.724+-12.434] [ks=0.337]7]\n",
      "6500 [D loss: 0.547336041927, acc.: 0.00%] [G loss: 1.34702157974] [mll=95.945+-8.281] [ks=0.498]]\n",
      "10000/10000 [==============================] - 1s 93us/step\n",
      "6600 [D loss: 0.59176748991, acc.: 0.00%] [G loss: 1.1753102541] [mll=88.423+-8.953] [ks=0.258]8]]\n",
      "6700 [D loss: 0.535088062286, acc.: 0.00%] [G loss: 1.21541893482] [mll=78.339+-16.453] [ks=0.442]\n",
      "6800 [D loss: 0.501619279385, acc.: 0.00%] [G loss: 1.25325572491] [mll=109.406+-24.233] [ks=0.624]]\n",
      "6900 [D loss: 0.527482450008, acc.: 0.00%] [G loss: 1.05882501602] [mll=105.047+-20.678] [ks=0.520]\n",
      "7000 [D loss: 0.508859574795, acc.: 0.00%] [G loss: 1.25634467602] [mll=95.907+-9.464] [ks=0.454]]\n",
      "10000/10000 [==============================] - 1s 84us/step\n",
      "7100 [D loss: 0.499331444502, acc.: 0.00%] [G loss: 1.14156091213] [mll=90.729+-7.748] [ks=0.202]]\n",
      "7200 [D loss: 0.494386434555, acc.: 0.00%] [G loss: 1.16330230236] [mll=97.496+-8.766] [ks=0.509]]\n",
      "7300 [D loss: 0.538574635983, acc.: 0.00%] [G loss: 1.1824054718] [mll=89.831+-9.043] [ks=0.200]]]\n",
      "7400 [D loss: 0.564726889133, acc.: 0.00%] [G loss: 1.12480580807] [mll=95.816+-11.799] [ks=0.373]\n",
      "7500 [D loss: 0.521644175053, acc.: 0.00%] [G loss: 1.13591110706] [mll=90.686+-8.960] [ks=0.204]\n",
      "10000/10000 [==============================] - 1s 88us/step\n",
      "7600 [D loss: 0.537868738174, acc.: 0.00%] [G loss: 1.15547907352] [mll=95.102+-8.021] [ks=0.408]]\n",
      "7700 [D loss: 0.467647612095, acc.: 0.00%] [G loss: 1.16537320614] [mll=93.975+-7.668] [ks=0.401]]\n",
      "7800 [D loss: 0.501376211643, acc.: 0.00%] [G loss: 1.3882958889] [mll=89.873+-7.275] [ks=0.155]]]\n",
      "7900 [D loss: 0.492551267147, acc.: 0.00%] [G loss: 1.31064331532] [mll=85.821+-11.341] [ks=0.281]]\n",
      "8000 [D loss: 0.469031095505, acc.: 0.00%] [G loss: 1.14542758465] [mll=91.063+-12.707] [ks=0.345]]\n",
      "10000/10000 [==============================] - 1s 88us/step\n",
      "8100 [D loss: 0.367978543043, acc.: 0.20%] [G loss: 1.72247290611] [mll=90.978+-8.239] [ks=0.163]\n",
      "8200 [D loss: 0.306267291307, acc.: 0.00%] [G loss: 1.69069123268] [mll=107.212+-25.487] [ks=0.484]\n",
      "8300 [D loss: 0.29156550765, acc.: 0.00%] [G loss: 1.74618136883] [mll=89.319+-10.794] [ks=0.247]]\n",
      "8400 [D loss: 0.303540915251, acc.: 0.00%] [G loss: 1.74671339989] [mll=82.300+-9.191] [ks=0.522]\n",
      "8500 [D loss: 0.24446105957, acc.: 0.00%] [G loss: 1.89932072163] [mll=84.760+-10.158] [ks=0.427]]\n",
      "10000/10000 [==============================] - 1s 68us/step\n",
      "8600 [D loss: 0.267678946257, acc.: 0.00%] [G loss: 1.89023005962] [mll=85.300+-6.711] [ks=0.445]\n",
      "8700 [D loss: 0.288249582052, acc.: 0.00%] [G loss: 1.73913288116] [mll=92.022+-13.673] [ks=0.262]\n",
      "8800 [D loss: 7.97119188309, acc.: 0.00%] [G loss: nan] [mll=79.248+-19.051] [ks=0.302] [ks=0.302]\n",
      "8900 [D loss: 7.97119188309, acc.: 0.00%] [G loss: nan] [mll=nan+-nan] [ks=1.000]\n",
      "9000 [D loss: 7.97119188309, acc.: 0.00%] [G loss: nan] [mll=nan+-nan] [ks=1.000]\n",
      "10000/10000 [==============================] - 1s 65us/step\n",
      "9100 [D loss: 7.97119188309, acc.: 0.00%] [G loss: nan] [mll=nan+-nan] [ks=1.000]\n",
      "9200 [D loss: 7.97119188309, acc.: 0.00%] [G loss: nan] [mll=nan+-nan] [ks=1.000]\n",
      "9300 [D loss: 7.97119188309, acc.: 0.00%] [G loss: nan] [mll=nan+-nan] [ks=1.000]\n",
      "9400 [D loss: 7.97119188309, acc.: 0.00%] [G loss: nan] [mll=nan+-nan] [ks=1.000]\n",
      "9500 [D loss: 7.97119188309, acc.: 0.00%] [G loss: nan] [mll=nan+-nan] [ks=1.000]\n",
      "10000/10000 [==============================] - 1s 62us/step\n",
      "9600 [D loss: 7.97119188309, acc.: 0.00%] [G loss: nan] [mll=nan+-nan] [ks=1.000]\n",
      "9700 [D loss: 7.97119188309, acc.: 0.00%] [G loss: nan] [mll=nan+-nan] [ks=1.000]\n",
      "9726 [D loss: 7.97119188309, acc.: 0.00%] [G loss: nan] [mll=nan+-nan] [ks=1.000]BREAKING because disc/gen loss has remained the same for 1001/0 epochs!\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_80 (InputLayer)           (None, 17)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_466 (Dense)               (None, 128)          2304        input_80[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_412 (LeakyReLU)     (None, 128)          0           dense_466[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_467 (Dense)               (None, 128)          16512       leaky_re_lu_412[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_413 (LeakyReLU)     (None, 128)          0           dense_467[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_468 (Dense)               (None, 256)          33024       leaky_re_lu_413[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_414 (LeakyReLU)     (None, 256)          0           dense_468[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_469 (Dense)               (None, 256)          65792       leaky_re_lu_414[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_415 (LeakyReLU)     (None, 256)          0           dense_469[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_470 (Dense)               (None, 128)          32896       leaky_re_lu_415[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_416 (LeakyReLU)     (None, 128)          0           dense_470[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_471 (Dense)               (None, 64)           8256        leaky_re_lu_416[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_417 (LeakyReLU)     (None, 64)           0           dense_471[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_472 (Dense)               (None, 32)           2080        leaky_re_lu_417[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_418 (LeakyReLU)     (None, 32)           0           dense_472[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_473 (Dense)               (None, 16)           528         leaky_re_lu_418[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_419 (LeakyReLU)     (None, 16)           0           dense_473[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_474 (Dense)               (None, 8)            136         leaky_re_lu_419[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_420 (LeakyReLU)     (None, 8)            0           dense_474[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_475 (Dense)               (None, 1)            9           leaky_re_lu_420[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_88 (Lambda)              (None, 1)            0           input_80[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_89 (Lambda)              (None, 1)            0           input_80[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_90 (Lambda)              (None, 1)            0           input_80[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_30 (Concatenate)    (None, 4)            0           dense_475[0][0]                  \n",
      "                                                                 lambda_88[0][0]                  \n",
      "                                                                 lambda_89[0][0]                  \n",
      "                                                                 lambda_90[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 161,537\n",
      "Trainable params: 161,537\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Discriminator params: 161537\n",
      "Generator params: 340497\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_81 (InputLayer)        (None, 17)                0         \n",
      "_________________________________________________________________\n",
      "dense_476 (Dense)            (None, 64)                1152      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_421 (LeakyReLU)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_477 (Dense)            (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_422 (LeakyReLU)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_478 (Dense)            (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_423 (LeakyReLU)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_479 (Dense)            (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_424 (LeakyReLU)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_480 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_425 (LeakyReLU)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_481 (Dense)            (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_426 (LeakyReLU)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_482 (Dense)            (None, 17)                2193      \n",
      "=================================================================\n",
      "Total params: 340,497\n",
      "Trainable params: 340,497\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 [D loss: 0.0142467236146, acc.: 50.00%] [G loss: 4.70237398148] [mll=-1.000+--1.000] [ks=999.000]\n",
      "200 [D loss: 0.253324747086, acc.: 50.00%] [G loss: 2.26986789703] [mll=nan+-nan] [ks=1.000]]\n",
      "300 [D loss: 0.163585186005, acc.: 0.00%] [G loss: 1.95599150658] [mll=nan+-nan] [ks=1.000]]]\n",
      "400 [D loss: 0.0368924289942, acc.: 0.00%] [G loss: 4.20573806763] [mll=76.700+-7.894] [ks=0.703]\n",
      "500 [D loss: 0.060878213495, acc.: 0.00%] [G loss: 4.37385988235] [mll=51.351+-4.190] [ks=0.962]]\n",
      "10000/10000 [==============================] - 1s 76us/step\n",
      "600 [D loss: 0.0321386568248, acc.: 0.00%] [G loss: 4.59229707718] [mll=70.456+-5.994] [ks=0.861]\n",
      "700 [D loss: 0.0444951690733, acc.: 0.00%] [G loss: 4.70367574692] [mll=62.271+-5.075] [ks=0.924]\n",
      "800 [D loss: 0.0231989640743, acc.: 0.00%] [G loss: 5.32637548447] [mll=80.535+-6.834] [ks=0.620]]\n",
      "900 [D loss: 7.97119188309, acc.: 0.00%] [G loss: nan] [mll=90.150+-7.388] [ks=0.177]] [ks=0.177]\n",
      "1000 [D loss: 7.97119188309, acc.: 0.00%] [G loss: nan] [mll=nan+-nan] [ks=1.000]\n",
      "10000/10000 [==============================] - 1s 72us/step\n",
      "1100 [D loss: 7.97119188309, acc.: 0.00%] [G loss: nan] [mll=nan+-nan] [ks=1.000]\n",
      "1200 [D loss: 7.97119188309, acc.: 0.00%] [G loss: nan] [mll=nan+-nan] [ks=1.000]\n",
      "1300 [D loss: 7.97119188309, acc.: 0.00%] [G loss: nan] [mll=nan+-nan] [ks=1.000]\n",
      "1400 [D loss: 7.97119188309, acc.: 0.00%] [G loss: nan] [mll=nan+-nan] [ks=1.000]\n",
      "1500 [D loss: 7.97119188309, acc.: 0.00%] [G loss: nan] [mll=nan+-nan] [ks=1.000]\n",
      "10000/10000 [==============================] - 1s 81us/step\n",
      "1600 [D loss: 7.97119188309, acc.: 0.00%] [G loss: nan] [mll=nan+-nan] [ks=1.000]\n",
      "1700 [D loss: 7.97119188309, acc.: 0.00%] [G loss: nan] [mll=nan+-nan] [ks=1.000]\n",
      "1800 [D loss: 7.97119188309, acc.: 0.00%] [G loss: nan] [mll=nan+-nan] [ks=1.000]\n",
      "1832 [D loss: 7.97119188309, acc.: 0.00%] [G loss: nan] [mll=nan+-nan] [ks=1.000]BREAKING because disc/gen loss has remained the same for 1001/0 epochs!\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_83 (InputLayer)           (None, 17)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_483 (Dense)               (None, 128)          2304        input_83[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_427 (LeakyReLU)     (None, 128)          0           dense_483[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_484 (Dense)               (None, 128)          16512       leaky_re_lu_427[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_428 (LeakyReLU)     (None, 128)          0           dense_484[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_485 (Dense)               (None, 256)          33024       leaky_re_lu_428[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_429 (LeakyReLU)     (None, 256)          0           dense_485[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_486 (Dense)               (None, 256)          65792       leaky_re_lu_429[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_430 (LeakyReLU)     (None, 256)          0           dense_486[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_487 (Dense)               (None, 128)          32896       leaky_re_lu_430[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_431 (LeakyReLU)     (None, 128)          0           dense_487[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_488 (Dense)               (None, 64)           8256        leaky_re_lu_431[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_432 (LeakyReLU)     (None, 64)           0           dense_488[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_489 (Dense)               (None, 32)           2080        leaky_re_lu_432[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_433 (LeakyReLU)     (None, 32)           0           dense_489[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_490 (Dense)               (None, 16)           528         leaky_re_lu_433[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_434 (LeakyReLU)     (None, 16)           0           dense_490[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_491 (Dense)               (None, 8)            136         leaky_re_lu_434[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_435 (LeakyReLU)     (None, 8)            0           dense_491[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_492 (Dense)               (None, 1)            9           leaky_re_lu_435[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_91 (Lambda)              (None, 1)            0           input_83[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_92 (Lambda)              (None, 1)            0           input_83[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_93 (Lambda)              (None, 1)            0           input_83[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_31 (Concatenate)    (None, 4)            0           dense_492[0][0]                  \n",
      "                                                                 lambda_91[0][0]                  \n",
      "                                                                 lambda_92[0][0]                  \n",
      "                                                                 lambda_93[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 161,537\n",
      "Trainable params: 161,537\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Discriminator params: 161537\n",
      "Generator params: 340497\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_84 (InputLayer)        (None, 17)                0         \n",
      "_________________________________________________________________\n",
      "dense_493 (Dense)            (None, 64)                1152      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_436 (LeakyReLU)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_494 (Dense)            (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_437 (LeakyReLU)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_495 (Dense)            (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_438 (LeakyReLU)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_496 (Dense)            (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_439 (LeakyReLU)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_497 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_440 (LeakyReLU)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_498 (Dense)            (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_441 (LeakyReLU)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_499 (Dense)            (None, 17)                2193      \n",
      "=================================================================\n",
      "Total params: 340,497\n",
      "Trainable params: 340,497\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "params[\"tag\"] = \"lepflatnoise3\"\n",
    "gan = GAN(**params)\n",
    "gan.batchTrain(\"lepflatnoise3\", 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find best models from batch training\n",
    "\n",
    "Below, you can make load in any previous batch tag and perform a search on the outputs to find the best models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_34 (InputLayer)           (None, 17)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_188 (Dense)               (None, 128)          2304        input_34[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_166 (LeakyReLU)     (None, 128)          0           dense_188[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_189 (Dense)               (None, 128)          16512       leaky_re_lu_166[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_167 (LeakyReLU)     (None, 128)          0           dense_189[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_190 (Dense)               (None, 256)          33024       leaky_re_lu_167[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_168 (LeakyReLU)     (None, 256)          0           dense_190[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_191 (Dense)               (None, 256)          65792       leaky_re_lu_168[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_169 (LeakyReLU)     (None, 256)          0           dense_191[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_192 (Dense)               (None, 128)          32896       leaky_re_lu_169[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_170 (LeakyReLU)     (None, 128)          0           dense_192[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_193 (Dense)               (None, 64)           8256        leaky_re_lu_170[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_171 (LeakyReLU)     (None, 64)           0           dense_193[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_194 (Dense)               (None, 32)           2080        leaky_re_lu_171[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_172 (LeakyReLU)     (None, 32)           0           dense_194[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_195 (Dense)               (None, 16)           528         leaky_re_lu_172[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_173 (LeakyReLU)     (None, 16)           0           dense_195[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_196 (Dense)               (None, 8)            136         leaky_re_lu_173[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_174 (LeakyReLU)     (None, 8)            0           dense_196[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_197 (Dense)               (None, 1)            9           leaky_re_lu_174[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_34 (Lambda)              (None, 1)            0           input_34[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 2)            0           dense_197[0][0]                  \n",
      "                                                                 lambda_34[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 161,537\n",
      "Trainable params: 161,537\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Discriminator params: 161537\n",
      "Generator params: 340497\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_35 (InputLayer)        (None, 17)                0         \n",
      "_________________________________________________________________\n",
      "dense_198 (Dense)            (None, 64)                1152      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_175 (LeakyReLU)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_199 (Dense)            (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_176 (LeakyReLU)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_200 (Dense)            (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_177 (LeakyReLU)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_201 (Dense)            (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_178 (LeakyReLU)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_202 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_179 (LeakyReLU)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_203 (Dense)            (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_180 (LeakyReLU)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_204 (Dense)            (None, 17)                2193      \n",
      "=================================================================\n",
      "Total params: 340,497\n",
      "Trainable params: 340,497\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gan = GAN(**params)\n",
    "gan.tag=\"flatnoise_1\"\n",
    "x = gan.findBest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1_19300', [4118, {'lep1_iso': 8, 'metphi': 1112, 'mll': 1894}]),\n",
       " ('1_13800', [4808, {'lep1_iso': 1296, 'metphi': 284, 'mll': 1664}]),\n",
       " ('1_8500', [5130, {'lep1_iso': 1112, 'metphi': 330, 'mll': 2262}]),\n",
       " ('1_14100', [5176, {'lep1_iso': 606, 'metphi': 1618, 'mll': 744}]),\n",
       " ('1_17400', [5636, {'lep1_iso': 284, 'metphi': 1940, 'mll': 1204}]),\n",
       " ('1_16300', [5682, {'lep1_iso': 238, 'metphi': 1388, 'mll': 2446}]),\n",
       " ('1_16800', [5682, {'lep1_iso': 1342, 'metphi': 1158, 'mll': 698}]),\n",
       " ('1_11300', [6142, {'lep1_iso': 1020, 'metphi': 1894, 'mll': 330}]),\n",
       " ('1_17800', [6556, {'lep1_iso': 468, 'metphi': 1802, 'mll': 2032}]),\n",
       " ('1_17600', [6878, {'lep1_iso': 652, 'metphi': 790, 'mll': 4010}]),\n",
       " ('1_18000', [7200, {'lep1_iso': 1204, 'metphi': 1526, 'mll': 1756}]),\n",
       " ('1_16200', [7200, {'lep1_iso': 192, 'metphi': 974, 'mll': 4884}]),\n",
       " ('1_12800', [7522, {'lep1_iso': 2170, 'metphi': 514, 'mll': 2170}]),\n",
       " ('1_16000', [7706, {'lep1_iso': 2538, 'metphi': 54, 'mll': 2538}]),\n",
       " ('1_15300', [7936, {'lep1_iso': 3182, 'metphi': 146, 'mll': 1296}]),\n",
       " ('1_17300', [8350, {'lep1_iso': 376, 'metphi': 2400, 'mll': 2814}]),\n",
       " ('1_10000', [8442, {'lep1_iso': 3136, 'metphi': 560, 'mll': 1066}]),\n",
       " ('1_16100', [8488, {'lep1_iso': 790, 'metphi': 2998, 'mll': 928}]),\n",
       " ('1_11400', [9362, {'lep1_iso': 1618, 'metphi': 698, 'mll': 4746}]),\n",
       " ('1_15800', [9454, {'lep1_iso': 836, 'metphi': 2860, 'mll': 2078}]),\n",
       " ('1_19100', [9546, {'lep1_iso': 100, 'metphi': 1434, 'mll': 6494}]),\n",
       " ('1_11100', [9960, {'lep1_iso': 3504, 'metphi': 192, 'mll': 2584}]),\n",
       " ('1_19200', [9960, {'lep1_iso': 2078, 'metphi': 1342, 'mll': 3136}]),\n",
       " ('1_18900', [10190, {'lep1_iso': 1848, 'metphi': 652, 'mll': 5206}]),\n",
       " ('1_16600', [10190, {'lep1_iso': 2814, 'metphi': 1848, 'mll': 882}]),\n",
       " ('1_15400', [11018, {'lep1_iso': 882, 'metphi': 1020, 'mll': 7230}]),\n",
       " ('1_14900', [11064, {'lep1_iso': 2722, 'metphi': 2262, 'mll': 1112}]),\n",
       " ('1_9400', [11110, {'lep1_iso': 2262, 'metphi': 2906, 'mll': 790}]),\n",
       " ('1_8400', [11156, {'lep1_iso': 3044, 'metphi': 238, 'mll': 4608}]),\n",
       " ('1_13500', [11248, {'lep1_iso': 2032, 'metphi': 1664, 'mll': 3872}]),\n",
       " ('1_16900', [11938, {'lep1_iso': 330, 'metphi': 3136, 'mll': 5022}]),\n",
       " ('1_17000', [11984, {'lep1_iso': 2446, 'metphi': 836, 'mll': 5436}]),\n",
       " ('1_11000', [12168, {'lep1_iso': 1940, 'metphi': 2630, 'mll': 3044}]),\n",
       " ('1_9800', [12168, {'lep1_iso': 2860, 'metphi': 468, 'mll': 5528}]),\n",
       " ('1_18800', [12260, {'lep1_iso': 514, 'metphi': 2124, 'mll': 7000}]),\n",
       " ('1_9500', [12444, {'lep1_iso': 1250, 'metphi': 2170, 'mll': 5620}]),\n",
       " ('1_7600', [12490, {'lep1_iso': 2492, 'metphi': 1204, 'mll': 5114}]),\n",
       " ('1_17100', [12536, {'lep1_iso': 422, 'metphi': 5160, 'mll': 1388}]),\n",
       " ('1_13100', [12628, {'lep1_iso': 2768, 'metphi': 2216, 'mll': 2676}]),\n",
       " ('1_19000', [12674, {'lep1_iso': 54, 'metphi': 3320, 'mll': 5942}]),\n",
       " ('1_14300', [12904, {'lep1_iso': 2676, 'metphi': 3550, 'mll': 468}]),\n",
       " ('1_14800', [13088, {'lep1_iso': 4516, 'metphi': 606, 'mll': 2860}]),\n",
       " ('1_18100', [13318, {'lep1_iso': 1664, 'metphi': 3044, 'mll': 3918}]),\n",
       " ('1_19400', [13318, {'lep1_iso': 2400, 'metphi': 3090, 'mll': 2354}]),\n",
       " ('1_18200', [13594, {'lep1_iso': 698, 'metphi': 4562, 'mll': 3090}]),\n",
       " ('1_18300', [13962, {'lep1_iso': 146, 'metphi': 4056, 'mll': 5574}]),\n",
       " ('1_11900', [13962, {'lep1_iso': 4378, 'metphi': 744, 'mll': 3734}]),\n",
       " ('1_15500', [14008, {'lep1_iso': 1802, 'metphi': 2032, 'mll': 6356}]),\n",
       " ('1_17500', [14008, {'lep1_iso': 1526, 'metphi': 3596, 'mll': 3780}]),\n",
       " ('1_13700', [14146, {'lep1_iso': 3918, 'metphi': 100, 'mll': 6126}]),\n",
       " ('1_11600', [14192, {'lep1_iso': 4470, 'metphi': 376, 'mll': 4516}]),\n",
       " ('1_17900', [14192, {'lep1_iso': 1572, 'metphi': 2952, 'mll': 5160}]),\n",
       " ('1_15900', [14238, {'lep1_iso': 3964, 'metphi': 1572, 'mll': 3182}]),\n",
       " ('1_16700', [15020, {'lep1_iso': 4056, 'metphi': 8, 'mll': 6908}]),\n",
       " ('1_8800', [15296, {'lep1_iso': 928, 'metphi': 4194, 'mll': 5068}]),\n",
       " ('1_17200', [15434, {'lep1_iso': 3090, 'metphi': 1296, 'mll': 6678}]),\n",
       " ('1_12500', [15480, {'lep1_iso': 1480, 'metphi': 3182, 'mll': 6172}]),\n",
       " ('1_10700', [15912, {'lep1_iso': 3613, 'metphi': 2584, 'mll': 3550}]),\n",
       " ('1_7000', [15986, {'lep1_iso': 5068, 'metphi': 1250, 'mll': 3366}]),\n",
       " ('1_18500', [16032, {'lep1_iso': 744, 'metphi': 3504, 'mll': 7552}]),\n",
       " ('1_5500', [16216, {'lep1_iso': 974, 'metphi': 6080, 'mll': 2124}]),\n",
       " ('1_8200', [16262, {'lep1_iso': 6264, 'metphi': 1756, 'mll': 238}]),\n",
       " ('1_12300', [16446, {'lep1_iso': 5344, 'metphi': 1066, 'mll': 3642}]),\n",
       " ('1_13200', [16512, {'lep1_iso': 3614, 'metphi': 2676, 'mll': 3964}]),\n",
       " ('1_18400', [16778, {'lep1_iso': 2354, 'metphi': 3375, 'mll': 5344}]),\n",
       " ('1_12200', [16860, {'lep1_iso': 3412, 'metphi': 3780, 'mll': 2492}]),\n",
       " ('1_9600', [17236, {'lep1_iso': 5114, 'metphi': 3374, 'mll': 284}]),\n",
       " ('1_18600', [17320, {'lep1_iso': 2216, 'metphi': 5068, 'mll': 2768}]),\n",
       " ('1_6000', [17596, {'lep1_iso': 2630, 'metphi': 2308, 'mll': 7736}]),\n",
       " ('1_4100', [17734, {'lep1_iso': 2124, 'metphi': 5114, 'mll': 3274}]),\n",
       " ('1_14700', [17734, {'lep1_iso': 2998, 'metphi': 3688, 'mll': 4378}]),\n",
       " ('1_15600', [17826, {'lep1_iso': 3228, 'metphi': 3964, 'mll': 3458}]),\n",
       " ('1_9300', [17964, {'lep1_iso': 4884, 'metphi': 882, 'mll': 6448}]),\n",
       " ('1_7800', [18102, {'lep1_iso': 4792, 'metphi': 422, 'mll': 7690}]),\n",
       " ('1_13600', [18102, {'lep1_iso': 4332, 'metphi': 2814, 'mll': 3826}]),\n",
       " ('1_7900', [18194, {'lep1_iso': 1066, 'metphi': 5574, 'mll': 4930}]),\n",
       " ('1_16400', [18194, {'lep1_iso': 560, 'metphi': 6494, 'mll': 4102}]),\n",
       " ('1_3900', [18286, {'lep1_iso': 1710, 'metphi': 3458, 'mll': 7966}]),\n",
       " ('1_13000', [18332, {'lep1_iso': 5850, 'metphi': 928, 'mll': 4792}]),\n",
       " ('1_11200', [18378, {'lep1_iso': 6172, 'metphi': 2538, 'mll': 974}]),\n",
       " ('1_13900', [18424, {'lep1_iso': 1986, 'metphi': 4332, 'mll': 5804}]),\n",
       " ('1_6300', [18624, {'lep1_iso': 3612, 'metphi': 5298, 'mll': 836}]),\n",
       " ('1_10300', [19022, {'lep1_iso': 5804, 'metphi': 3642, 'mll': 146}]),\n",
       " ('1_15700', [19206, {'lep1_iso': 2952, 'metphi': 5988, 'mll': 1342}]),\n",
       " ('1_14000', [19344, {'lep1_iso': 2906, 'metphi': 5850, 'mll': 1848}]),\n",
       " ('1_16500', [19666, {'lep1_iso': 3320, 'metphi': 5206, 'mll': 2630}]),\n",
       " ('1_4900', [20310, {'lep1_iso': 7552, 'metphi': 1710, 'mll': 1802}]),\n",
       " ('1_9000', [20448, {'lep1_iso': 3826, 'metphi': 4286, 'mll': 4240}]),\n",
       " ('1_12700', [20540, {'lep1_iso': 3872, 'metphi': 5252, 'mll': 2308}]),\n",
       " ('1_12600', [20586, {'lep1_iso': 4654, 'metphi': 2078, 'mll': 7138}]),\n",
       " ('1_7700', [20724, {'lep1_iso': 7414, 'metphi': 2768, 'mll': 376}]),\n",
       " ('1_7100', [20816, {'lep1_iso': 5482, 'metphi': 2722, 'mll': 4424}]),\n",
       " ('1_18700', [20954, {'lep1_iso': 3550, 'metphi': 4240, 'mll': 5390}]),\n",
       " ('1_3800', [21046, {'lep1_iso': 4930, 'metphi': 2446, 'mll': 6310}]),\n",
       " ('1_5300', [21184, {'lep1_iso': 1388, 'metphi': 5666, 'mll': 7092}]),\n",
       " ('1_4300', [21276, {'lep1_iso': 6448, 'metphi': 4148, 'mll': 100}]),\n",
       " ('1_10900', [21322, {'lep1_iso': 2584, 'metphi': 5942, 'mll': 4286}]),\n",
       " ('1_3600', [21598, {'lep1_iso': 1434, 'metphi': 5896, 'mll': 6954}]),\n",
       " ('1_9900', [21828, {'lep1_iso': 3366, 'metphi': 5758, 'mll': 3596}]),\n",
       " ('1_11500', [22058, {'lep1_iso': 5022, 'metphi': 4516, 'mll': 2998}]),\n",
       " ('1_4400', [22104, {'lep1_iso': 6632, 'metphi': 4102, 'mll': 652}]),\n",
       " ('1_4000', [22150, {'lep1_iso': 7874, 'metphi': 2354, 'mll': 1710}]),\n",
       " ('1_15100', [22196, {'lep1_iso': 4148, 'metphi': 4930, 'mll': 4056}]),\n",
       " ('1_6500', [22702, {'lep1_iso': 1756, 'metphi': 7322, 'mll': 4562}]),\n",
       " ('1_9200', [22748, {'lep1_iso': 6862, 'metphi': 4010, 'mll': 1020}]),\n",
       " ('1_4200', [22932, {'lep1_iso': 6770, 'metphi': 4608, 'mll': 192}]),\n",
       " ('1_10800', [23024, {'lep1_iso': 5758, 'metphi': 4654, 'mll': 2216}]),\n",
       " ('1_14500', [23024, {'lep1_iso': 5942, 'metphi': 1986, 'mll': 7184}]),\n",
       " ('1_10200', [23024, {'lep1_iso': 4240, 'metphi': 7000, 'mll': 560}]),\n",
       " ('1_8700', [23162, {'lep1_iso': 5436, 'metphi': 5390, 'mll': 1526}]),\n",
       " ('1_11800', [23228, {'lep1_iso': 6052, 'metphi': 3872, 'mll': 3412}]),\n",
       " ('1_12900', [23438, {'lep1_iso': 5298, 'metphi': 5620, 'mll': 1618}]),\n",
       " ('1_14200', [23530, {'lep1_iso': 4286, 'metphi': 4378, 'mll': 6218}]),\n",
       " ('1_7500', [23530, {'lep1_iso': 5620, 'metphi': 3274, 'mll': 5758}]),\n",
       " ('1_12000', [23576, {'lep1_iso': 4194, 'metphi': 6126, 'mll': 2952}]),\n",
       " ('1_6600', [23576, {'lep1_iso': 1894, 'metphi': 6632, 'mll': 6540}]),\n",
       " ('1_8300', [23898, {'lep1_iso': 7506, 'metphi': 3734, 'mll': 1434}]),\n",
       " ('1_5400', [24128, {'lep1_iso': 1158, 'metphi': 8058, 'mll': 5712}]),\n",
       " ('1_11700', [24266, {'lep1_iso': 4424, 'metphi': 4884, 'mll': 5666}]),\n",
       " ('1_7300', [24312, {'lep1_iso': 3734, 'metphi': 5804, 'mll': 5252}]),\n",
       " ('1_7400', [24404, {'lep1_iso': 2308, 'metphi': 7552, 'mll': 4700}]),\n",
       " ('1_14400', [24450, {'lep1_iso': 4608, 'metphi': 7414, 'mll': 422}]),\n",
       " ('1_13300', [24588, {'lep1_iso': 5528, 'metphi': 3826, 'mll': 5896}]),\n",
       " ('1_3400', [24726, {'lep1_iso': 4700, 'metphi': 7368, 'mll': 606}]),\n",
       " ('1_10500', [24864, {'lep1_iso': 5574, 'metphi': 5022, 'mll': 3688}]),\n",
       " ('1_6100', [25002, {'lep1_iso': 4102, 'metphi': 7782, 'mll': 1250}]),\n",
       " ('1_9100', [25140, {'lep1_iso': 3780, 'metphi': 7046, 'mll': 3504}]),\n",
       " ('1_15000', [25416, {'lep1_iso': 4976, 'metphi': 4700, 'mll': 6080}]),\n",
       " ('1_8100', [25646, {'lep1_iso': 4562, 'metphi': 4746, 'mll': 7046}]),\n",
       " ('1_12100', [26106, {'lep1_iso': 6218, 'metphi': 5482, 'mll': 2722}]),\n",
       " ('1_8000', [26106, {'lep1_iso': 6908, 'metphi': 3918, 'mll': 4470}]),\n",
       " ('1_10400', [26152, {'lep1_iso': 5160, 'metphi': 7138, 'mll': 1572}]),\n",
       " ('1_3200', [26290, {'lep1_iso': 5712, 'metphi': 5344, 'mll': 4194}]),\n",
       " ('1_6900', [26474, {'lep1_iso': 6586, 'metphi': 3228, 'mll': 6862}]),\n",
       " ('1_17700', [26612, {'lep1_iso': 5206, 'metphi': 4976, 'mll': 6264}]),\n",
       " ('1_12400', [26750, {'lep1_iso': 6310, 'metphi': 4424, 'mll': 5298}]),\n",
       " ('1_2700', [27072, {'lep1_iso': 8334, 'metphi': 1480, 'mll': 7460}]),\n",
       " ('1_6800', [27808, {'lep1_iso': 7460, 'metphi': 6448, 'mll': 8}]),\n",
       " ('1_3500', [27900, {'lep1_iso': 6954, 'metphi': 4838, 'mll': 4332}]),\n",
       " ('1_6700', [28010, {'lep1_iso': 6051, 'metphi': 7230, 'mll': 1480}]),\n",
       " ('1_3100', [28038, {'lep1_iso': 8150, 'metphi': 2492, 'mll': 6770}]),\n",
       " ('1_4800', [28038, {'lep1_iso': 7276, 'metphi': 6172, 'mll': 1158}]),\n",
       " ('1_5800', [28130, {'lep1_iso': 3458, 'metphi': 7690, 'mll': 5850}]),\n",
       " ('1_10600', [28222, {'lep1_iso': 7736, 'metphi': 6356, 'mll': 54}]),\n",
       " ('1_13400', [28498, {'lep1_iso': 5988, 'metphi': 6816, 'mll': 2906}]),\n",
       " ('1_8900', [28498, {'lep1_iso': 6494, 'metphi': 5436, 'mll': 4654}]),\n",
       " ('1_2800', [28682, {'lep1_iso': 5666, 'metphi': 4792, 'mll': 7782}]),\n",
       " ('1_4500', [29740, {'lep1_iso': 6402, 'metphi': 6862, 'mll': 3228}]),\n",
       " ('1_8600', [30108, {'lep1_iso': 5896, 'metphi': 7092, 'mll': 4148}]),\n",
       " ('1_14600', [30292, {'lep1_iso': 5390, 'metphi': 6402, 'mll': 6724}]),\n",
       " ('1_5100', [30384, {'lep1_iso': 7092, 'metphi': 6908, 'mll': 2400}]),\n",
       " ('1_1900', [30430, {'lep1_iso': 3274, 'metphi': 8150, 'mll': 7598}]),\n",
       " ('1_4700', [30522, {'lep1_iso': 4010, 'metphi': 7184, 'mll': 8150}]),\n",
       " ('1_7200', [30568, {'lep1_iso': 7782, 'metphi': 6540, 'mll': 1940}]),\n",
       " ('1_15200', [30706, {'lep1_iso': 4746, 'metphi': 6954, 'mll': 7322}]),\n",
       " ('1_5200', [31120, {'lep1_iso': 7184, 'metphi': 6724, 'mll': 3320}]),\n",
       " ('1_2400', [31350, {'lep1_iso': 7138, 'metphi': 8288, 'mll': 514}]),\n",
       " ('1_2500', [31902, {'lep1_iso': 7368, 'metphi': 7598, 'mll': 1986}]),\n",
       " ('1_5000', [32086, {'lep1_iso': 7828, 'metphi': 4470, 'mll': 7506}]),\n",
       " ('1_9700', [32178, {'lep1_iso': 6540, 'metphi': 6264, 'mll': 6586}]),\n",
       " ('1_10100', [32224, {'lep1_iso': 7322, 'metphi': 6310, 'mll': 4976}]),\n",
       " ('1_4600', [32270, {'lep1_iso': 6724, 'metphi': 5712, 'mll': 7414}]),\n",
       " ('1_3300', [32408, {'lep1_iso': 5252, 'metphi': 7644, 'mll': 6632}]),\n",
       " ('1_3700', [32546, {'lep1_iso': 4838, 'metphi': 7506, 'mll': 7874}]),\n",
       " ('1_5600', [33052, {'lep1_iso': 6816, 'metphi': 6034, 'mll': 7368}]),\n",
       " ('1_2600', [34110, {'lep1_iso': 7966, 'metphi': 6678, 'mll': 4838}]),\n",
       " ('1_2200', [34432, {'lep1_iso': 6356, 'metphi': 7874, 'mll': 5988}]),\n",
       " ('1_3000', [34478, {'lep1_iso': 7230, 'metphi': 7276, 'mll': 5482}]),\n",
       " ('1_2100', [34816, {'lep1_iso': 6050, 'metphi': 7460, 'mll': 7828}]),\n",
       " ('1_6200', [34846, {'lep1_iso': 6678, 'metphi': 7736, 'mll': 6034}]),\n",
       " ('1_2900', [35168, {'lep1_iso': 8012, 'metphi': 5528, 'mll': 8104}]),\n",
       " ('1_5700', [35720, {'lep1_iso': 7644, 'metphi': 6218, 'mll': 8012}]),\n",
       " ('1_1700', [36548, {'lep1_iso': 7598, 'metphi': 6586, 'mll': 8196}]),\n",
       " ('1_2000', [37422, {'lep1_iso': 7920, 'metphi': 6770, 'mll': 8058}]),\n",
       " ('1_5900', [38158, {'lep1_iso': 7690, 'metphi': 8196, 'mll': 6402}]),\n",
       " ('1_6400', [38664, {'lep1_iso': 7046, 'metphi': 8472, 'mll': 7644}]),\n",
       " ('1_1600', [39584, {'lep1_iso': 8196, 'metphi': 7966, 'mll': 7276}]),\n",
       " ('1_2300', [40044, {'lep1_iso': 8058, 'metphi': 8012, 'mll': 7920}]),\n",
       " ('1_300', [40228, {'lep1_iso': 7000, 'metphi': 8794, 'mll': 8656}]),\n",
       " ('1_600', [40918, {'lep1_iso': 8104, 'metphi': 8104, 'mll': 8518}]),\n",
       " ('1_1300', [42022, {'lep1_iso': 8380, 'metphi': 8518, 'mll': 8242}]),\n",
       " ('1_1200', [42114, {'lep1_iso': 8242, 'metphi': 8656, 'mll': 8334}]),\n",
       " ('1_1100', [42620, {'lep1_iso': 8288, 'metphi': 8840, 'mll': 8380}]),\n",
       " ('1_1800', [59410, {'lep1_iso': 26955, 'metphi': 8610, 'mll': 6816}]),\n",
       " ('1_900', [59586, {'lep1_iso': 26951, 'metphi': 7828, 'mll': 8564}]),\n",
       " ('1_1400', [59820, {'lep1_iso': 26953, 'metphi': 7920, 'mll': 8610}]),\n",
       " ('1_1500', [60512, {'lep1_iso': 26954, 'metphi': 8426, 'mll': 8288}]),\n",
       " ('1_500', [60546, {'lep1_iso': 26948, 'metphi': 8242, 'mll': 8702}]),\n",
       " ('1_1000', [60600, {'lep1_iso': 26952, 'metphi': 8380, 'mll': 8472}]),\n",
       " ('1_700', [60824, {'lep1_iso': 26949, 'metphi': 8334, 'mll': 8794}]),\n",
       " ('1_800', [60918, {'lep1_iso': 26950, 'metphi': 8564, 'mll': 8426}]),\n",
       " ('1_200', [61600, {'lep1_iso': 26946, 'metphi': 8702, 'mll': 8840}]),\n",
       " ('1_400', [61602, {'lep1_iso': 26947, 'metphi': 8748, 'mll': 8748}]),\n",
       " ('1_19500', [98914, {'lep1_iso': 26956, 'metphi': 27326, 'mll': 27326}]),\n",
       " ('1_19600', [98919, {'lep1_iso': 26957, 'metphi': 27327, 'mll': 27327}]),\n",
       " ('1_19700', [98924, {'lep1_iso': 26958, 'metphi': 27328, 'mll': 27328}]),\n",
       " ('1_19800', [98929, {'lep1_iso': 26959, 'metphi': 27329, 'mll': 27329}]),\n",
       " ('1_19900', [98934, {'lep1_iso': 26960, 'metphi': 27330, 'mll': 27330}]),\n",
       " ('1_20000', [98939, {'lep1_iso': 26961, 'metphi': 27331, 'mll': 27331}]),\n",
       " ('1_20100', [98944, {'lep1_iso': 26962, 'metphi': 27332, 'mll': 27332}]),\n",
       " ('1_20200', [98949, {'lep1_iso': 26963, 'metphi': 27333, 'mll': 27333}]),\n",
       " ('1_20300', [98954, {'lep1_iso': 26964, 'metphi': 27334, 'mll': 27334}]),\n",
       " ('1_20400', [98959, {'lep1_iso': 26965, 'metphi': 27335, 'mll': 27335}]),\n",
       " ('1_20500', [98964, {'lep1_iso': 26966, 'metphi': 27336, 'mll': 27336}]),\n",
       " ('1_20600', [98969, {'lep1_iso': 26967, 'metphi': 27337, 'mll': 27337}]),\n",
       " ('1_20700', [98974, {'lep1_iso': 26968, 'metphi': 27338, 'mll': 27338}]),\n",
       " ('1_20800', [98979, {'lep1_iso': 26969, 'metphi': 27339, 'mll': 27339}]),\n",
       " ('1_20900', [98984, {'lep1_iso': 26970, 'metphi': 27340, 'mll': 27340}]),\n",
       " ('1_21000', [98989, {'lep1_iso': 26971, 'metphi': 27341, 'mll': 27341}]),\n",
       " ('1_21100', [98994, {'lep1_iso': 26972, 'metphi': 27342, 'mll': 27342}]),\n",
       " ('1_21200', [98999, {'lep1_iso': 26973, 'metphi': 27343, 'mll': 27343}]),\n",
       " ('1_21300', [99004, {'lep1_iso': 26974, 'metphi': 27344, 'mll': 27344}]),\n",
       " ('1_21400', [99009, {'lep1_iso': 26975, 'metphi': 27345, 'mll': 27345}]),\n",
       " ('1_21500', [99014, {'lep1_iso': 26976, 'metphi': 27346, 'mll': 27346}]),\n",
       " ('1_21600', [99019, {'lep1_iso': 26977, 'metphi': 27347, 'mll': 27347}]),\n",
       " ('1_21700', [99024, {'lep1_iso': 26978, 'metphi': 27348, 'mll': 27348}]),\n",
       " ('1_21800', [99029, {'lep1_iso': 26979, 'metphi': 27349, 'mll': 27349}]),\n",
       " ('1_21900', [99034, {'lep1_iso': 26980, 'metphi': 27350, 'mll': 27350}]),\n",
       " ('1_22000', [99039, {'lep1_iso': 26981, 'metphi': 27351, 'mll': 27351}]),\n",
       " ('1_22100', [99044, {'lep1_iso': 26982, 'metphi': 27352, 'mll': 27352}]),\n",
       " ('1_22200', [99049, {'lep1_iso': 26983, 'metphi': 27353, 'mll': 27353}]),\n",
       " ('1_22300', [99054, {'lep1_iso': 26984, 'metphi': 27354, 'mll': 27354}]),\n",
       " ('1_22400', [99059, {'lep1_iso': 26985, 'metphi': 27355, 'mll': 27355}]),\n",
       " ('1_22500', [99064, {'lep1_iso': 26986, 'metphi': 27356, 'mll': 27356}]),\n",
       " ('1_22600', [99069, {'lep1_iso': 26987, 'metphi': 27357, 'mll': 27357}]),\n",
       " ('1_22700', [99074, {'lep1_iso': 26988, 'metphi': 27358, 'mll': 27358}]),\n",
       " ('1_22800', [99079, {'lep1_iso': 26989, 'metphi': 27359, 'mll': 27359}]),\n",
       " ('1_22900', [99084, {'lep1_iso': 26990, 'metphi': 27360, 'mll': 27360}]),\n",
       " ('1_23000', [99089, {'lep1_iso': 26991, 'metphi': 27361, 'mll': 27361}]),\n",
       " ('1_23100', [99094, {'lep1_iso': 26992, 'metphi': 27362, 'mll': 27362}]),\n",
       " ('1_23200', [99099, {'lep1_iso': 26993, 'metphi': 27363, 'mll': 27363}]),\n",
       " ('1_23300', [99104, {'lep1_iso': 26994, 'metphi': 27364, 'mll': 27364}]),\n",
       " ('1_23400', [99109, {'lep1_iso': 26995, 'metphi': 27365, 'mll': 27365}]),\n",
       " ('1_23500', [99114, {'lep1_iso': 26996, 'metphi': 27366, 'mll': 27366}]),\n",
       " ('1_23600', [99119, {'lep1_iso': 26997, 'metphi': 27367, 'mll': 27367}]),\n",
       " ('1_23700', [99124, {'lep1_iso': 26998, 'metphi': 27368, 'mll': 27368}]),\n",
       " ('1_23800', [99129, {'lep1_iso': 26999, 'metphi': 27369, 'mll': 27369}]),\n",
       " ('1_23900', [99134, {'lep1_iso': 27000, 'metphi': 27370, 'mll': 27370}]),\n",
       " ('1_24000', [99139, {'lep1_iso': 27001, 'metphi': 27371, 'mll': 27371}]),\n",
       " ('1_24100', [99144, {'lep1_iso': 27002, 'metphi': 27372, 'mll': 27372}]),\n",
       " ('1_24200', [99149, {'lep1_iso': 27003, 'metphi': 27373, 'mll': 27373}]),\n",
       " ('1_24300', [99154, {'lep1_iso': 27004, 'metphi': 27374, 'mll': 27374}]),\n",
       " ('1_24400', [99159, {'lep1_iso': 27005, 'metphi': 27375, 'mll': 27375}]),\n",
       " ('1_24500', [99164, {'lep1_iso': 27006, 'metphi': 27376, 'mll': 27376}]),\n",
       " ('1_24600', [99169, {'lep1_iso': 27007, 'metphi': 27377, 'mll': 27377}]),\n",
       " ('1_24700', [99174, {'lep1_iso': 27008, 'metphi': 27378, 'mll': 27378}]),\n",
       " ('1_24800', [99179, {'lep1_iso': 27009, 'metphi': 27379, 'mll': 27379}]),\n",
       " ('1_24900', [99184, {'lep1_iso': 27010, 'metphi': 27380, 'mll': 27380}]),\n",
       " ('1_25000', [99189, {'lep1_iso': 27011, 'metphi': 27381, 'mll': 27381}]),\n",
       " ('1_25100', [99194, {'lep1_iso': 27012, 'metphi': 27382, 'mll': 27382}]),\n",
       " ('1_25200', [99199, {'lep1_iso': 27013, 'metphi': 27383, 'mll': 27383}]),\n",
       " ('1_25300', [99204, {'lep1_iso': 27014, 'metphi': 27384, 'mll': 27384}]),\n",
       " ('1_25400', [99209, {'lep1_iso': 27015, 'metphi': 27385, 'mll': 27385}]),\n",
       " ('1_25500', [99214, {'lep1_iso': 27016, 'metphi': 27386, 'mll': 27386}]),\n",
       " ('1_25600', [99219, {'lep1_iso': 27017, 'metphi': 27387, 'mll': 27387}]),\n",
       " ('1_25700', [99224, {'lep1_iso': 27018, 'metphi': 27388, 'mll': 27388}]),\n",
       " ('1_25800', [99229, {'lep1_iso': 27019, 'metphi': 27389, 'mll': 27389}]),\n",
       " ('1_25900', [99234, {'lep1_iso': 27020, 'metphi': 27390, 'mll': 27390}]),\n",
       " ('1_26000', [99239, {'lep1_iso': 27021, 'metphi': 27391, 'mll': 27391}]),\n",
       " ('1_26100', [99244, {'lep1_iso': 27022, 'metphi': 27392, 'mll': 27392}]),\n",
       " ('1_26200', [99249, {'lep1_iso': 27023, 'metphi': 27393, 'mll': 27393}]),\n",
       " ('1_26300', [99254, {'lep1_iso': 27024, 'metphi': 27394, 'mll': 27394}]),\n",
       " ('1_26400', [99259, {'lep1_iso': 27025, 'metphi': 27395, 'mll': 27395}]),\n",
       " ('1_26500', [99264, {'lep1_iso': 27026, 'metphi': 27396, 'mll': 27396}]),\n",
       " ('1_26600', [99269, {'lep1_iso': 27027, 'metphi': 27397, 'mll': 27397}]),\n",
       " ('1_26700', [99274, {'lep1_iso': 27028, 'metphi': 27398, 'mll': 27398}]),\n",
       " ('1_26800', [99279, {'lep1_iso': 27029, 'metphi': 27399, 'mll': 27399}]),\n",
       " ('1_26900', [99284, {'lep1_iso': 27030, 'metphi': 27400, 'mll': 27400}]),\n",
       " ('1_27000', [99289, {'lep1_iso': 27031, 'metphi': 27401, 'mll': 27401}]),\n",
       " ('1_27100', [99294, {'lep1_iso': 27032, 'metphi': 27402, 'mll': 27402}]),\n",
       " ('1_27200', [99299, {'lep1_iso': 27033, 'metphi': 27403, 'mll': 27403}]),\n",
       " ('1_27300', [99304, {'lep1_iso': 27034, 'metphi': 27404, 'mll': 27404}]),\n",
       " ('1_27400', [99309, {'lep1_iso': 27035, 'metphi': 27405, 'mll': 27405}]),\n",
       " ('1_27500', [99314, {'lep1_iso': 27036, 'metphi': 27406, 'mll': 27406}]),\n",
       " ('1_27600', [99319, {'lep1_iso': 27037, 'metphi': 27407, 'mll': 27407}]),\n",
       " ('1_27700', [99324, {'lep1_iso': 27038, 'metphi': 27408, 'mll': 27408}]),\n",
       " ('1_27800', [99329, {'lep1_iso': 27039, 'metphi': 27409, 'mll': 27409}]),\n",
       " ('1_27900', [99334, {'lep1_iso': 27040, 'metphi': 27410, 'mll': 27410}]),\n",
       " ('1_28000', [99339, {'lep1_iso': 27041, 'metphi': 27411, 'mll': 27411}]),\n",
       " ('1_28100', [99344, {'lep1_iso': 27042, 'metphi': 27412, 'mll': 27412}]),\n",
       " ('1_28200', [99349, {'lep1_iso': 27043, 'metphi': 27413, 'mll': 27413}]),\n",
       " ('1_28300', [99354, {'lep1_iso': 27044, 'metphi': 27414, 'mll': 27414}]),\n",
       " ('1_28400', [99359, {'lep1_iso': 27045, 'metphi': 27415, 'mll': 27415}]),\n",
       " ('1_28500', [99364, {'lep1_iso': 27046, 'metphi': 27416, 'mll': 27416}]),\n",
       " ('1_28600', [99369, {'lep1_iso': 27047, 'metphi': 27417, 'mll': 27417}]),\n",
       " ('1_28700', [99374, {'lep1_iso': 27048, 'metphi': 27418, 'mll': 27418}]),\n",
       " ('1_28800', [99379, {'lep1_iso': 27049, 'metphi': 27419, 'mll': 27419}]),\n",
       " ('1_28900', [99384, {'lep1_iso': 27050, 'metphi': 27420, 'mll': 27420}]),\n",
       " ('1_29000', [99389, {'lep1_iso': 27051, 'metphi': 27421, 'mll': 27421}]),\n",
       " ('1_29100', [99394, {'lep1_iso': 27052, 'metphi': 27422, 'mll': 27422}]),\n",
       " ('1_29200', [99399, {'lep1_iso': 27053, 'metphi': 27423, 'mll': 27423}]),\n",
       " ('1_29300', [99404, {'lep1_iso': 27054, 'metphi': 27424, 'mll': 27424}]),\n",
       " ('1_29400', [99409, {'lep1_iso': 27055, 'metphi': 27425, 'mll': 27425}]),\n",
       " ('1_29500', [99414, {'lep1_iso': 27056, 'metphi': 27426, 'mll': 27426}]),\n",
       " ('1_29600', [99419, {'lep1_iso': 27057, 'metphi': 27427, 'mll': 27427}]),\n",
       " ('1_29700', [99424, {'lep1_iso': 27058, 'metphi': 27428, 'mll': 27428}]),\n",
       " ('1_29800', [99429, {'lep1_iso': 27059, 'metphi': 27429, 'mll': 27429}]),\n",
       " ('1_29900', [99434, {'lep1_iso': 27060, 'metphi': 27430, 'mll': 27430}]),\n",
       " ('1_30000', [99439, {'lep1_iso': 27061, 'metphi': 27431, 'mll': 27431}]),\n",
       " ('1_30100', [99444, {'lep1_iso': 27062, 'metphi': 27432, 'mll': 27432}]),\n",
       " ('1_30200', [99449, {'lep1_iso': 27063, 'metphi': 27433, 'mll': 27433}]),\n",
       " ('1_30300', [99454, {'lep1_iso': 27064, 'metphi': 27434, 'mll': 27434}]),\n",
       " ('1_30400', [99459, {'lep1_iso': 27065, 'metphi': 27435, 'mll': 27435}]),\n",
       " ('1_30500', [99464, {'lep1_iso': 27066, 'metphi': 27436, 'mll': 27436}]),\n",
       " ('1_30600', [99469, {'lep1_iso': 27067, 'metphi': 27437, 'mll': 27437}]),\n",
       " ('1_30700', [99474, {'lep1_iso': 27068, 'metphi': 27438, 'mll': 27438}]),\n",
       " ('1_30800', [99479, {'lep1_iso': 27069, 'metphi': 27439, 'mll': 27439}]),\n",
       " ('1_30900', [99484, {'lep1_iso': 27070, 'metphi': 27440, 'mll': 27440}]),\n",
       " ('1_31000', [99489, {'lep1_iso': 27071, 'metphi': 27441, 'mll': 27441}]),\n",
       " ('1_31100', [99494, {'lep1_iso': 27072, 'metphi': 27442, 'mll': 27442}]),\n",
       " ('1_31200', [99499, {'lep1_iso': 27073, 'metphi': 27443, 'mll': 27443}]),\n",
       " ('1_31300', [99504, {'lep1_iso': 27074, 'metphi': 27444, 'mll': 27444}]),\n",
       " ('1_31400', [99509, {'lep1_iso': 27075, 'metphi': 27445, 'mll': 27445}]),\n",
       " ('1_31500', [99514, {'lep1_iso': 27076, 'metphi': 27446, 'mll': 27446}]),\n",
       " ('1_31600', [99519, {'lep1_iso': 27077, 'metphi': 27447, 'mll': 27447}]),\n",
       " ('1_31700', [99524, {'lep1_iso': 27078, 'metphi': 27448, 'mll': 27448}]),\n",
       " ('1_31800', [99529, {'lep1_iso': 27079, 'metphi': 27449, 'mll': 27449}]),\n",
       " ('1_31900', [99534, {'lep1_iso': 27080, 'metphi': 27450, 'mll': 27450}]),\n",
       " ('1_32000', [99539, {'lep1_iso': 27081, 'metphi': 27451, 'mll': 27451}]),\n",
       " ('1_32100', [99544, {'lep1_iso': 27082, 'metphi': 27452, 'mll': 27452}]),\n",
       " ('1_32200', [99549, {'lep1_iso': 27083, 'metphi': 27453, 'mll': 27453}]),\n",
       " ('1_32300', [99554, {'lep1_iso': 27084, 'metphi': 27454, 'mll': 27454}]),\n",
       " ('1_32400', [99559, {'lep1_iso': 27085, 'metphi': 27455, 'mll': 27455}]),\n",
       " ('1_32500', [99564, {'lep1_iso': 27086, 'metphi': 27456, 'mll': 27456}]),\n",
       " ('1_32600', [99569, {'lep1_iso': 27087, 'metphi': 27457, 'mll': 27457}]),\n",
       " ('1_32700', [99574, {'lep1_iso': 27088, 'metphi': 27458, 'mll': 27458}]),\n",
       " ('1_32800', [99579, {'lep1_iso': 27089, 'metphi': 27459, 'mll': 27459}]),\n",
       " ('1_32900', [99584, {'lep1_iso': 27090, 'metphi': 27460, 'mll': 27460}]),\n",
       " ('1_33000', [99589, {'lep1_iso': 27091, 'metphi': 27461, 'mll': 27461}]),\n",
       " ('1_33100', [99594, {'lep1_iso': 27092, 'metphi': 27462, 'mll': 27462}]),\n",
       " ('1_33200', [99599, {'lep1_iso': 27093, 'metphi': 27463, 'mll': 27463}]),\n",
       " ('1_33300', [99604, {'lep1_iso': 27094, 'metphi': 27464, 'mll': 27464}]),\n",
       " ('1_33400', [99609, {'lep1_iso': 27095, 'metphi': 27465, 'mll': 27465}]),\n",
       " ('1_33500', [99614, {'lep1_iso': 27096, 'metphi': 27466, 'mll': 27466}]),\n",
       " ('1_33600', [99619, {'lep1_iso': 27097, 'metphi': 27467, 'mll': 27467}]),\n",
       " ('1_33700', [99624, {'lep1_iso': 27098, 'metphi': 27468, 'mll': 27468}]),\n",
       " ('1_33800', [99629, {'lep1_iso': 27099, 'metphi': 27469, 'mll': 27469}]),\n",
       " ('1_33900', [99634, {'lep1_iso': 27100, 'metphi': 27470, 'mll': 27470}]),\n",
       " ('1_34000', [99639, {'lep1_iso': 27101, 'metphi': 27471, 'mll': 27471}]),\n",
       " ('1_34100', [99644, {'lep1_iso': 27102, 'metphi': 27472, 'mll': 27472}]),\n",
       " ('1_34200', [99649, {'lep1_iso': 27103, 'metphi': 27473, 'mll': 27473}]),\n",
       " ('1_34300', [99654, {'lep1_iso': 27104, 'metphi': 27474, 'mll': 27474}]),\n",
       " ('1_34400', [99659, {'lep1_iso': 27105, 'metphi': 27475, 'mll': 27475}]),\n",
       " ('1_34500', [99664, {'lep1_iso': 27106, 'metphi': 27476, 'mll': 27476}]),\n",
       " ('1_34600', [99669, {'lep1_iso': 27107, 'metphi': 27477, 'mll': 27477}]),\n",
       " ('1_34700', [99674, {'lep1_iso': 27108, 'metphi': 27478, 'mll': 27478}]),\n",
       " ('1_34800', [99679, {'lep1_iso': 27109, 'metphi': 27479, 'mll': 27479}]),\n",
       " ('1_34900', [99684, {'lep1_iso': 27110, 'metphi': 27480, 'mll': 27480}]),\n",
       " ('1_35000', [99689, {'lep1_iso': 27111, 'metphi': 27481, 'mll': 27481}]),\n",
       " ('1_35100', [99694, {'lep1_iso': 27112, 'metphi': 27482, 'mll': 27482}]),\n",
       " ('1_35200', [99699, {'lep1_iso': 27113, 'metphi': 27483, 'mll': 27483}]),\n",
       " ('1_35300', [99704, {'lep1_iso': 27114, 'metphi': 27484, 'mll': 27484}]),\n",
       " ('1_35400', [99709, {'lep1_iso': 27115, 'metphi': 27485, 'mll': 27485}]),\n",
       " ('1_35500', [99714, {'lep1_iso': 27116, 'metphi': 27486, 'mll': 27486}]),\n",
       " ('1_35600', [99719, {'lep1_iso': 27117, 'metphi': 27487, 'mll': 27487}]),\n",
       " ('1_35700', [99724, {'lep1_iso': 27118, 'metphi': 27488, 'mll': 27488}]),\n",
       " ('1_35800', [99729, {'lep1_iso': 27119, 'metphi': 27489, 'mll': 27489}]),\n",
       " ('1_35900', [99734, {'lep1_iso': 27120, 'metphi': 27490, 'mll': 27490}]),\n",
       " ('1_36000', [99739, {'lep1_iso': 27121, 'metphi': 27491, 'mll': 27491}]),\n",
       " ('1_36100', [99744, {'lep1_iso': 27122, 'metphi': 27492, 'mll': 27492}]),\n",
       " ('1_36200', [99749, {'lep1_iso': 27123, 'metphi': 27493, 'mll': 27493}]),\n",
       " ('1_36300', [99754, {'lep1_iso': 27124, 'metphi': 27494, 'mll': 27494}]),\n",
       " ('1_36400', [99759, {'lep1_iso': 27125, 'metphi': 27495, 'mll': 27495}]),\n",
       " ('1_36500', [99764, {'lep1_iso': 27126, 'metphi': 27496, 'mll': 27496}]),\n",
       " ('1_36600', [99769, {'lep1_iso': 27127, 'metphi': 27497, 'mll': 27497}]),\n",
       " ('1_36700', [99774, {'lep1_iso': 27128, 'metphi': 27498, 'mll': 27498}]),\n",
       " ('1_36800', [99779, {'lep1_iso': 27129, 'metphi': 27499, 'mll': 27499}]),\n",
       " ('1_36900', [99784, {'lep1_iso': 27130, 'metphi': 27500, 'mll': 27500}]),\n",
       " ('1_37000', [99789, {'lep1_iso': 27131, 'metphi': 27501, 'mll': 27501}]),\n",
       " ('1_37100', [99794, {'lep1_iso': 27132, 'metphi': 27502, 'mll': 27502}]),\n",
       " ('1_37200', [99799, {'lep1_iso': 27133, 'metphi': 27503, 'mll': 27503}]),\n",
       " ('1_37300', [99804, {'lep1_iso': 27134, 'metphi': 27504, 'mll': 27504}]),\n",
       " ('1_37400', [99809, {'lep1_iso': 27135, 'metphi': 27505, 'mll': 27505}]),\n",
       " ('1_37500', [99814, {'lep1_iso': 27136, 'metphi': 27506, 'mll': 27506}]),\n",
       " ('1_37600', [99819, {'lep1_iso': 27137, 'metphi': 27507, 'mll': 27507}]),\n",
       " ('1_37700', [99824, {'lep1_iso': 27138, 'metphi': 27508, 'mll': 27508}]),\n",
       " ('1_37800', [99829, {'lep1_iso': 27139, 'metphi': 27509, 'mll': 27509}]),\n",
       " ('1_37900', [99834, {'lep1_iso': 27140, 'metphi': 27510, 'mll': 27510}]),\n",
       " ('1_38000', [99839, {'lep1_iso': 27141, 'metphi': 27511, 'mll': 27511}]),\n",
       " ('1_38100', [99844, {'lep1_iso': 27142, 'metphi': 27512, 'mll': 27512}]),\n",
       " ('1_38200', [99849, {'lep1_iso': 27143, 'metphi': 27513, 'mll': 27513}]),\n",
       " ('1_38300', [99854, {'lep1_iso': 27144, 'metphi': 27514, 'mll': 27514}]),\n",
       " ('1_38400', [99859, {'lep1_iso': 27145, 'metphi': 27515, 'mll': 27515}]),\n",
       " ('1_38500', [99864, {'lep1_iso': 27146, 'metphi': 27516, 'mll': 27516}]),\n",
       " ('1_38600', [99869, {'lep1_iso': 27147, 'metphi': 27517, 'mll': 27517}]),\n",
       " ('1_38700', [99874, {'lep1_iso': 27148, 'metphi': 27518, 'mll': 27518}]),\n",
       " ('1_38800', [99879, {'lep1_iso': 27149, 'metphi': 27519, 'mll': 27519}]),\n",
       " ('1_38900', [99884, {'lep1_iso': 27150, 'metphi': 27520, 'mll': 27520}]),\n",
       " ('1_39000', [99889, {'lep1_iso': 27151, 'metphi': 27521, 'mll': 27521}]),\n",
       " ('1_39100', [99894, {'lep1_iso': 27152, 'metphi': 27522, 'mll': 27522}]),\n",
       " ('1_39200', [99899, {'lep1_iso': 27153, 'metphi': 27523, 'mll': 27523}]),\n",
       " ('1_39300', [99904, {'lep1_iso': 27154, 'metphi': 27524, 'mll': 27524}]),\n",
       " ('1_39400', [99909, {'lep1_iso': 27155, 'metphi': 27525, 'mll': 27525}]),\n",
       " ('1_39500', [99914, {'lep1_iso': 27156, 'metphi': 27526, 'mll': 27526}]),\n",
       " ('1_39600', [99919, {'lep1_iso': 27157, 'metphi': 27527, 'mll': 27527}]),\n",
       " ('1_39700', [99924, {'lep1_iso': 27158, 'metphi': 27528, 'mll': 27528}]),\n",
       " ('1_39800', [99929, {'lep1_iso': 27159, 'metphi': 27529, 'mll': 27529}]),\n",
       " ('1_39900', [99934, {'lep1_iso': 27160, 'metphi': 27530, 'mll': 27530}]),\n",
       " ('1_40000', [99939, {'lep1_iso': 27161, 'metphi': 27531, 'mll': 27531}]),\n",
       " ('4_100', [264450, {'lep1_iso': 29810, 'metphi': 30150, 'mll': 30150}]),\n",
       " ('4_8500', [268418, {'lep1_iso': 29894, 'metphi': 30234, 'mll': 30234}]),\n",
       " ('4_16200', [268867, {'lep1_iso': 29971, 'metphi': 30311, 'mll': 30311}]),\n",
       " ('4_19300', [269178, {'lep1_iso': 30002, 'metphi': 30342, 'mll': 30342}]),\n",
       " ('4_13800', [269383, {'lep1_iso': 29947, 'metphi': 30287, 'mll': 30287}]),\n",
       " ('4_17600', [269601, {'lep1_iso': 29985, 'metphi': 30325, 'mll': 30325}]),\n",
       " ('4_16300', [269800, {'lep1_iso': 29972, 'metphi': 30312, 'mll': 30312}]),\n",
       " ('4_19100', [269980, {'lep1_iso': 30000, 'metphi': 30340, 'mll': 30340}]),\n",
       " ('4_15400', [270235, {'lep1_iso': 29963, 'metphi': 30303, 'mll': 30303}]),\n",
       " ('4_11400', [270543, {'lep1_iso': 29923, 'metphi': 30263, 'mll': 30263}]),\n",
       " ('4_14100', [270710, {'lep1_iso': 29950, 'metphi': 30290, 'mll': 30290}]),\n",
       " ('4_17400', [271139, {'lep1_iso': 29983, 'metphi': 30323, 'mll': 30323}]),\n",
       " ('4_17800', [271283, {'lep1_iso': 29987, 'metphi': 30327, 'mll': 30327}]),\n",
       " ('4_12800', [271461, {'lep1_iso': 29937, 'metphi': 30277, 'mll': 30277}]),\n",
       " ('4_16800', [271613, {'lep1_iso': 29977, 'metphi': 30317, 'mll': 30317}]),\n",
       " ('4_16000', [271693, {'lep1_iso': 29969, 'metphi': 30309, 'mll': 30309}]),\n",
       " ('4_11300', [271726, {'lep1_iso': 29922, 'metphi': 30262, 'mll': 30262}]),\n",
       " ('4_18900', [271886, {'lep1_iso': 29998, 'metphi': 30338, 'mll': 30338}]),\n",
       " ('4_8400', [272085, {'lep1_iso': 29893, 'metphi': 30233, 'mll': 30233}]),\n",
       " ('4_18800', [272149, {'lep1_iso': 29997, 'metphi': 30337, 'mll': 30337}]),\n",
       " ('4_18000', [272229, {'lep1_iso': 29989, 'metphi': 30329, 'mll': 30329}]),\n",
       " ('4_17300', [272230, {'lep1_iso': 29982, 'metphi': 30322, 'mll': 30322}]),\n",
       " ('4_9800', [272359, {'lep1_iso': 29907, 'metphi': 30247, 'mll': 30247}]),\n",
       " ('4_9500', [272504, {'lep1_iso': 29904, 'metphi': 30244, 'mll': 30244}]),\n",
       " ('4_7600', [272809, {'lep1_iso': 29885, 'metphi': 30225, 'mll': 30225}]),\n",
       " ('4_15300', [273074, {'lep1_iso': 29962, 'metphi': 30302, 'mll': 30302}]),\n",
       " ('4_10000', [273121, {'lep1_iso': 29909, 'metphi': 30249, 'mll': 30249}]),\n",
       " ('4_17000', [273203, {'lep1_iso': 29979, 'metphi': 30319, 'mll': 30319}]),\n",
       " ('4_11100', [273264, {'lep1_iso': 29920, 'metphi': 30260, 'mll': 30260}]),\n",
       " ('4_16900', [273558, {'lep1_iso': 29978, 'metphi': 30318, 'mll': 30318}]),\n",
       " ('4_13500', [273576, {'lep1_iso': 29944, 'metphi': 30284, 'mll': 30284}]),\n",
       " ('4_19000', [273647, {'lep1_iso': 29999, 'metphi': 30339, 'mll': 30339}]),\n",
       " ('4_19200', [273765, {'lep1_iso': 30001, 'metphi': 30341, 'mll': 30341}]),\n",
       " ('4_15800', [273875, {'lep1_iso': 29967, 'metphi': 30307, 'mll': 30307}]),\n",
       " ('4_16100', [274098, {'lep1_iso': 29970, 'metphi': 30310, 'mll': 30310}]),\n",
       " ('4_15500', [274112, {'lep1_iso': 29964, 'metphi': 30304, 'mll': 30304}]),\n",
       " ('4_13700', [274246, {'lep1_iso': 29946, 'metphi': 30286, 'mll': 30286}]),\n",
       " ('4_16700', [274728, {'lep1_iso': 29976, 'metphi': 30316, 'mll': 30316}]),\n",
       " ('4_11000', [274999, {'lep1_iso': 29919, 'metphi': 30259, 'mll': 30259}]),\n",
       " ('4_6000', [275085, {'lep1_iso': 29869, 'metphi': 30209, 'mll': 30209}]),\n",
       " ('4_18300', [275212, {'lep1_iso': 29992, 'metphi': 30332, 'mll': 30332}]),\n",
       " ('4_3900', [275272, {'lep1_iso': 29848, 'metphi': 30188, 'mll': 30188}]),\n",
       " ('4_18500', [275330, {'lep1_iso': 29994, 'metphi': 30334, 'mll': 30334}]),\n",
       " ('4_12500', [275378, {'lep1_iso': 29934, 'metphi': 30274, 'mll': 30274}]),\n",
       " ('4_17200', [275437, {'lep1_iso': 29981, 'metphi': 30321, 'mll': 30321}]),\n",
       " ('4_11600', [275629, {'lep1_iso': 29925, 'metphi': 30265, 'mll': 30265}]),\n",
       " ('4_17900', [275804, {'lep1_iso': 29988, 'metphi': 30328, 'mll': 30328}]),\n",
       " ('4_8800', [275817, {'lep1_iso': 29897, 'metphi': 30237, 'mll': 30237}]),\n",
       " ('4_7800', [275871, {'lep1_iso': 29887, 'metphi': 30227, 'mll': 30227}]),\n",
       " ('4_16600', [275911, {'lep1_iso': 29975, 'metphi': 30315, 'mll': 30315}]),\n",
       " ('4_9400', [275987, {'lep1_iso': 29903, 'metphi': 30243, 'mll': 30243}]),\n",
       " ('4_13100', [276100, {'lep1_iso': 29940, 'metphi': 30280, 'mll': 30280}]),\n",
       " ('4_18100', [276198, {'lep1_iso': 29990, 'metphi': 30330, 'mll': 30330}]),\n",
       " ('4_11900', [276220, {'lep1_iso': 29928, 'metphi': 30268, 'mll': 30268}]),\n",
       " ('4_14900', [276334, {'lep1_iso': 29958, 'metphi': 30298, 'mll': 30298}]),\n",
       " ('4_14800', [276597, {'lep1_iso': 29957, 'metphi': 30297, 'mll': 30297}]),\n",
       " ('4_17500', [276948, {'lep1_iso': 29984, 'metphi': 30324, 'mll': 30324}]),\n",
       " ('4_9300', [277170, {'lep1_iso': 29902, 'metphi': 30242, 'mll': 30242}]),\n",
       " ('4_18200', [277315, {'lep1_iso': 29991, 'metphi': 30331, 'mll': 30331}]),\n",
       " ('4_15900', [277568, {'lep1_iso': 29968, 'metphi': 30308, 'mll': 30308}]),\n",
       " ('4_17100', [277816, {'lep1_iso': 29980, 'metphi': 30320, 'mll': 30320}]),\n",
       " ('4_19400', [277931, {'lep1_iso': 30003, 'metphi': 30343, 'mll': 30343}]),\n",
       " ('4_7000', [277975, {'lep1_iso': 29879, 'metphi': 30219, 'mll': 30219}]),\n",
       " ('4_10700', [278206, {'lep1_iso': 29916, 'metphi': 30256, 'mll': 30256}]),\n",
       " ('4_18400', [278275, {'lep1_iso': 29993, 'metphi': 30333, 'mll': 30333}]),\n",
       " ('4_13200', [278717, {'lep1_iso': 29941, 'metphi': 30281, 'mll': 30281}]),\n",
       " ('4_7900', [278736, {'lep1_iso': 29888, 'metphi': 30228, 'mll': 30228}]),\n",
       " ('4_14300', [278740, {'lep1_iso': 29952, 'metphi': 30292, 'mll': 30292}]),\n",
       " ('4_12300', [278848, {'lep1_iso': 29932, 'metphi': 30272, 'mll': 30272}]),\n",
       " ('4_13900', [278872, {'lep1_iso': 29948, 'metphi': 30288, 'mll': 30288}]),\n",
       " ('4_5300', [279226, {'lep1_iso': 29862, 'metphi': 30202, 'mll': 30202}]),\n",
       " ('4_5500', [279252, {'lep1_iso': 29864, 'metphi': 30204, 'mll': 30204}]),\n",
       " ('4_4100', [279438, {'lep1_iso': 29850, 'metphi': 30190, 'mll': 30190}]),\n",
       " ('4_12600', [279531, {'lep1_iso': 29935, 'metphi': 30275, 'mll': 30275}]),\n",
       " ('4_3600', [279557, {'lep1_iso': 29845, 'metphi': 30185, 'mll': 30185}]),\n",
       " ('4_3800', [279675, {'lep1_iso': 29847, 'metphi': 30187, 'mll': 30187}]),\n",
       " ('4_13000', [279675, {'lep1_iso': 29939, 'metphi': 30279, 'mll': 30279}]),\n",
       " ('4_14700', [279712, {'lep1_iso': 29956, 'metphi': 30296, 'mll': 30296}]),\n",
       " ('4_12200', [280399, {'lep1_iso': 29931, 'metphi': 30271, 'mll': 30271}]),\n",
       " ('4_13600', [280489, {'lep1_iso': 29945, 'metphi': 30285, 'mll': 30285}]),\n",
       " ('4_16400', [280669, {'lep1_iso': 29973, 'metphi': 30313, 'mll': 30313}]),\n",
       " ('4_15600', [280841, {'lep1_iso': 29965, 'metphi': 30305, 'mll': 30305}]),\n",
       " ('4_18600', [281415, {'lep1_iso': 29995, 'metphi': 30335, 'mll': 30335}]),\n",
       " ('4_8200', [281535, {'lep1_iso': 29891, 'metphi': 30231, 'mll': 30231}]),\n",
       " ('4_7100', [281760, {'lep1_iso': 29880, 'metphi': 30220, 'mll': 30220}]),\n",
       " ('4_9000', [281823, {'lep1_iso': 29899, 'metphi': 30239, 'mll': 30239}]),\n",
       " ('4_14500', [282170, {'lep1_iso': 29954, 'metphi': 30294, 'mll': 30294}]),\n",
       " ('4_6600', [282339, {'lep1_iso': 29875, 'metphi': 30215, 'mll': 30215}]),\n",
       " ('4_18700', [282440, {'lep1_iso': 29996, 'metphi': 30336, 'mll': 30336}]),\n",
       " ('4_9600', [282649, {'lep1_iso': 29905, 'metphi': 30245, 'mll': 30245}]),\n",
       " ('4_10900', [282898, {'lep1_iso': 29918, 'metphi': 30258, 'mll': 30258}]),\n",
       " ('4_6300', [283060, {'lep1_iso': 29872, 'metphi': 30212, 'mll': 30212}]),\n",
       " ('4_7500', [283192, {'lep1_iso': 29884, 'metphi': 30224, 'mll': 30224}]),\n",
       " ('4_11200', [283305, {'lep1_iso': 29921, 'metphi': 30261, 'mll': 30261}]),\n",
       " ('4_6500', [283430, {'lep1_iso': 29874, 'metphi': 30214, 'mll': 30214}]),\n",
       " ('4_5400', [283563, {'lep1_iso': 29863, 'metphi': 30203, 'mll': 30203}]),\n",
       " ('4_4900', [283590, {'lep1_iso': 29858, 'metphi': 30198, 'mll': 30198}]),\n",
       " ('4_14200', [283603, {'lep1_iso': 29951, 'metphi': 30291, 'mll': 30291}]),\n",
       " ('4_16500', [283626, {'lep1_iso': 29974, 'metphi': 30314, 'mll': 30314}]),\n",
       " ('4_14000', [283761, {'lep1_iso': 29949, 'metphi': 30289, 'mll': 30289}]),\n",
       " ('4_9900', [283964, {'lep1_iso': 29908, 'metphi': 30248, 'mll': 30248}]),\n",
       " ('4_8100', [284098, {'lep1_iso': 29890, 'metphi': 30230, 'mll': 30230}]),\n",
       " ('4_12700', [284328, {'lep1_iso': 29936, 'metphi': 30276, 'mll': 30276}]),\n",
       " ('4_15700', [284350, {'lep1_iso': 29966, 'metphi': 30306, 'mll': 30306}]),\n",
       " ('4_2700', [284408, {'lep1_iso': 29836, 'metphi': 30176, 'mll': 30176}]),\n",
       " ('4_7300', [284454, {'lep1_iso': 29882, 'metphi': 30222, 'mll': 30222}]),\n",
       " ('4_15100', [284548, {'lep1_iso': 29960, 'metphi': 30300, 'mll': 30300}]),\n",
       " ('4_11700', [284566, {'lep1_iso': 29926, 'metphi': 30266, 'mll': 30266}]),\n",
       " ('4_10300', [284660, {'lep1_iso': 29912, 'metphi': 30252, 'mll': 30252}]),\n",
       " ('4_13300', [284866, {'lep1_iso': 29942, 'metphi': 30282, 'mll': 30282}]),\n",
       " ('4_6900', [284954, {'lep1_iso': 29878, 'metphi': 30218, 'mll': 30218}]),\n",
       " ('4_11500', [285000, {'lep1_iso': 29924, 'metphi': 30264, 'mll': 30264}]),\n",
       " ('4_7400', [285111, {'lep1_iso': 29883, 'metphi': 30223, 'mll': 30223}]),\n",
       " ('4_4000', [285405, {'lep1_iso': 29849, 'metphi': 30189, 'mll': 30189}]),\n",
       " ('4_2800', [285709, {'lep1_iso': 29837, 'metphi': 30177, 'mll': 30177}]),\n",
       " ('4_15000', [285731, {'lep1_iso': 29959, 'metphi': 30299, 'mll': 30299}]),\n",
       " ('4_7700', [285794, {'lep1_iso': 29886, 'metphi': 30226, 'mll': 30226}]),\n",
       " ('4_11800', [285803, {'lep1_iso': 29927, 'metphi': 30267, 'mll': 30267}]),\n",
       " ('4_3100', [286116, {'lep1_iso': 29840, 'metphi': 30180, 'mll': 30180}]),\n",
       " ('4_4300', [286180, {'lep1_iso': 29852, 'metphi': 30192, 'mll': 30192}]),\n",
       " ('4_4400', [286469, {'lep1_iso': 29853, 'metphi': 30193, 'mll': 30193}]),\n",
       " ('4_12000', [286629, {'lep1_iso': 29929, 'metphi': 30269, 'mll': 30269}]),\n",
       " ('4_10800', [286657, {'lep1_iso': 29917, 'metphi': 30257, 'mll': 30257}]),\n",
       " ('4_3200', [286957, {'lep1_iso': 29841, 'metphi': 30181, 'mll': 30181}]),\n",
       " ('4_10500', [286986, {'lep1_iso': 29914, 'metphi': 30254, 'mll': 30254}]),\n",
       " ('4_17700', [287094, {'lep1_iso': 29986, 'metphi': 30326, 'mll': 30326}]),\n",
       " ('4_8000', [287121, {'lep1_iso': 29889, 'metphi': 30229, 'mll': 30229}]),\n",
       " ('4_8700', [287212, {'lep1_iso': 29896, 'metphi': 30236, 'mll': 30236}]),\n",
       " ('4_9100', [287264, {'lep1_iso': 29900, 'metphi': 30240, 'mll': 30240}]),\n",
       " ('4_9200', [287369, {'lep1_iso': 29901, 'metphi': 30241, 'mll': 30241}]),\n",
       " ('4_4700', [287428, {'lep1_iso': 29856, 'metphi': 30196, 'mll': 30196}]),\n",
       " ('4_5800', [287479, {'lep1_iso': 29867, 'metphi': 30207, 'mll': 30207}]),\n",
       " ('4_12400', [287509, {'lep1_iso': 29933, 'metphi': 30273, 'mll': 30273}]),\n",
       " ('4_1900', [287524, {'lep1_iso': 29828, 'metphi': 30168, 'mll': 30168}]),\n",
       " ('4_4200', [287731, {'lep1_iso': 29851, 'metphi': 30191, 'mll': 30191}]),\n",
       " ('4_12900', [287942, {'lep1_iso': 29938, 'metphi': 30278, 'mll': 30278}]),\n",
       " ('4_8300', [287988, {'lep1_iso': 29892, 'metphi': 30232, 'mll': 30232}]),\n",
       " ('4_10200', [288235, {'lep1_iso': 29911, 'metphi': 30251, 'mll': 30251}]),\n",
       " ('4_3500', [288468, {'lep1_iso': 29844, 'metphi': 30184, 'mll': 30184}]),\n",
       " ('4_6100', [288990, {'lep1_iso': 29870, 'metphi': 30210, 'mll': 30210}]),\n",
       " ('4_3400', [289007, {'lep1_iso': 29843, 'metphi': 30183, 'mll': 30183}]),\n",
       " ('4_12100', [289402, {'lep1_iso': 29930, 'metphi': 30270, 'mll': 30270}]),\n",
       " ('4_8900', [289446, {'lep1_iso': 29898, 'metphi': 30238, 'mll': 30238}]),\n",
       " ('4_3700', [289598, {'lep1_iso': 29846, 'metphi': 30186, 'mll': 30186}]),\n",
       " ('4_5000', [289675, {'lep1_iso': 29859, 'metphi': 30199, 'mll': 30199}]),\n",
       " ('4_15200', [289805, {'lep1_iso': 29961, 'metphi': 30301, 'mll': 30301}]),\n",
       " ('4_4600', [289899, {'lep1_iso': 29855, 'metphi': 30195, 'mll': 30195}]),\n",
       " ('4_14600', [289911, {'lep1_iso': 29955, 'metphi': 30295, 'mll': 30295}]),\n",
       " ('4_14400', [290345, {'lep1_iso': 29953, 'metphi': 30293, 'mll': 30293}]),\n",
       " ('4_10400', [290377, {'lep1_iso': 29913, 'metphi': 30253, 'mll': 30253}]),\n",
       " ('4_3300', [290650, {'lep1_iso': 29842, 'metphi': 30182, 'mll': 30182}]),\n",
       " ('4_5600', [290857, {'lep1_iso': 29865, 'metphi': 30205, 'mll': 30205}]),\n",
       " ('4_9700', [291298, {'lep1_iso': 29906, 'metphi': 30246, 'mll': 30246}]),\n",
       " ('4_8600', [291523, {'lep1_iso': 29895, 'metphi': 30235, 'mll': 30235}]),\n",
       " ('4_4500', [291542, {'lep1_iso': 29854, 'metphi': 30194, 'mll': 30194}]),\n",
       " ('4_2100', [291714, {'lep1_iso': 29830, 'metphi': 30170, 'mll': 30170}]),\n",
       " ('4_13400', [291779, {'lep1_iso': 29943, 'metphi': 30283, 'mll': 30283}]),\n",
       " ('4_6700', [291854, {'lep1_iso': 29876, 'metphi': 30216, 'mll': 30216}]),\n",
       " ('4_2900', [291886, {'lep1_iso': 29838, 'metphi': 30178, 'mll': 30178}]),\n",
       " ('4_4800', [291949, {'lep1_iso': 29857, 'metphi': 30197, 'mll': 30197}]),\n",
       " ('4_5700', [292894, {'lep1_iso': 29866, 'metphi': 30206, 'mll': 30206}]),\n",
       " ('4_5200', [292921, {'lep1_iso': 29861, 'metphi': 30201, 'mll': 30201}]),\n",
       " ('4_10100', [293006, {'lep1_iso': 29910, 'metphi': 30250, 'mll': 30250}]),\n",
       " ('4_1700', [293018, {'lep1_iso': 29826, 'metphi': 30166, 'mll': 30166}]),\n",
       " ('4_5100', [293092, {'lep1_iso': 29860, 'metphi': 30200, 'mll': 30200}]),\n",
       " ('4_6800', [293129, {'lep1_iso': 29877, 'metphi': 30217, 'mll': 30217}]),\n",
       " ('4_2200', [293175, {'lep1_iso': 29831, 'metphi': 30171, 'mll': 30171}]),\n",
       " ('4_3000', [293831, {'lep1_iso': 29839, 'metphi': 30179, 'mll': 30179}]),\n",
       " ('4_10600', [293991, {'lep1_iso': 29915, 'metphi': 30255, 'mll': 30255}]),\n",
       " ('4_7200', [294009, {'lep1_iso': 29881, 'metphi': 30221, 'mll': 30221}]),\n",
       " ('4_2600', [294055, {'lep1_iso': 29835, 'metphi': 30175, 'mll': 30175}]),\n",
       " ('4_6200', [294063, {'lep1_iso': 29871, 'metphi': 30211, 'mll': 30211}]),\n",
       " ('4_2000', [294069, {'lep1_iso': 29829, 'metphi': 30169, 'mll': 30169}]),\n",
       " ('4_2500', [294686, {'lep1_iso': 29834, 'metphi': 30174, 'mll': 30174}]),\n",
       " ('4_2400', [295593, {'lep1_iso': 29833, 'metphi': 30173, 'mll': 30173}]),\n",
       " ('4_300', [296056, {'lep1_iso': 29812, 'metphi': 30152, 'mll': 30152}]),\n",
       " ('4_6400', [296297, {'lep1_iso': 29873, 'metphi': 30213, 'mll': 30213}]),\n",
       " ('4_2300', [296868, {'lep1_iso': 29832, 'metphi': 30172, 'mll': 30172}]),\n",
       " ('4_600', [296923, {'lep1_iso': 29815, 'metphi': 30155, 'mll': 30155}]),\n",
       " ('4_1600', [296961, {'lep1_iso': 29825, 'metphi': 30165, 'mll': 30165}]),\n",
       " ('4_5900', [296968, {'lep1_iso': 29868, 'metphi': 30208, 'mll': 30208}]),\n",
       " ('4_1200', [298381, {'lep1_iso': 29821, 'metphi': 30161, 'mll': 30161}]),\n",
       " ('4_1300', [298394, {'lep1_iso': 29822, 'metphi': 30162, 'mll': 30162}]),\n",
       " ('4_1100', [298828, {'lep1_iso': 29820, 'metphi': 30160, 'mll': 30160}]),\n",
       " ('4_900', [317644, {'lep1_iso': 29818, 'metphi': 30158, 'mll': 30158}]),\n",
       " ('4_1400', [317897, {'lep1_iso': 29823, 'metphi': 30163, 'mll': 30163}]),\n",
       " ('4_500', [318414, {'lep1_iso': 29814, 'metphi': 30154, 'mll': 30154}]),\n",
       " ('4_700', [318626, {'lep1_iso': 29816, 'metphi': 30156, 'mll': 30156}]),\n",
       " ('4_1000', [318763, {'lep1_iso': 29819, 'metphi': 30159, 'mll': 30159}]),\n",
       " ('4_1500', [318924, {'lep1_iso': 29824, 'metphi': 30164, 'mll': 30164}]),\n",
       " ('4_800', [319101, {'lep1_iso': 29817, 'metphi': 30157, 'mll': 30157}]),\n",
       " ('4_200', [319291, {'lep1_iso': 29811, 'metphi': 30151, 'mll': 30151}]),\n",
       " ('4_1800', [319333, {'lep1_iso': 29827, 'metphi': 30167, 'mll': 30167}]),\n",
       " ('4_400', [319411, {'lep1_iso': 29813, 'metphi': 30153, 'mll': 30153}]),\n",
       " ('4_19500', [342648, {'lep1_iso': 30004, 'metphi': 30344, 'mll': 30344}]),\n",
       " ('4_19600', [342665, {'lep1_iso': 30005, 'metphi': 30345, 'mll': 30345}]),\n",
       " ('4_19700', [342682, {'lep1_iso': 30006, 'metphi': 30346, 'mll': 30346}]),\n",
       " ('4_19800', [342699, {'lep1_iso': 30007, 'metphi': 30347, 'mll': 30347}]),\n",
       " ('4_19900', [342716, {'lep1_iso': 30008, 'metphi': 30348, 'mll': 30348}]),\n",
       " ('4_20000', [342733, {'lep1_iso': 30009, 'metphi': 30349, 'mll': 30349}]),\n",
       " ('4_20100', [342750, {'lep1_iso': 30010, 'metphi': 30350, 'mll': 30350}]),\n",
       " ('4_20200', [342767, {'lep1_iso': 30011, 'metphi': 30351, 'mll': 30351}]),\n",
       " ('4_20300', [342784, {'lep1_iso': 30012, 'metphi': 30352, 'mll': 30352}]),\n",
       " ('4_20400', [342801, {'lep1_iso': 30013, 'metphi': 30353, 'mll': 30353}]),\n",
       " ('4_20500', [342818, {'lep1_iso': 30014, 'metphi': 30354, 'mll': 30354}]),\n",
       " ('4_20600', [342835, {'lep1_iso': 30015, 'metphi': 30355, 'mll': 30355}]),\n",
       " ('4_20700', [342852, {'lep1_iso': 30016, 'metphi': 30356, 'mll': 30356}]),\n",
       " ('4_20800', [342869, {'lep1_iso': 30017, 'metphi': 30357, 'mll': 30357}]),\n",
       " ('4_20900', [342886, {'lep1_iso': 30018, 'metphi': 30358, 'mll': 30358}]),\n",
       " ('4_21000', [342903, {'lep1_iso': 30019, 'metphi': 30359, 'mll': 30359}]),\n",
       " ('4_21100', [342920, {'lep1_iso': 30020, 'metphi': 30360, 'mll': 30360}]),\n",
       " ('4_21200', [342937, {'lep1_iso': 30021, 'metphi': 30361, 'mll': 30361}]),\n",
       " ('4_21300', [342954, {'lep1_iso': 30022, 'metphi': 30362, 'mll': 30362}]),\n",
       " ('4_21400', [342971, {'lep1_iso': 30023, 'metphi': 30363, 'mll': 30363}]),\n",
       " ('4_21500', [342988, {'lep1_iso': 30024, 'metphi': 30364, 'mll': 30364}]),\n",
       " ('4_21600', [343005, {'lep1_iso': 30025, 'metphi': 30365, 'mll': 30365}]),\n",
       " ('4_21700', [343022, {'lep1_iso': 30026, 'metphi': 30366, 'mll': 30366}]),\n",
       " ('4_21800', [343039, {'lep1_iso': 30027, 'metphi': 30367, 'mll': 30367}]),\n",
       " ('4_21900', [343056, {'lep1_iso': 30028, 'metphi': 30368, 'mll': 30368}]),\n",
       " ('4_22000', [343073, {'lep1_iso': 30029, 'metphi': 30369, 'mll': 30369}]),\n",
       " ('4_22100', [343090, {'lep1_iso': 30030, 'metphi': 30370, 'mll': 30370}]),\n",
       " ('4_22200', [343107, {'lep1_iso': 30031, 'metphi': 30371, 'mll': 30371}]),\n",
       " ('4_22300', [343124, {'lep1_iso': 30032, 'metphi': 30372, 'mll': 30372}]),\n",
       " ('4_22400', [343141, {'lep1_iso': 30033, 'metphi': 30373, 'mll': 30373}]),\n",
       " ('4_22500', [343158, {'lep1_iso': 30034, 'metphi': 30374, 'mll': 30374}]),\n",
       " ('4_22600', [343175, {'lep1_iso': 30035, 'metphi': 30375, 'mll': 30375}]),\n",
       " ('4_22700', [343192, {'lep1_iso': 30036, 'metphi': 30376, 'mll': 30376}]),\n",
       " ('4_22800', [343209, {'lep1_iso': 30037, 'metphi': 30377, 'mll': 30377}]),\n",
       " ('4_22900', [343226, {'lep1_iso': 30038, 'metphi': 30378, 'mll': 30378}]),\n",
       " ('4_23000', [343243, {'lep1_iso': 30039, 'metphi': 30379, 'mll': 30379}]),\n",
       " ('4_23100', [343260, {'lep1_iso': 30040, 'metphi': 30380, 'mll': 30380}]),\n",
       " ('4_23200', [343277, {'lep1_iso': 30041, 'metphi': 30381, 'mll': 30381}]),\n",
       " ('4_23300', [343294, {'lep1_iso': 30042, 'metphi': 30382, 'mll': 30382}]),\n",
       " ('4_23400', [343311, {'lep1_iso': 30043, 'metphi': 30383, 'mll': 30383}]),\n",
       " ('4_23500', [343328, {'lep1_iso': 30044, 'metphi': 30384, 'mll': 30384}]),\n",
       " ('4_23600', [343345, {'lep1_iso': 30045, 'metphi': 30385, 'mll': 30385}]),\n",
       " ('4_23700', [343362, {'lep1_iso': 30046, 'metphi': 30386, 'mll': 30386}]),\n",
       " ('4_23800', [343379, {'lep1_iso': 30047, 'metphi': 30387, 'mll': 30387}]),\n",
       " ('4_23900', [343396, {'lep1_iso': 30048, 'metphi': 30388, 'mll': 30388}]),\n",
       " ('4_24000', [343413, {'lep1_iso': 30049, 'metphi': 30389, 'mll': 30389}]),\n",
       " ('4_24100', [343430, {'lep1_iso': 30050, 'metphi': 30390, 'mll': 30390}]),\n",
       " ('4_24200', [343447, {'lep1_iso': 30051, 'metphi': 30391, 'mll': 30391}]),\n",
       " ('4_24300', [343464, {'lep1_iso': 30052, 'metphi': 30392, 'mll': 30392}]),\n",
       " ('4_24400', [343481, {'lep1_iso': 30053, 'metphi': 30393, 'mll': 30393}]),\n",
       " ('4_24500', [343498, {'lep1_iso': 30054, 'metphi': 30394, 'mll': 30394}]),\n",
       " ('4_24600', [343515, {'lep1_iso': 30055, 'metphi': 30395, 'mll': 30395}]),\n",
       " ('4_24700', [343532, {'lep1_iso': 30056, 'metphi': 30396, 'mll': 30396}]),\n",
       " ('4_24800', [343549, {'lep1_iso': 30057, 'metphi': 30397, 'mll': 30397}]),\n",
       " ('4_24900', [343566, {'lep1_iso': 30058, 'metphi': 30398, 'mll': 30398}]),\n",
       " ('4_25000', [343583, {'lep1_iso': 30059, 'metphi': 30399, 'mll': 30399}]),\n",
       " ('4_25100', [343600, {'lep1_iso': 30060, 'metphi': 30400, 'mll': 30400}]),\n",
       " ('4_25200', [343617, {'lep1_iso': 30061, 'metphi': 30401, 'mll': 30401}]),\n",
       " ('4_25300', [343634, {'lep1_iso': 30062, 'metphi': 30402, 'mll': 30402}]),\n",
       " ('4_25400', [343651, {'lep1_iso': 30063, 'metphi': 30403, 'mll': 30403}]),\n",
       " ('4_25500', [343668, {'lep1_iso': 30064, 'metphi': 30404, 'mll': 30404}]),\n",
       " ('4_25600', [343685, {'lep1_iso': 30065, 'metphi': 30405, 'mll': 30405}]),\n",
       " ('4_25700', [343702, {'lep1_iso': 30066, 'metphi': 30406, 'mll': 30406}]),\n",
       " ('4_25800', [343719, {'lep1_iso': 30067, 'metphi': 30407, 'mll': 30407}]),\n",
       " ('4_25900', [343736, {'lep1_iso': 30068, 'metphi': 30408, 'mll': 30408}]),\n",
       " ('4_26000', [343753, {'lep1_iso': 30069, 'metphi': 30409, 'mll': 30409}]),\n",
       " ('4_26100', [343770, {'lep1_iso': 30070, 'metphi': 30410, 'mll': 30410}]),\n",
       " ('4_26200', [343787, {'lep1_iso': 30071, 'metphi': 30411, 'mll': 30411}]),\n",
       " ('4_26300', [343804, {'lep1_iso': 30072, 'metphi': 30412, 'mll': 30412}]),\n",
       " ('4_26400', [343821, {'lep1_iso': 30073, 'metphi': 30413, 'mll': 30413}]),\n",
       " ('4_26500', [343838, {'lep1_iso': 30074, 'metphi': 30414, 'mll': 30414}]),\n",
       " ('4_26600', [343855, {'lep1_iso': 30075, 'metphi': 30415, 'mll': 30415}]),\n",
       " ('4_26700', [343872, {'lep1_iso': 30076, 'metphi': 30416, 'mll': 30416}]),\n",
       " ('4_26800', [343889, {'lep1_iso': 30077, 'metphi': 30417, 'mll': 30417}]),\n",
       " ('4_26900', [343906, {'lep1_iso': 30078, 'metphi': 30418, 'mll': 30418}]),\n",
       " ('4_27000', [343923, {'lep1_iso': 30079, 'metphi': 30419, 'mll': 30419}]),\n",
       " ('4_27100', [343940, {'lep1_iso': 30080, 'metphi': 30420, 'mll': 30420}]),\n",
       " ('4_27200', [343957, {'lep1_iso': 30081, 'metphi': 30421, 'mll': 30421}]),\n",
       " ('4_27300', [343974, {'lep1_iso': 30082, 'metphi': 30422, 'mll': 30422}]),\n",
       " ('4_27400', [343991, {'lep1_iso': 30083, 'metphi': 30423, 'mll': 30423}]),\n",
       " ('4_27500', [344008, {'lep1_iso': 30084, 'metphi': 30424, 'mll': 30424}]),\n",
       " ('4_27600', [344025, {'lep1_iso': 30085, 'metphi': 30425, 'mll': 30425}]),\n",
       " ('4_27700', [344042, {'lep1_iso': 30086, 'metphi': 30426, 'mll': 30426}]),\n",
       " ('4_27800', [344059, {'lep1_iso': 30087, 'metphi': 30427, 'mll': 30427}]),\n",
       " ('4_27900', [344076, {'lep1_iso': 30088, 'metphi': 30428, 'mll': 30428}]),\n",
       " ('4_28000', [344093, {'lep1_iso': 30089, 'metphi': 30429, 'mll': 30429}]),\n",
       " ('4_28100', [344110, {'lep1_iso': 30090, 'metphi': 30430, 'mll': 30430}]),\n",
       " ('4_28200', [344127, {'lep1_iso': 30091, 'metphi': 30431, 'mll': 30431}]),\n",
       " ('4_28300', [344144, {'lep1_iso': 30092, 'metphi': 30432, 'mll': 30432}]),\n",
       " ('4_28400', [344161, {'lep1_iso': 30093, 'metphi': 30433, 'mll': 30433}]),\n",
       " ('4_28500', [344178, {'lep1_iso': 30094, 'metphi': 30434, 'mll': 30434}]),\n",
       " ('4_28600', [344195, {'lep1_iso': 30095, 'metphi': 30435, 'mll': 30435}]),\n",
       " ('4_28700', [344212, {'lep1_iso': 30096, 'metphi': 30436, 'mll': 30436}]),\n",
       " ('4_28800', [344229, {'lep1_iso': 30097, 'metphi': 30437, 'mll': 30437}]),\n",
       " ('4_28900', [344246, {'lep1_iso': 30098, 'metphi': 30438, 'mll': 30438}]),\n",
       " ('4_29000', [344263, {'lep1_iso': 30099, 'metphi': 30439, 'mll': 30439}]),\n",
       " ('4_29100', [344280, {'lep1_iso': 30100, 'metphi': 30440, 'mll': 30440}]),\n",
       " ('4_29200', [344297, {'lep1_iso': 30101, 'metphi': 30441, 'mll': 30441}]),\n",
       " ('4_29300', [344314, {'lep1_iso': 30102, 'metphi': 30442, 'mll': 30442}]),\n",
       " ('4_29400', [344331, {'lep1_iso': 30103, 'metphi': 30443, 'mll': 30443}]),\n",
       " ('4_29500', [344348, {'lep1_iso': 30104, 'metphi': 30444, 'mll': 30444}]),\n",
       " ('4_29600', [344365, {'lep1_iso': 30105, 'metphi': 30445, 'mll': 30445}]),\n",
       " ('4_29700', [344382, {'lep1_iso': 30106, 'metphi': 30446, 'mll': 30446}]),\n",
       " ('4_29800', [344399, {'lep1_iso': 30107, 'metphi': 30447, 'mll': 30447}]),\n",
       " ('4_29900', [344416, {'lep1_iso': 30108, 'metphi': 30448, 'mll': 30448}]),\n",
       " ('4_30000', [344433, {'lep1_iso': 30109, 'metphi': 30449, 'mll': 30449}]),\n",
       " ('4_30100', [344450, {'lep1_iso': 30110, 'metphi': 30450, 'mll': 30450}]),\n",
       " ('4_30200', [344467, {'lep1_iso': 30111, 'metphi': 30451, 'mll': 30451}]),\n",
       " ('4_30300', [344484, {'lep1_iso': 30112, 'metphi': 30452, 'mll': 30452}]),\n",
       " ('4_30400', [344501, {'lep1_iso': 30113, 'metphi': 30453, 'mll': 30453}]),\n",
       " ('4_30500', [344518, {'lep1_iso': 30114, 'metphi': 30454, 'mll': 30454}]),\n",
       " ('4_30600', [344535, {'lep1_iso': 30115, 'metphi': 30455, 'mll': 30455}]),\n",
       " ('4_30700', [344552, {'lep1_iso': 30116, 'metphi': 30456, 'mll': 30456}]),\n",
       " ('4_30800', [344569, {'lep1_iso': 30117, 'metphi': 30457, 'mll': 30457}]),\n",
       " ('4_30900', [344586, {'lep1_iso': 30118, 'metphi': 30458, 'mll': 30458}]),\n",
       " ('4_31000', [344603, {'lep1_iso': 30119, 'metphi': 30459, 'mll': 30459}]),\n",
       " ('4_31100', [344620, {'lep1_iso': 30120, 'metphi': 30460, 'mll': 30460}]),\n",
       " ('4_31200', [344637, {'lep1_iso': 30121, 'metphi': 30461, 'mll': 30461}]),\n",
       " ('4_31300', [344654, {'lep1_iso': 30122, 'metphi': 30462, 'mll': 30462}]),\n",
       " ('4_31400', [344671, {'lep1_iso': 30123, 'metphi': 30463, 'mll': 30463}]),\n",
       " ('4_31500', [344688, {'lep1_iso': 30124, 'metphi': 30464, 'mll': 30464}]),\n",
       " ('4_31600', [344705, {'lep1_iso': 30125, 'metphi': 30465, 'mll': 30465}]),\n",
       " ('4_31700', [344722, {'lep1_iso': 30126, 'metphi': 30466, 'mll': 30466}]),\n",
       " ('4_31800', [344739, {'lep1_iso': 30127, 'metphi': 30467, 'mll': 30467}]),\n",
       " ('4_31900', [344756, {'lep1_iso': 30128, 'metphi': 30468, 'mll': 30468}]),\n",
       " ('4_32000', [344773, {'lep1_iso': 30129, 'metphi': 30469, 'mll': 30469}]),\n",
       " ('4_32100', [344790, {'lep1_iso': 30130, 'metphi': 30470, 'mll': 30470}]),\n",
       " ('4_32200', [344807, {'lep1_iso': 30131, 'metphi': 30471, 'mll': 30471}]),\n",
       " ('4_32300', [344824, {'lep1_iso': 30132, 'metphi': 30472, 'mll': 30472}]),\n",
       " ('4_32400', [344841, {'lep1_iso': 30133, 'metphi': 30473, 'mll': 30473}]),\n",
       " ('4_32500', [344858, {'lep1_iso': 30134, 'metphi': 30474, 'mll': 30474}]),\n",
       " ('4_32600', [344875, {'lep1_iso': 30135, 'metphi': 30475, 'mll': 30475}]),\n",
       " ('4_32700', [344892, {'lep1_iso': 30136, 'metphi': 30476, 'mll': 30476}]),\n",
       " ('4_32800', [344909, {'lep1_iso': 30137, 'metphi': 30477, 'mll': 30477}]),\n",
       " ('4_32900', [344926, {'lep1_iso': 30138, 'metphi': 30478, 'mll': 30478}]),\n",
       " ('4_33000', [344943, {'lep1_iso': 30139, 'metphi': 30479, 'mll': 30479}]),\n",
       " ('4_33100', [344960, {'lep1_iso': 30140, 'metphi': 30480, 'mll': 30480}]),\n",
       " ('4_33200', [344977, {'lep1_iso': 30141, 'metphi': 30481, 'mll': 30481}]),\n",
       " ('4_33300', [344994, {'lep1_iso': 30142, 'metphi': 30482, 'mll': 30482}]),\n",
       " ('4_33400', [345011, {'lep1_iso': 30143, 'metphi': 30483, 'mll': 30483}]),\n",
       " ('4_33500', [345028, {'lep1_iso': 30144, 'metphi': 30484, 'mll': 30484}]),\n",
       " ('4_33600', [345045, {'lep1_iso': 30145, 'metphi': 30485, 'mll': 30485}]),\n",
       " ('4_33700', [345062, {'lep1_iso': 30146, 'metphi': 30486, 'mll': 30486}]),\n",
       " ('4_33800', [345079, {'lep1_iso': 30147, 'metphi': 30487, 'mll': 30487}]),\n",
       " ('4_33900', [345096, {'lep1_iso': 30148, 'metphi': 30488, 'mll': 30488}]),\n",
       " ('4_34000', [345113, {'lep1_iso': 30149, 'metphi': 30489, 'mll': 30489}]),\n",
       " ('4_34100', [345130, {'lep1_iso': 30150, 'metphi': 30490, 'mll': 30490}]),\n",
       " ('4_34200', [345147, {'lep1_iso': 30151, 'metphi': 30491, 'mll': 30491}]),\n",
       " ('4_34300', [345164, {'lep1_iso': 30152, 'metphi': 30492, 'mll': 30492}]),\n",
       " ('4_34400', [345181, {'lep1_iso': 30153, 'metphi': 30493, 'mll': 30493}]),\n",
       " ('4_34500', [345198, {'lep1_iso': 30154, 'metphi': 30494, 'mll': 30494}]),\n",
       " ('4_34600', [345215, {'lep1_iso': 30155, 'metphi': 30495, 'mll': 30495}]),\n",
       " ('4_34700', [345232, {'lep1_iso': 30156, 'metphi': 30496, 'mll': 30496}]),\n",
       " ('4_34800', [345249, {'lep1_iso': 30157, 'metphi': 30497, 'mll': 30497}]),\n",
       " ('4_34900', [345266, {'lep1_iso': 30158, 'metphi': 30498, 'mll': 30498}]),\n",
       " ('4_35000', [345283, {'lep1_iso': 30159, 'metphi': 30499, 'mll': 30499}]),\n",
       " ('4_35100', [345300, {'lep1_iso': 30160, 'metphi': 30500, 'mll': 30500}]),\n",
       " ('4_35200', [345317, {'lep1_iso': 30161, 'metphi': 30501, 'mll': 30501}]),\n",
       " ('4_35300', [345334, {'lep1_iso': 30162, 'metphi': 30502, 'mll': 30502}]),\n",
       " ('4_35400', [345351, {'lep1_iso': 30163, 'metphi': 30503, 'mll': 30503}]),\n",
       " ('4_35500', [345368, {'lep1_iso': 30164, 'metphi': 30504, 'mll': 30504}]),\n",
       " ('4_35600', [345385, {'lep1_iso': 30165, 'metphi': 30505, 'mll': 30505}]),\n",
       " ('4_35700', [345402, {'lep1_iso': 30166, 'metphi': 30506, 'mll': 30506}]),\n",
       " ('4_35800', [345419, {'lep1_iso': 30167, 'metphi': 30507, 'mll': 30507}]),\n",
       " ('4_35900', [345436, {'lep1_iso': 30168, 'metphi': 30508, 'mll': 30508}]),\n",
       " ('4_36000', [345453, {'lep1_iso': 30169, 'metphi': 30509, 'mll': 30509}]),\n",
       " ('4_36100', [345470, {'lep1_iso': 30170, 'metphi': 30510, 'mll': 30510}]),\n",
       " ('4_36200', [345487, {'lep1_iso': 30171, 'metphi': 30511, 'mll': 30511}]),\n",
       " ('4_36300', [345504, {'lep1_iso': 30172, 'metphi': 30512, 'mll': 30512}]),\n",
       " ('4_36400', [345521, {'lep1_iso': 30173, 'metphi': 30513, 'mll': 30513}]),\n",
       " ('4_36500', [345538, {'lep1_iso': 30174, 'metphi': 30514, 'mll': 30514}]),\n",
       " ('4_36600', [345555, {'lep1_iso': 30175, 'metphi': 30515, 'mll': 30515}]),\n",
       " ('4_36700', [345572, {'lep1_iso': 30176, 'metphi': 30516, 'mll': 30516}]),\n",
       " ('4_36800', [345589, {'lep1_iso': 30177, 'metphi': 30517, 'mll': 30517}]),\n",
       " ('4_36900', [345606, {'lep1_iso': 30178, 'metphi': 30518, 'mll': 30518}]),\n",
       " ('4_37000', [345623, {'lep1_iso': 30179, 'metphi': 30519, 'mll': 30519}]),\n",
       " ('4_37100', [345640, {'lep1_iso': 30180, 'metphi': 30520, 'mll': 30520}]),\n",
       " ('4_37200', [345657, {'lep1_iso': 30181, 'metphi': 30521, 'mll': 30521}]),\n",
       " ('4_37300', [345674, {'lep1_iso': 30182, 'metphi': 30522, 'mll': 30522}]),\n",
       " ('4_37400', [345691, {'lep1_iso': 30183, 'metphi': 30523, 'mll': 30523}]),\n",
       " ('4_37500', [345708, {'lep1_iso': 30184, 'metphi': 30524, 'mll': 30524}]),\n",
       " ('4_37600', [345725, {'lep1_iso': 30185, 'metphi': 30525, 'mll': 30525}]),\n",
       " ('4_37700', [345742, {'lep1_iso': 30186, 'metphi': 30526, 'mll': 30526}]),\n",
       " ('4_37800', [345759, {'lep1_iso': 30187, 'metphi': 30527, 'mll': 30527}]),\n",
       " ('4_37900', [345776, {'lep1_iso': 30188, 'metphi': 30528, 'mll': 30528}]),\n",
       " ('4_38000', [345793, {'lep1_iso': 30189, 'metphi': 30529, 'mll': 30529}]),\n",
       " ('4_38100', [345810, {'lep1_iso': 30190, 'metphi': 30530, 'mll': 30530}]),\n",
       " ('4_38200', [345827, {'lep1_iso': 30191, 'metphi': 30531, 'mll': 30531}]),\n",
       " ('4_38300', [345844, {'lep1_iso': 30192, 'metphi': 30532, 'mll': 30532}]),\n",
       " ('4_38400', [345861, {'lep1_iso': 30193, 'metphi': 30533, 'mll': 30533}]),\n",
       " ('4_38500', [345878, {'lep1_iso': 30194, 'metphi': 30534, 'mll': 30534}]),\n",
       " ('4_38600', [345895, {'lep1_iso': 30195, 'metphi': 30535, 'mll': 30535}]),\n",
       " ('4_38700', [345912, {'lep1_iso': 30196, 'metphi': 30536, 'mll': 30536}]),\n",
       " ('4_38800', [345929, {'lep1_iso': 30197, 'metphi': 30537, 'mll': 30537}]),\n",
       " ('4_38900', [345946, {'lep1_iso': 30198, 'metphi': 30538, 'mll': 30538}]),\n",
       " ('4_39000', [345963, {'lep1_iso': 30199, 'metphi': 30539, 'mll': 30539}]),\n",
       " ('4_39100', [345980, {'lep1_iso': 30200, 'metphi': 30540, 'mll': 30540}]),\n",
       " ('4_39200', [345997, {'lep1_iso': 30201, 'metphi': 30541, 'mll': 30541}]),\n",
       " ('4_39300', [346014, {'lep1_iso': 30202, 'metphi': 30542, 'mll': 30542}]),\n",
       " ('4_39400', [346031, {'lep1_iso': 30203, 'metphi': 30543, 'mll': 30543}]),\n",
       " ('4_39500', [346048, {'lep1_iso': 30204, 'metphi': 30544, 'mll': 30544}]),\n",
       " ('4_39600', [346065, {'lep1_iso': 30205, 'metphi': 30545, 'mll': 30545}]),\n",
       " ('4_39700', [346082, {'lep1_iso': 30206, 'metphi': 30546, 'mll': 30546}]),\n",
       " ('4_39800', [346099, {'lep1_iso': 30207, 'metphi': 30547, 'mll': 30547}]),\n",
       " ('4_39900', [346116, {'lep1_iso': 30208, 'metphi': 30548, 'mll': 30548}]),\n",
       " ('4_40000', [346133, {'lep1_iso': 30209, 'metphi': 30549, 'mll': 30549}]),\n",
       " ('5_100', [370132, {'lep1_iso': 31626, 'metphi': 31956, 'mll': 31956}]),\n",
       " ('5_8500', [374440, {'lep1_iso': 31710, 'metphi': 32040, 'mll': 32040}]),\n",
       " ('5_16200', [375197, {'lep1_iso': 31787, 'metphi': 32117, 'mll': 32117}]),\n",
       " ('5_13800', [375617, {'lep1_iso': 31763, 'metphi': 32093, 'mll': 32093}]),\n",
       " ('5_19300', [375632, {'lep1_iso': 31818, 'metphi': 32148, 'mll': 32148}]),\n",
       " ('5_17600', [375987, {'lep1_iso': 31801, 'metphi': 32131, 'mll': 32131}]),\n",
       " ('5_16300', [376134, {'lep1_iso': 31788, 'metphi': 32118, 'mll': 32118}]),\n",
       " ('5_19100', [376426, {'lep1_iso': 31816, 'metphi': 32146, 'mll': 32146}]),\n",
       " ('5_15400', [376533, {'lep1_iso': 31779, 'metphi': 32109, 'mll': 32109}]),\n",
       " ('5_11400', [376681, {'lep1_iso': 31739, 'metphi': 32069, 'mll': 32069}]),\n",
       " ('5_14100', [376956, {'lep1_iso': 31766, 'metphi': 32096, 'mll': 32096}]),\n",
       " ('5_17400', [377517, {'lep1_iso': 31799, 'metphi': 32129, 'mll': 32129}]),\n",
       " ('5_12800', [377655, {'lep1_iso': 31753, 'metphi': 32083, 'mll': 32083}]),\n",
       " ('5_17800', [377677, {'lep1_iso': 31803, 'metphi': 32133, 'mll': 32133}]),\n",
       " ('5_11300', [377860, {'lep1_iso': 31738, 'metphi': 32068, 'mll': 32068}]),\n",
       " ('5_16800', [377967, {'lep1_iso': 31793, 'metphi': 32123, 'mll': 32123}]),\n",
       " ('5_16000', [378015, {'lep1_iso': 31785, 'metphi': 32115, 'mll': 32115}]),\n",
       " ('5_8400', [378103, {'lep1_iso': 31709, 'metphi': 32039, 'mll': 32039}]),\n",
       " ('5_18900', [378324, {'lep1_iso': 31814, 'metphi': 32144, 'mll': 32144}]),\n",
       " ('5_9800', [378433, {'lep1_iso': 31723, 'metphi': 32053, 'mll': 32053}]),\n",
       " ('5_9500', [378566, {'lep1_iso': 31720, 'metphi': 32050, 'mll': 32050}]),\n",
       " ('5_18800', [378583, {'lep1_iso': 31813, 'metphi': 32143, 'mll': 32143}]),\n",
       " ('5_17300', [378604, {'lep1_iso': 31798, 'metphi': 32128, 'mll': 32128}]),\n",
       " ('5_18000', [378631, {'lep1_iso': 31805, 'metphi': 32135, 'mll': 32135}]),\n",
       " ('5_7600', [378795, {'lep1_iso': 31701, 'metphi': 32031, 'mll': 32031}]),\n",
       " ('5_10000', [379203, {'lep1_iso': 31725, 'metphi': 32055, 'mll': 32055}]),\n",
       " ('5_15300', [379368, {'lep1_iso': 31778, 'metphi': 32108, 'mll': 32108}]),\n",
       " ('5_11100', [379390, {'lep1_iso': 31736, 'metphi': 32066, 'mll': 32066}]),\n",
       " ('5_17000', [379565, {'lep1_iso': 31795, 'metphi': 32125, 'mll': 32125}]),\n",
       " ('5_13500', [379798, {'lep1_iso': 31760, 'metphi': 32090, 'mll': 32090}]),\n",
       " ('5_16900', [379916, {'lep1_iso': 31794, 'metphi': 32124, 'mll': 32124}]),\n",
       " ('5_19000', [380089, {'lep1_iso': 31815, 'metphi': 32145, 'mll': 32145}]),\n",
       " ('5_15800', [380189, {'lep1_iso': 31783, 'metphi': 32113, 'mll': 32113}]),\n",
       " ('5_19200', [380215, {'lep1_iso': 31817, 'metphi': 32147, 'mll': 32147}]),\n",
       " ('5_15500', [380414, {'lep1_iso': 31780, 'metphi': 32110, 'mll': 32110}]),\n",
       " ('5_16100', [380424, {'lep1_iso': 31786, 'metphi': 32116, 'mll': 32116}]),\n",
       " ('5_13700', [380476, {'lep1_iso': 31762, 'metphi': 32092, 'mll': 32092}]),\n",
       " ('5_6000', [381007, {'lep1_iso': 31685, 'metphi': 32015, 'mll': 32015}]),\n",
       " ('5_16700', [381078, {'lep1_iso': 31792, 'metphi': 32122, 'mll': 32122}]),\n",
       " ('5_3900', [381110, {'lep1_iso': 31664, 'metphi': 31994, 'mll': 31994}]),\n",
       " ('5_11000', [381121, {'lep1_iso': 31735, 'metphi': 32065, 'mll': 32065}]),\n",
       " ('5_12500', [381560, {'lep1_iso': 31750, 'metphi': 32080, 'mll': 32080}]),\n",
       " ('5_18300', [381626, {'lep1_iso': 31808, 'metphi': 32138, 'mll': 32138}]),\n",
       " ('5_18500', [381752, {'lep1_iso': 31810, 'metphi': 32140, 'mll': 32140}]),\n",
       " ('5_11600', [381775, {'lep1_iso': 31741, 'metphi': 32071, 'mll': 32071}]),\n",
       " ('5_17200', [381807, {'lep1_iso': 31797, 'metphi': 32127, 'mll': 32127}]),\n",
       " ('5_8800', [381851, {'lep1_iso': 31713, 'metphi': 32043, 'mll': 32043}]),\n",
       " ('5_7800', [381865, {'lep1_iso': 31703, 'metphi': 32033, 'mll': 32033}]),\n",
       " ('5_9400', [382045, {'lep1_iso': 31719, 'metphi': 32049, 'mll': 32049}]),\n",
       " ('5_17900', [382202, {'lep1_iso': 31804, 'metphi': 32134, 'mll': 32134}]),\n",
       " ('5_16600', [382257, {'lep1_iso': 31791, 'metphi': 32121, 'mll': 32121}]),\n",
       " ('5_13100', [382306, {'lep1_iso': 31756, 'metphi': 32086, 'mll': 32086}]),\n",
       " ('5_11900', [382378, {'lep1_iso': 31744, 'metphi': 32074, 'mll': 32074}]),\n",
       " ('5_18100', [382604, {'lep1_iso': 31806, 'metphi': 32136, 'mll': 32136}]),\n",
       " ('5_14900', [382612, {'lep1_iso': 31774, 'metphi': 32104, 'mll': 32104}]),\n",
       " ('5_14800', [382871, {'lep1_iso': 31773, 'metphi': 32103, 'mll': 32103}]),\n",
       " ('5_9300', [383224, {'lep1_iso': 31718, 'metphi': 32048, 'mll': 32048}]),\n",
       " ('5_17500', [383330, {'lep1_iso': 31800, 'metphi': 32130, 'mll': 32130}]),\n",
       " ('5_18200', [383725, {'lep1_iso': 31807, 'metphi': 32137, 'mll': 32137}]),\n",
       " ('5_15900', [383886, {'lep1_iso': 31784, 'metphi': 32114, 'mll': 32114}]),\n",
       " ('5_7000', [383937, {'lep1_iso': 31695, 'metphi': 32025, 'mll': 32025}]),\n",
       " ('5_17100', [384182, {'lep1_iso': 31796, 'metphi': 32126, 'mll': 32126}]),\n",
       " ('5_10700', [384320, {'lep1_iso': 31732, 'metphi': 32062, 'mll': 32062}]),\n",
       " ('5_19400', [384389, {'lep1_iso': 31819, 'metphi': 32149, 'mll': 32149}]),\n",
       " ('5_18400', [384695, {'lep1_iso': 31809, 'metphi': 32139, 'mll': 32139}]),\n",
       " ('5_7900', [384734, {'lep1_iso': 31704, 'metphi': 32034, 'mll': 32034}]),\n",
       " ('5_13200', [384931, {'lep1_iso': 31757, 'metphi': 32087, 'mll': 32087}]),\n",
       " ('5_14300', [384994, {'lep1_iso': 31768, 'metphi': 32098, 'mll': 32098}]),\n",
       " ('5_12300', [385022, {'lep1_iso': 31748, 'metphi': 32078, 'mll': 32078}]),\n",
       " ('5_13900', [385110, {'lep1_iso': 31764, 'metphi': 32094, 'mll': 32094}]),\n",
       " ('5_5300', [385120, {'lep1_iso': 31678, 'metphi': 32008, 'mll': 32008}]),\n",
       " ('5_5500', [385154, {'lep1_iso': 31680, 'metphi': 32010, 'mll': 32010}]),\n",
       " ('5_4100', [385284, {'lep1_iso': 31666, 'metphi': 31996, 'mll': 31996}]),\n",
       " ('5_3600', [385383, {'lep1_iso': 31661, 'metphi': 31991, 'mll': 31991}]),\n",
       " ('5_3800', [385509, {'lep1_iso': 31663, 'metphi': 31993, 'mll': 31993}]),\n",
       " ('5_12600', [385717, {'lep1_iso': 31751, 'metphi': 32081, 'mll': 32081}]),\n",
       " ('5_13000', [385877, {'lep1_iso': 31755, 'metphi': 32085, 'mll': 32085}]),\n",
       " ('5_14700', [385982, {'lep1_iso': 31772, 'metphi': 32102, 'mll': 32102}]),\n",
       " ('5_12200', [386569, {'lep1_iso': 31747, 'metphi': 32077, 'mll': 32077}]),\n",
       " ('5_13600', [386715, {'lep1_iso': 31761, 'metphi': 32091, 'mll': 32091}]),\n",
       " ('5_16400', [387007, {'lep1_iso': 31789, 'metphi': 32119, 'mll': 32119}]),\n",
       " ('5_15600', [387147, {'lep1_iso': 31781, 'metphi': 32111, 'mll': 32111}]),\n",
       " ('5_8200', [387545, {'lep1_iso': 31707, 'metphi': 32037, 'mll': 32037}]),\n",
       " ('5_7100', [387726, {'lep1_iso': 31696, 'metphi': 32026, 'mll': 32026}]),\n",
       " ('5_18600', [387841, {'lep1_iso': 31811, 'metphi': 32141, 'mll': 32141}]),\n",
       " ('5_9000', [387865, {'lep1_iso': 31715, 'metphi': 32045, 'mll': 32045}]),\n",
       " ('5_6600', [388285, {'lep1_iso': 31691, 'metphi': 32021, 'mll': 32021}]),\n",
       " ('5_14500', [388432, {'lep1_iso': 31770, 'metphi': 32100, 'mll': 32100}]),\n",
       " ('5_9600', [388717, {'lep1_iso': 31721, 'metphi': 32051, 'mll': 32051}]),\n",
       " ('5_18700', [388870, {'lep1_iso': 31812, 'metphi': 32142, 'mll': 32142}]),\n",
       " ('5_6300', [388998, {'lep1_iso': 31688, 'metphi': 32018, 'mll': 32018}]),\n",
       " ('5_10900', [389016, {'lep1_iso': 31734, 'metphi': 32064, 'mll': 32064}]),\n",
       " ('5_7500', [389174, {'lep1_iso': 31700, 'metphi': 32030, 'mll': 32030}]),\n",
       " ('5_6500', [389372, {'lep1_iso': 31690, 'metphi': 32020, 'mll': 32020}]),\n",
       " ('5_11200', [389435, {'lep1_iso': 31737, 'metphi': 32067, 'mll': 32067}]),\n",
       " ('5_5400', [389461, {'lep1_iso': 31679, 'metphi': 32009, 'mll': 32009}]),\n",
       " ('5_4900', [389468, {'lep1_iso': 31674, 'metphi': 32004, 'mll': 32004}]),\n",
       " ('5_14200', [389853, {'lep1_iso': 31767, 'metphi': 32097, 'mll': 32097}]),\n",
       " ('5_16500', [389968, {'lep1_iso': 31790, 'metphi': 32120, 'mll': 32120}]),\n",
       " ('5_14000', [390003, {'lep1_iso': 31765, 'metphi': 32095, 'mll': 32095}]),\n",
       " ('5_9900', [390042, {'lep1_iso': 31724, 'metphi': 32054, 'mll': 32054}]),\n",
       " ('5_8100', [390104, {'lep1_iso': 31706, 'metphi': 32036, 'mll': 32036}]),\n",
       " ('5_2700', [390198, {'lep1_iso': 31652, 'metphi': 31982, 'mll': 31982}]),\n",
       " ('5_7300', [390428, {'lep1_iso': 31698, 'metphi': 32028, 'mll': 32028}]),\n",
       " ('5_12700', [390518, {'lep1_iso': 31752, 'metphi': 32082, 'mll': 32082}]),\n",
       " ('5_15700', [390660, {'lep1_iso': 31782, 'metphi': 32112, 'mll': 32112}]),\n",
       " ('5_11700', [390716, {'lep1_iso': 31742, 'metphi': 32072, 'mll': 32072}]),\n",
       " ('5_10300', [390754, {'lep1_iso': 31728, 'metphi': 32058, 'mll': 32058}]),\n",
       " ('5_15100', [390834, {'lep1_iso': 31776, 'metphi': 32106, 'mll': 32106}]),\n",
       " ('5_6900', [390912, {'lep1_iso': 31694, 'metphi': 32024, 'mll': 32024}]),\n",
       " ('5_13300', [391080, {'lep1_iso': 31758, 'metphi': 32088, 'mll': 32088}]),\n",
       " ('5_7400', [391089, {'lep1_iso': 31699, 'metphi': 32029, 'mll': 32029}]),\n",
       " ('5_11500', [391142, {'lep1_iso': 31740, 'metphi': 32070, 'mll': 32070}]),\n",
       " ('5_4000', [391247, {'lep1_iso': 31665, 'metphi': 31995, 'mll': 31995}]),\n",
       " ('5_2800', [391503, {'lep1_iso': 31653, 'metphi': 31983, 'mll': 31983}]),\n",
       " ('5_7700', [391784, {'lep1_iso': 31702, 'metphi': 32032, 'mll': 32032}]),\n",
       " ('5_3100', [391922, {'lep1_iso': 31656, 'metphi': 31986, 'mll': 31986}]),\n",
       " ('5_11800', [391961, {'lep1_iso': 31743, 'metphi': 32073, 'mll': 32073}]),\n",
       " ('5_15000', [392013, {'lep1_iso': 31775, 'metphi': 32105, 'mll': 32105}]),\n",
       " ('5_4300', [392034, {'lep1_iso': 31668, 'metphi': 31998, 'mll': 31998}]),\n",
       " ('5_4400', [392327, {'lep1_iso': 31669, 'metphi': 31999, 'mll': 31999}]),\n",
       " ('5_3200', [392767, {'lep1_iso': 31657, 'metphi': 31987, 'mll': 31987}]),\n",
       " ('5_10800', [392771, {'lep1_iso': 31733, 'metphi': 32063, 'mll': 32063}]),\n",
       " ('5_12000', [392791, {'lep1_iso': 31745, 'metphi': 32075, 'mll': 32075}]),\n",
       " ('5_10500', [393088, {'lep1_iso': 31730, 'metphi': 32060, 'mll': 32060}]),\n",
       " ('5_8000', [393123, {'lep1_iso': 31705, 'metphi': 32035, 'mll': 32035}]),\n",
       " ('5_8700', [393242, {'lep1_iso': 31712, 'metphi': 32042, 'mll': 32042}]),\n",
       " ('5_1900', [393282, {'lep1_iso': 31644, 'metphi': 31974, 'mll': 31974}]),\n",
       " ('5_4700', [393298, {'lep1_iso': 31672, 'metphi': 32002, 'mll': 32002}]),\n",
       " ('5_9100', [393310, {'lep1_iso': 31716, 'metphi': 32046, 'mll': 32046}]),\n",
       " ('5_5800', [393393, {'lep1_iso': 31683, 'metphi': 32013, 'mll': 32013}]),\n",
       " ('5_9200', [393419, {'lep1_iso': 31717, 'metphi': 32047, 'mll': 32047}]),\n",
       " ('5_17700', [393484, {'lep1_iso': 31802, 'metphi': 32132, 'mll': 32132}]),\n",
       " ('5_4200', [393581, {'lep1_iso': 31667, 'metphi': 31997, 'mll': 31997}]),\n",
       " ('5_12400', [393687, {'lep1_iso': 31749, 'metphi': 32079, 'mll': 32079}]),\n",
       " ('5_8300', [394002, {'lep1_iso': 31708, 'metphi': 32038, 'mll': 32038}]),\n",
       " ('5_12900', [394140, {'lep1_iso': 31754, 'metphi': 32084, 'mll': 32084}]),\n",
       " ('5_3500', [394290, {'lep1_iso': 31660, 'metphi': 31990, 'mll': 31990}]),\n",
       " ('5_10200', [394325, {'lep1_iso': 31727, 'metphi': 32057, 'mll': 32057}]),\n",
       " ('5_3400', [394825, {'lep1_iso': 31659, 'metphi': 31989, 'mll': 31989}]),\n",
       " ('5_6100', [394916, {'lep1_iso': 31686, 'metphi': 32016, 'mll': 32016}]),\n",
       " ('5_3700', [395428, {'lep1_iso': 31662, 'metphi': 31992, 'mll': 31992}]),\n",
       " ('5_8900', [395484, {'lep1_iso': 31714, 'metphi': 32044, 'mll': 32044}]),\n",
       " ('5_5000', [395557, {'lep1_iso': 31675, 'metphi': 32005, 'mll': 32005}]),\n",
       " ('5_12100', [395568, {'lep1_iso': 31746, 'metphi': 32076, 'mll': 32076}]),\n",
       " ('5_4600', [395765, {'lep1_iso': 31671, 'metphi': 32001, 'mll': 32001}]),\n",
       " ('5_15200', [396095, {'lep1_iso': 31777, 'metphi': 32107, 'mll': 32107}]),\n",
       " ('5_14600', [396177, {'lep1_iso': 31771, 'metphi': 32101, 'mll': 32101}]),\n",
       " ('5_3300', [396464, {'lep1_iso': 31658, 'metphi': 31988, 'mll': 31988}]),\n",
       " ('5_10400', [396475, {'lep1_iso': 31729, 'metphi': 32059, 'mll': 32059}]),\n",
       " ('5_14400', [396603, {'lep1_iso': 31769, 'metphi': 32099, 'mll': 32099}]),\n",
       " ('5_5600', [396763, {'lep1_iso': 31681, 'metphi': 32011, 'mll': 32011}]),\n",
       " ('5_9700', [397368, {'lep1_iso': 31722, 'metphi': 32052, 'mll': 32052}]),\n",
       " ('5_4500', [397404, {'lep1_iso': 31670, 'metphi': 32000, 'mll': 32000}]),\n",
       " ('5_2100', [397484, {'lep1_iso': 31646, 'metphi': 31976, 'mll': 31976}]),\n",
       " ('5_8600', [397549, {'lep1_iso': 31711, 'metphi': 32041, 'mll': 32041}]),\n",
       " ('5_2900', [397684, {'lep1_iso': 31654, 'metphi': 31984, 'mll': 31984}]),\n",
       " ('5_6700', [397808, {'lep1_iso': 31692, 'metphi': 32022, 'mll': 32022}]),\n",
       " ('5_4800', [397823, {'lep1_iso': 31673, 'metphi': 32003, 'mll': 32003}]),\n",
       " ('5_13400', [397997, {'lep1_iso': 31759, 'metphi': 32089, 'mll': 32089}]),\n",
       " ('5_1700', [398768, {'lep1_iso': 31642, 'metphi': 31972, 'mll': 31972}]),\n",
       " ('5_5700', [398804, {'lep1_iso': 31682, 'metphi': 32012, 'mll': 32012}]),\n",
       " ('5_5200', [398811, {'lep1_iso': 31677, 'metphi': 32007, 'mll': 32007}]),\n",
       " ('5_2200', [398945, {'lep1_iso': 31647, 'metphi': 31977, 'mll': 31977}]),\n",
       " ('5_5100', [398978, {'lep1_iso': 31676, 'metphi': 32006, 'mll': 32006}]),\n",
       " ('5_6800', [399083, {'lep1_iso': 31693, 'metphi': 32023, 'mll': 32023}]),\n",
       " ('5_10100', [399092, {'lep1_iso': 31726, 'metphi': 32056, 'mll': 32056}]),\n",
       " ('5_3000', [399633, {'lep1_iso': 31655, 'metphi': 31985, 'mll': 31985}]),\n",
       " ('5_2000', [399831, {'lep1_iso': 31645, 'metphi': 31975, 'mll': 31975}]),\n",
       " ('5_2600', [399841, {'lep1_iso': 31651, 'metphi': 31981, 'mll': 31981}]),\n",
       " ('5_7200', [399979, {'lep1_iso': 31697, 'metphi': 32027, 'mll': 32027}]),\n",
       " ('5_6200', [399993, {'lep1_iso': 31687, 'metphi': 32017, 'mll': 32017}]),\n",
       " ('5_10600', [400097, {'lep1_iso': 31731, 'metphi': 32061, 'mll': 32061}]),\n",
       " ('5_2500', [400468, {'lep1_iso': 31650, 'metphi': 31980, 'mll': 31980}]),\n",
       " ('5_2400', [401371, {'lep1_iso': 31649, 'metphi': 31979, 'mll': 31979}]),\n",
       " ('5_300', [401750, {'lep1_iso': 31628, 'metphi': 31958, 'mll': 31958}]),\n",
       " ('5_6400', [402235, {'lep1_iso': 31689, 'metphi': 32019, 'mll': 32019}]),\n",
       " ('5_600', [402629, {'lep1_iso': 31631, 'metphi': 31961, 'mll': 31961}]),\n",
       " ('5_2300', [402642, {'lep1_iso': 31648, 'metphi': 31978, 'mll': 31978}]),\n",
       " ('5_1600', [402707, {'lep1_iso': 31641, 'metphi': 31971, 'mll': 31971}]),\n",
       " ('5_5900', [402886, {'lep1_iso': 31684, 'metphi': 32014, 'mll': 32014}]),\n",
       " ('5_1200', [404111, {'lep1_iso': 31637, 'metphi': 31967, 'mll': 31967}]),\n",
       " ('5_1300', [404128, {'lep1_iso': 31638, 'metphi': 31968, 'mll': 31968}]),\n",
       " ('5_1100', [404554, {'lep1_iso': 31636, 'metphi': 31966, 'mll': 31966}]),\n",
       " ('5_900', [426192, {'lep1_iso': 31634, 'metphi': 31964, 'mll': 31964}]),\n",
       " ('5_1400', [426465, {'lep1_iso': 31639, 'metphi': 31969, 'mll': 31969}]),\n",
       " ('5_500', [426946, {'lep1_iso': 31630, 'metphi': 31960, 'mll': 31960}]),\n",
       " ('5_700', [427166, {'lep1_iso': 31632, 'metphi': 31962, 'mll': 31962}]),\n",
       " ('5_1000', [427315, {'lep1_iso': 31635, 'metphi': 31965, 'mll': 31965}]),\n",
       " ('5_1500', [427496, {'lep1_iso': 31640, 'metphi': 31970, 'mll': 31970}]),\n",
       " ('5_800', [427645, {'lep1_iso': 31633, 'metphi': 31963, 'mll': 31963}]),\n",
       " ('5_200', [427811, {'lep1_iso': 31627, 'metphi': 31957, 'mll': 31957}]),\n",
       " ('5_1800', [427917, {'lep1_iso': 31643, 'metphi': 31973, 'mll': 31973}]),\n",
       " ('5_400', [427939, {'lep1_iso': 31629, 'metphi': 31959, 'mll': 31959}]),\n",
       " ('5_19500', [454750, {'lep1_iso': 31820, 'metphi': 32150, 'mll': 32150}]),\n",
       " ('5_19600', [454771, {'lep1_iso': 31821, 'metphi': 32151, 'mll': 32151}]),\n",
       " ('5_19700', [454792, {'lep1_iso': 31822, 'metphi': 32152, 'mll': 32152}]),\n",
       " ('5_19800', [454813, {'lep1_iso': 31823, 'metphi': 32153, 'mll': 32153}]),\n",
       " ('5_19900', [454834, {'lep1_iso': 31824, 'metphi': 32154, 'mll': 32154}]),\n",
       " ('5_20000', [454855, {'lep1_iso': 31825, 'metphi': 32155, 'mll': 32155}]),\n",
       " ('5_20100', [454876, {'lep1_iso': 31826, 'metphi': 32156, 'mll': 32156}]),\n",
       " ...]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot metrics vs epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150000/150000 [==============================] - 10s 68us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in sqrt\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:74: RuntimeWarning: invalid value encountered in sqrt\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x936 with 25 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "target_trial = 1\n",
    "target_epoch = 13800\n",
    "N=150000\n",
    "if target_trial and target_epoch:\n",
    "    gan.load_checkpoint(target_epoch, target_trial)\n",
    "#gan.load_checkpoint(target_epoch)\n",
    "preds = gan.predict(N,1.0)\n",
    "make_plots(preds,gan.data[:N])\n",
    "#gan.tag = \"jetisoscaleRobust_mllwidth_flatNegNoise_%d\" % target_trial "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gan.tag = \"v5_default_scan_0\"\n",
    "gan.load_last_checkpoint()\n",
    "print \"Loaded\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def smooth(x,window=31,npoly=2):\n",
    "    from scipy.signal import savgol_filter\n",
    "    return savgol_filter(x,window,npoly)\n",
    "\n",
    "fig, (ax1, ax2, ax3,ax4) = plt.subplots(1,4,figsize=(15,3))\n",
    "# plot losses\n",
    "ax1.plot(gan.d_epochinfo[\"epoch\"],gan.d_epochinfo[\"d_loss\"],color=\"C0\",lw=0.5)\n",
    "ax1.plot(gan.d_epochinfo[\"epoch\"],gan.d_epochinfo[\"g_loss\"],color=\"C1\",lw=0.5)\n",
    "ax1.plot(gan.d_epochinfo[\"epoch\"],smooth(gan.d_epochinfo[\"d_loss\"]),label=\"d loss (smoothed)\",color=\"C0\")\n",
    "ax1.plot(gan.d_epochinfo[\"epoch\"],smooth(gan.d_epochinfo[\"g_loss\"]),label=\"g loss (smoothed)\",color=\"C1\")\n",
    "ax1.legend()\n",
    "# plot masses\n",
    "ax2.plot(gan.d_epochinfo[\"epoch\"],gan.d_epochinfo[\"mass_sig\"],color=\"C0\",lw=0.5)\n",
    "ax2.plot(gan.d_epochinfo[\"epoch\"],gan.d_epochinfo[\"mass_mu\"],color=\"C1\",lw=0.5)\n",
    "ax2.plot(gan.d_epochinfo[\"epoch\"],smooth(gan.d_epochinfo[\"mass_sig\"]),label=\"Z width (smoothed)\",color=\"C0\")\n",
    "ax2.plot(gan.d_epochinfo[\"epoch\"],smooth(gan.d_epochinfo[\"mass_mu\"]),label=\"Z mass (smoothed)\",color=\"C1\")\n",
    "ax2.legend()\n",
    "# plot ks metrics\n",
    "ax3.plot(gan.d_epochinfo[\"epoch\"],gan.d_epochinfo[\"ks\"], lw=0.5,color=\"C0\")\n",
    "ax3.plot(gan.d_epochinfo[\"epoch\"],smooth(gan.d_epochinfo[\"ks\"]), label=\"KS metric (smooth)\",color=\"C0\")\n",
    "ax3.legend();\n",
    "# plot times\n",
    "ax4.plot(gan.d_epochinfo[\"epoch\"],1./60*(np.array(gan.d_epochinfo[\"time\"])-gan.d_epochinfo[\"time\"][0]), label=\"times [min]\")\n",
    "ax4.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot predictions and real events\n",
    "Get the noise from the gan object, feed it into the generator, then also take true events (`data`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 15000\n",
    "# frac = 0.0001 # 2 events\n",
    "# frac = 0.00005 # 1 event\n",
    "frac = 1.0\n",
    "_, noise = gan.get_noise(N,max_true_samples_frac=frac)\n",
    "# print preds.shape\n",
    "preds = gan.generator.predict(noise,verbose=1)\n",
    "make_plots(preds,gan.data[:N])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recarray\n",
    "The real data (`gan.data`) is a recarray, so column indexing happens with `gan.data[\"lep1_px\"]`.\n",
    "The predictions from the generator are just normal matrices, so you have to do yucky things like `preds[:,7]`\n",
    "and keep track of what that means. Use a helper function to get a `np.view` into the predictions so that\n",
    "they columns can be accessed in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Z_pt(data,haspy2=False):\n",
    "    pxsum = data[\"lep1_px\"]+data[\"lep2_px\"]\n",
    "    if haspy2: pysum = data[\"lep1_py\"]+data[\"lep2_py\"]\n",
    "    else: pysum = data[\"lep1_py\"]\n",
    "    return np.hypot(pxsum,pysum)\n",
    "\n",
    "def Z_phi(data,haspy2=False):\n",
    "    pxsum = data[\"lep1_px\"]+data[\"lep2_px\"]\n",
    "    if haspy2: pysum = data[\"lep1_py\"]+data[\"lep2_py\"]\n",
    "    else: pysum = data[\"lep1_py\"]\n",
    "    return np.arctan2(pysum,pxsum)\n",
    "\n",
    "def dphiphi(phi1,phi2):\n",
    "    dphi = phi1-phi2\n",
    "    dphi[dphi>np.pi] -= 2*np.pi\n",
    "    dphi[dphi<-np.pi] += 2*np.pi \n",
    "    return np.abs(dphi)\n",
    "\n",
    "\n",
    "def met_phi(data):\n",
    "    if \"met\" in data.dtype.names:\n",
    "        return data[\"metphi\"]\n",
    "    else:\n",
    "        return np.arctan2(data[\"mety\"],data[\"metx\"])\n",
    "    \n",
    "def met_pt(data):\n",
    "    if \"met\" in data.dtype.names:\n",
    "        return data[\"met\"]\n",
    "    else:\n",
    "        return np.hypot(data[\"metx\"],data[\"mety\"])\n",
    "    \n",
    "def ht(data):\n",
    "    return data[\"jet_pt1\"]*(data[\"jet_pt1\"] > 10) + \\\n",
    "           data[\"jet_pt2\"]*(data[\"jet_pt2\"] > 10) + \\\n",
    "           data[\"jet_pt3\"]*(data[\"jet_pt3\"] > 10) + \\\n",
    "           data[\"jet_pt4\"]*(data[\"jet_pt4\"] > 10) + \\\n",
    "           data[\"jet_pt5\"]*(data[\"jet_pt5\"] > 10)\n",
    "\n",
    "def nvtx(data):\n",
    "    if \"mll\" in data.dtype.names:\n",
    "        return data.nvtxs\n",
    "    else:\n",
    "        return np.rint(data[\"nvtxs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot Z_pT for predicted and real events# plot Z \n",
    "predsrec = get_recview(preds)\n",
    "fig,ax = plt.subplots()\n",
    "ax.hist(Z_pt(gan.data[:10000],haspy2=False), label=\"Z $p_{T}$ real\",bins=np.linspace(0,100,50),histtype=\"step\",density=True,lw=2)\n",
    "ax.hist(Z_pt(predsrec,haspy2=False),label=\"Z $p_{T}$ fake\",bins=np.linspace(0,100,50),histtype=\"step\",density=True,lw=2)\n",
    "ax.legend()\n",
    "ax.set_title(\"Z-boson $p_T$ for real and fake events\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "lowpu_real_met  = met_pt(gan.data)[nvtx(gan.data) < 15]\n",
    "highpu_real_met = met_pt(gan.data)[nvtx(gan.data) > 25]\n",
    "lowpu_fake_met  = met_pt(predsrec)[nvtx(predsrec) < 15]\n",
    "highpu_fake_met = met_pt(predsrec)[nvtx(predsrec) > 25]\n",
    "bins = np.linspace(0,100,50)\n",
    "_ = ax.hist(lowpu_real_met,label=\"low PU, real\",bins=bins,histtype=\"step\",density=True,lw=1.5)\n",
    "_ = ax.hist(lowpu_fake_met,label=\"low PU, fake\",bins=bins,histtype=\"step\",density=True,lw=1.5)\n",
    "_ = ax.hist(highpu_real_met,label=\"high PU, real\",bins=bins,histtype=\"step\",density=True,lw=1.5)\n",
    "_ = ax.hist(highpu_fake_met,label=\"high PU, fake\",bins=bins,histtype=\"step\",density=True,lw=1.5)\n",
    "_ = ax.legend()\n",
    "# ax.set_yscale(\"log\",nonposy=\"clip\")\n",
    "_ = ax.set_title(\"MET for low (nvtx<15) and high (nvtx>25) events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import binned_statistic\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1,2,figsize=(9,4), sharex=True,sharey=True)\n",
    "\n",
    "bins = [np.linspace(15,75,15),np.linspace(15,75,15)]\n",
    "\n",
    "ptj1 = gan.data[\"jet_pt1\"]\n",
    "zpt = Z_pt(gan.data)\n",
    "good = (zpt > 15.) & (ptj1 > 15.)\n",
    "_,_,_,im = ax1.hist2d(ptj1[good],zpt[good],label=\"real\",bins=bins,normed=True) #, norm=LogNorm())\n",
    "fig.colorbar(im,ax=ax1, format=\"%.0e\")\n",
    "\n",
    "vals,edges,_ = binned_statistic(ptj1[good],zpt[good],statistic=\"median\",bins=bins[0])\n",
    "xs = 0.5*(edges[1:] + edges[:-1])\n",
    "ax1.plot(xs,vals,\"ro-\",color=(1.0,0.3,0.3),lw=1.,ms=4.5, label=\"median\")\n",
    "ax1.legend()\n",
    "\n",
    "ptj1 = predsrec[\"jet_pt1\"]\n",
    "zpt = Z_pt(predsrec)\n",
    "good = (zpt > 15.) & (ptj1 > 15.)\n",
    "_,_,_,im = ax2.hist2d(ptj1[good],zpt[good],label=\"real\",bins=bins,normed=True)\n",
    "fig.colorbar(im,ax=ax2, format='%.0e')\n",
    "\n",
    "vals,edges,_ = binned_statistic(ptj1[good],zpt[good],statistic=\"median\",bins=bins[0])\n",
    "xs = 0.5*(edges[1:] + edges[:-1])\n",
    "ax2.plot(xs,vals,\"ro-\",color=(1.0,0.3,0.3),lw=1.,ms=4.5, label=\"median\")\n",
    "ax2.legend()\n",
    "\n",
    "ax1.set_title(\"Real\")\n",
    "ax2.set_title(\"Fake\")\n",
    "ax1.set_ylabel(\"Z $p_{T}$\")\n",
    "ax1.set_xlabel(\"jet 1 $p_{T}$\")\n",
    "ax2.set_xlabel(\"jet 1 $p_{T}$\")\n",
    "\n",
    "\n",
    "fig.set_tight_layout(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predmll = Minv(preds)\n",
    "predmll = predmll[np.isfinite(predmll)]\n",
    "bins = np.linspace(60,120,120)\n",
    "fig, (ax1, ax2) = plt.subplots(2,1, sharex=True,gridspec_kw={'height_ratios':[9, 2]})\n",
    "hreal = ax1.hist(gan.data[\"mll\"],bins=bins, label=\"real $m_{ll}$\",histtype=\"step\",density=True,lw=2)\n",
    "hfake = ax1.hist(predmll,bins=bins, label=\"fake $m_{ll}$\",histtype=\"step\",density=True,lw=2)\n",
    "ratio = hfake[0]/hreal[0]\n",
    "ax2.plot(bins[:-1],ratio,marker=\"o\",markersize=3,linewidth=1.5,linestyle=\"\")\n",
    "ax2.set_ylim([0.5,1.5])\n",
    "ax2.set_ylabel(\"fake/real\")\n",
    "_ = ax1.legend()\n",
    "realmll = gan.data[\"mll\"]\n",
    "print \"real:\",realmll.mean(), realmll.std()\n",
    "print \"fake:\",predmll.mean(), predmll.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jetmat = np.c_[predsrec[\"jet_pt1\"],predsrec[\"jet_pt2\"],predsrec[\"jet_pt3\"],predsrec[\"jet_pt4\"],predsrec[\"jet_pt5\"]]\n",
    "# njet counts just any jet with pt>15\n",
    "njets1 = (jetmat > 15.).sum(axis=-1)\n",
    "# njet counts only jets with pt>15 starting from first jet if all previous jets pass threshold\n",
    "# e.g., if jetpt1>15, jetpt2<15, jetpt3>15, then njets==1 in second case, but 2 in first case\n",
    "njets2 = (jetmat > 15.).argmin(axis=-1)\n",
    "# turns out the two are very similar\n",
    "\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1,3,figsize=(12,3), sharey=False)\n",
    "for numer,denom,ax in [\n",
    "    [\"jet_pt2\",\"jet_pt1\",ax1],\n",
    "    [\"jet_pt3\",\"jet_pt2\",ax2],\n",
    "    [\"jet_pt4\",\"jet_pt3\",ax3],\n",
    "]:\n",
    "    jetratio_real = (gan.data[:N][numer]/gan.data[:N][denom])[(gan.data[:N][denom] > 15.) & (gan.data[:N][numer] > 15.)]\n",
    "    jetratio_fake = (predsrec[numer]/predsrec[denom])[(predsrec[denom] > 15.) & (predsrec[numer] > 15.)]\n",
    "    bins = np.linspace(0,4,60)\n",
    "    ax.hist(jetratio_real,bins=bins,histtype=\"step\",density=True,label=\"real\",lw=1.5)\n",
    "    ax.hist(jetratio_fake,bins=bins,histtype=\"step\",density=True,label=\"fake\",lw=1.5)\n",
    "    ax.legend()\n",
    "    ax.set_title(\"{} / {} ratio\".format(numer,denom))\n",
    "fig.set_tight_layout(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import glob\n",
    "\n",
    "picklenames = glob.glob(\"progress/jetisoscaleRobust_mllwidth_flatNegNoise*/*pkl\")\n",
    "def get_epochks(fname):\n",
    "    with open(fname) as fh:\n",
    "        data = pickle.load(fh)\n",
    "        return np.array(data[\"epoch\"]),np.array(data[\"ks\"])\n",
    "    return [],[]\n",
    "#print dirnames\n",
    "print get_epochks(picklenames[0])\n",
    "fig,ax = plt.subplots()\n",
    "for pn in picklenames:\n",
    "    xs, ys = get_epochks(pn)\n",
    "#     good = ys < 0.08\n",
    "#     xs = xs[good]\n",
    "#     ys = ys[good]\n",
    "    print pn, xs[ys.argmin()], ys[ys.argmin()]\n",
    "#     ax.plot(xs,ys, label=pn)\n",
    "# ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

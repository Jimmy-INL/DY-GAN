{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import tensorflow\n",
      "import keras\n",
      "import matplotlib\n",
      "import sklearn\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "\"\"\"This version of the file is just for cooking up new ideas. gan_skip.ipynb is the master file.\"\"\"\n",
    "\n",
    "filename=\"gan_skip_2.ipynb\" #important for bookkeeping since ipython can't use __file__\n",
    "\n",
    "import os\n",
    "# running with non gpu singularity container, so commented out the next line to use CPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(42)\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "import hashlib\n",
    "import difflib\n",
    "print \"import tensorflow\"\n",
    "           \n",
    "import keras.backend.tensorflow_backend as K\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Dense, Dropout, Flatten, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D, LeakyReLU, Lambda\n",
    "from keras.layers import Input, merge, Concatenate, concatenate, Add\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.utils import plot_model\n",
    "from keras.datasets import mnist\n",
    "print \"import keras\"\n",
    "\n",
    "import numpy as np\n",
    "#from tqdm import tqdm\n",
    "import time\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "print \"import matplotlib\"\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from scipy.stats import binned_statistic_2d\n",
    "\n",
    "print \"import sklearn\"\n",
    "\n",
    "np.random.seed(42)\n",
    "cov_hash = None\n",
    "cov_ans = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "def onetime(func):\n",
    "    \"\"\"stores the functions output, returns the output if called again on the same input, else computes new output\"\"\"\n",
    "    def decorated(*args, **kwargs):\n",
    "        global cov_ans\n",
    "        global cov_hash\n",
    "        new_hash=hashlib.md5(str(args)+str(kwargs)).hexdigest() \n",
    "        if new_hash != cov_hash:\n",
    "            #print(\"computing\")\n",
    "            cov_ans = func(*args, **kwargs)\n",
    "        cov_hash=new_hash\n",
    "        return cov_ans\n",
    "    return decorated\n",
    "    \n",
    "\n",
    "@onetime\n",
    "def covariance_metrics(real_data, predictions):\n",
    "    \"\"\"Takes in real_data matrix with real entries as rows and predictions matrix with generated events as rows and returns the covariance matricies for the two as well as the average, maximum, and std. dev of the difference between the entries in the coverance matrix as well as in the average of the variables.\"\"\"\n",
    "    \n",
    "    cov_pred = np.cov(predictions.T)\n",
    "    avg_pred = predictions.mean(axis=0)\n",
    "    cov_real = np.cov(real_data.T)\n",
    "    avg_real = real_data.mean(axis=0)\n",
    "    \n",
    "    #cov_diff = np.abs((cov_pred - cov_real)/np.sqrt(np.abs(np.outer(avg_real, avg_pred))))\n",
    "    cov_diff = np.abs((cov_pred - cov_real)/cov_real)\n",
    "    ar=avg_real\n",
    "    ar[ar == 0] = 1\n",
    "    avg_diff = np.abs((avg_pred - avg_real)/ar)\n",
    "    \n",
    "    return cov_diff, avg_diff\n",
    "\n",
    "\n",
    "def get_score(real_data, predictions, weight_cov = (1/361.), weight_avg = (1/19.)):\n",
    "    cov_diff, avg_diff = covariance_metrics(real_data, predictions)\n",
    "    return weight_cov*np.sum(cov_diff)+weight_avg*np.sum(avg_diff)\n",
    "\n",
    "def getKS(real_data, predictions):\n",
    "    return ks_2samp(Minv(real_data,ptetaphi=False,nopy2=True), Minv(predictions,ptetaphi=False,nopy2=True))\n",
    "#a = np.matrix([[1,2,3],[4,5,6],[7,8,9]])\n",
    "#ap = np.matrix([[2,2,4],[4,5,6],[7,8,9]])\n",
    "#b = np.matrix([[-1,-2,-3],[-4,-5,-6],[-7,-8,-9]])\n",
    "\n",
    "\n",
    "#print(get_score(a,ap))\n",
    "#print(covariance_metrics(a,b))\n",
    "#print(covariance_metrics(a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Minv(cols,ptetaphi=False,nopy2=False):\n",
    "    \"\"\"\n",
    "    Computes M for two objects given the cartesian momentum projections\n",
    "    if `ptetaphi` is True, then assumes the 8 input columns are cylindrical eptetaphi\n",
    "    if `nopy2` is True, input is 7 columns with no py2\n",
    "    \"\"\"\n",
    "    if ptetaphi:\n",
    "        cols = ptetaphi_to_cartesian(cols)\n",
    "    if nopy2:\n",
    "        M2 = (cols[:,0]+cols[:,4])**2\n",
    "        M2 -= (cols[:,1]+cols[:,5])**2\n",
    "        M2 -= (cols[:,2]          )**2\n",
    "        M2 -= (cols[:,3]+cols[:,6])**2\n",
    "    else:\n",
    "        M2 = (cols[:,0]+cols[:,4])**2\n",
    "        M2 -= (cols[:,1]+cols[:,5])**2\n",
    "        M2 -= (cols[:,2]+cols[:,6])**2\n",
    "        M2 -= (cols[:,3]+cols[:,7])**2\n",
    "    return np.sqrt(M2)\n",
    "\n",
    "def cartesian_to_ptetaphi(eight_cartesian_cols):\n",
    "    \"\"\"\n",
    "    Takes 8 columns as cartesian e px py pz e px py pz\n",
    "    and converts to e pt eta phi e pt eta phi\n",
    "    \"\"\"\n",
    "    e1 =  eight_cartesian_cols[:,0]\n",
    "    e2 =  eight_cartesian_cols[:,4]\n",
    "    px1 = eight_cartesian_cols[:,1]\n",
    "    px2 = eight_cartesian_cols[:,5]\n",
    "    py1 = eight_cartesian_cols[:,2]\n",
    "    py2 = eight_cartesian_cols[:,6]\n",
    "    pz1 = eight_cartesian_cols[:,3]\n",
    "    pz2 = eight_cartesian_cols[:,7]\n",
    "    p1 = np.sqrt(px1**2+py1**2+pz1**2)\n",
    "    p2 = np.sqrt(px2**2+py2**2+pz2**2)\n",
    "    pt1 = np.sqrt(px1**2+py1**2)\n",
    "    pt2 = np.sqrt(px2**2+py2**2)\n",
    "    phi1 = np.arctan2(py1,px1)\n",
    "    phi2 = np.arctan2(py2,px2)\n",
    "    eta1 = np.arctanh(pz1/p1)\n",
    "    eta2 = np.arctanh(pz2/p2)\n",
    "    return np.c_[e1,pt1,eta1,phi1,e2,pt2,eta2,phi2]\n",
    "\n",
    "def ptetaphi_to_cartesian(eight_eptetaphi_cols):\n",
    "    \"\"\"\n",
    "    Takes 8 columns as e pt eta phi e pt eta phi\n",
    "    and converts to e px py pz e px py pz\n",
    "    \"\"\"\n",
    "    e1 =  eight_eptetaphi_cols[:,0]\n",
    "    e2 =  eight_eptetaphi_cols[:,4]\n",
    "    pt1 =  eight_eptetaphi_cols[:,1]\n",
    "    pt2 =  eight_eptetaphi_cols[:,5]\n",
    "    eta1 =  eight_eptetaphi_cols[:,2]\n",
    "    eta2 =  eight_eptetaphi_cols[:,6]\n",
    "    phi1 =  eight_eptetaphi_cols[:,3]\n",
    "    phi2 =  eight_eptetaphi_cols[:,7]\n",
    "    px1 = np.abs(pt1)*np.cos(phi1)\n",
    "    px2 = np.abs(pt2)*np.cos(phi2)\n",
    "    py1 = np.abs(pt1)*np.sin(phi1)\n",
    "    py2 = np.abs(pt2)*np.sin(phi2)\n",
    "    pz1 = np.abs(pt1)/np.tan(2.0*np.arctan(np.exp(-1.*eta1)))\n",
    "    pz2 = np.abs(pt2)/np.tan(2.0*np.arctan(np.exp(-1.*eta2)))\n",
    "    return np.c_[e1,px1,py1,pz1,e2,px2,py2,pz2]\n",
    "\n",
    "def get_dphi(px1,py1,px2,py2):\n",
    "    phi1 = np.arctan2(py1,px1)\n",
    "    phi2 = np.arctan2(py2,px2)\n",
    "    dphi = phi1-phi2\n",
    "    dphi[dphi>np.pi] -= 2*np.pi\n",
    "    dphi[dphi<-np.pi] += 2*np.pi \n",
    "    return dphi\n",
    "\n",
    "def get_rotated_pxpy(px1,py1,px2,py2):\n",
    "    \"\"\"\n",
    "    rotates two leptons such that phi2 = 0\n",
    "    \"\"\"\n",
    "    pt1 = np.sqrt(px1**2+py1**2)\n",
    "    pt2 = np.sqrt(px2**2+py2**2)\n",
    "    phi1 = np.arctan2(py1,px1)\n",
    "    phi2 = np.arctan2(py2,px2)\n",
    "    px1p = pt1*np.cos(phi1-phi2)\n",
    "    py1p = pt1*np.sin(phi1-phi2)\n",
    "    px2p = pt2*np.cos(phi2-phi2)\n",
    "    return px1p,py1p,px2p,np.zeros(len(pt2))\n",
    "    \n",
    "def cartesian_zerophi2(coords,ptetaphi=False):\n",
    "    \"\"\"\n",
    "    returns 8-1=7 columns rotating leptons such that phi2 is 0 (and removing it)\n",
    "    if `ptetaphi` is True, then return eptetaphi instead of epxpypz\n",
    "    \"\"\"\n",
    "    lepcoords_cyl = cartesian_to_ptetaphi(coords)\n",
    "    phi1 = lepcoords_cyl[:,3]\n",
    "    phi2 = lepcoords_cyl[:,7]\n",
    "    dphi = phi1-phi2\n",
    "    dphi[dphi>np.pi] -= 2*np.pi\n",
    "    dphi[dphi<-np.pi] += 2*np.pi\n",
    "    lepcoords_cyl[:,3] = dphi\n",
    "    lepcoords_cyl[:,7] = 0.\n",
    "    if ptetaphi:\n",
    "        return np.delete(lepcoords_cyl, [7], axis=1)\n",
    "    else:\n",
    "        return np.delete(ptetaphi_to_cartesian(lepcoords_cyl), [6], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "listener = open(\"listen.txt\", \"w+\")\n",
    "\n",
    "def invmass_from_8cartesian(x):\n",
    "    invmass = K.sqrt(\n",
    "                (x[:,0:1]+x[:,4:5])**2-\n",
    "                (x[:,1:2]+x[:,5:6])**2-\n",
    "                (x[:,2:3]+x[:,6:7])**2-\n",
    "                (x[:,3:4]+x[:,7:8])**2\n",
    "                )\n",
    "    return invmass\n",
    "\n",
    "def invmass_from_8cartesian_nopy2(x):\n",
    "    invmass = K.sqrt(\n",
    "                (x[:,0:1]+x[:,4:5])**2-\n",
    "                (x[:,1:2]+x[:,5:6])**2-\n",
    "                (x[:,2:3]         )**2-\n",
    "                (x[:,3:4]+x[:,6:7])**2\n",
    "                )\n",
    "    return invmass\n",
    "\n",
    "def get_first_N(x,N=19):\n",
    "    return x[:,0:N]\n",
    "\n",
    "def add_invmass_from_8cartesian(x):\n",
    "    return K.concatenate([x,invmass_from_8cartesian(x)])\n",
    "\n",
    "\n",
    "def fix_outputs(x):\n",
    "    \"\"\"\n",
    "    Take nominal delphes format of 19 columns and fix some columns\n",
    "    \"\"\"\n",
    "    return K.concatenate([\n",
    "        # x[:,0:21],\n",
    "        x[:,0:7], # epxpypz for lep1,lep2 -1 for no py2\n",
    "        x[:,7:8], # nvtx\n",
    "        K.sign(x[:,8:10]), # q1 q2\n",
    "        x[:,10:12], # iso1 iso2\n",
    "        x[:,12:14], # met, metphi\n",
    "        x[:,14:19], # jet pts\n",
    "        ])\n",
    "\n",
    "def custom_loss(c, loss_type = \"force_mll\"):\n",
    "    if loss_type == \"disc\":\n",
    "        def loss_func(y_true, y_pred_mll):\n",
    "            y_true = y_true[:,0]\n",
    "            y_pred = y_pred_mll[:,0]\n",
    "\n",
    "            return binary_crossentropy(y_true, y_pred)\n",
    "        return loss_func\n",
    "    elif loss_type == \"force_mll\":\n",
    "        def loss_func(y_true, y_pred_mll):\n",
    "            y_true = y_true[:,0]\n",
    "            y_pred = y_pred_mll[:,0]\n",
    "            mll_pred = y_pred_mll[:,1]\n",
    "\n",
    "            mll_loss = K.mean(K.abs(mll_pred - 91.2))\n",
    "\n",
    "    #         pseudomll = K.random_normal_variable(shape=(1,1), mean=91.2, scale=2)\n",
    "    #         mll_loss = K.mean((mll_pred - pseudomll)**2)\n",
    "\n",
    "            return binary_crossentropy(y_true, y_pred) + c*mll_loss\n",
    "        return loss_func\n",
    "    elif loss_type == \"force_z_width\":\n",
    "        def loss_func(y_true, y_pred_mll):\n",
    "            y_true = y_true[:,0]\n",
    "            y_pred = y_pred_mll[:,0]\n",
    "            mll_pred = y_pred_mll[:,1]\n",
    "            \n",
    "            mll_loss = K.mean(K.abs(mll_pred - 91.2))\n",
    "            mll_sigma_loss = K.abs(K.std(mll_pred) - 7.67)\n",
    "\n",
    "            return binary_crossentropy(y_true, y_pred) + c*mll_loss + c*mll_sigma_loss\n",
    "        return loss_func\n",
    "    \n",
    "    elif loss_type  == \"ks_test\":\n",
    "        data = np.load(\"/home/users/bhashemi/Projects/GIT/DY-GAN/delphes/total_Zmumu_13TeV_PU20_v2.npa\")\n",
    "        data = data[data[\"genmll\"] > 50.]\n",
    "        data = data[np.random.randint(0, data.shape[0], 2000)]\n",
    "        mll_true = data[\"mll\"]\n",
    "        def ks_approx(a,b):\n",
    "            \"\"\"Takes in two lists of numbers, a and b, and returns an approximation to the KS test statistic between them\"\"\"\n",
    "            pdf_scale = 0.01\n",
    "            max_scale = 2\n",
    "\n",
    "            c = K.concatenate([a,b])\n",
    "            #num_a = K.sum(a)/K.mean(a)\n",
    "            #num_b = b.shape[0]          #Fantastic little bug, seems I can't use this for a, and I can't use what I used for a on b...\n",
    "            a_pdf = K.sum(K.map_fn(lambda x: 1/(1+K.exp(pdf_scale*(x-c))), a), axis=0)/512\n",
    "            b_pdf = K.sum(K.map_fn(lambda x: 1/(1+K.exp(pdf_scale*(x-c))), b), axis=0)/2000\n",
    "            pdf_diff = (a_pdf - b_pdf)\n",
    "            pdf_diff = K.sqrt(pdf_diff*pdf_diff)\n",
    "            #return K.logsumexp(max_scale*(pdf_diff))/max_scale\n",
    "            return K.sum(pdf_diff)\n",
    "\n",
    "        def loss_func(y_true, y_pred_mll):\n",
    "            y_true = y_true[:,0]\n",
    "            y_pred = y_pred_mll[:,0]\n",
    "            mll_pred = y_pred_mll[:,0]\n",
    "\n",
    "            mll_loss = ks_approx(mll_pred, mll_true)\n",
    "            #mll_loss = 1\n",
    "\n",
    "            #return binary_crossentropy(y_true, y_pred) + c*mll_loss\n",
    "            return mll_loss\n",
    "        return loss_func\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(\"Can not make loss function of type %s\" % loss_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def make_plots(preds,reals,title=\"\",fname=\"\"):\\n    nrows, ncols = 5,5\\n    fig, axs = plt.subplots(nrows,ncols,figsize=(16,13))\\n    fig.subplots_adjust(wspace=0.1,hspace=0.3)\\n\\n\\n    #print(preds)\\n    info = [\\n        [\"mll\",(60,120,50)],\\n        [\"lep1_e\",(0,250,50)],\\n        [\"lep1_px\",(-100,100,50)],\\n        [\"lep1_py\",(-100,100,50)],\\n        [\"lep1_pz\",(-200,200,50)],\\n        [\"lep2_e\",(0,250,50)],\\n        [\"lep2_px\",(-100,100,50)],\\n        [\"lep2_pz\",(-200,200,50)],\\n        [\"dphi\",(-4,4,50)],\\n        [\"nvtxs\",(0,50,350)],\\n        [\"met\",(0,150,50)],\\n        [\"metphi\",(-6,6,50)],\\n        [\"lep1_charge\",(-7,7,30)],\\n        [\"lep2_charge\",(-7,7,30)],\\n        [\"lep1_iso\",(0,0.2,30)],\\n        [\"lep2_iso\",(0,0.2,30)],\\n        [\"genjet_pt1\",(0,100,50)],\\n        [\"genjet_pt2\",(0,100,50)],\\n        [\"genjet_pt3\",(0,100,50)],\\n        [\"genjet_pt4\",(0,100,50)],\\n        [\"genjet_pt5\",(0,100,50)],\\n\\n    ]\\n    for ic,(cname,crange) in enumerate(info):\\n        if cname == \"mll\":\\n            real = reals[\"mll\"]\\n            pred = Minv(preds,ptetaphi=False,nopy2=True)\\n        elif cname == \"lep1_e\": real, pred = reals[cname], preds[:,0]\\n        elif cname == \"lep1_pz\": real, pred = reals[cname], preds[:,3]\\n        elif cname == \"lep2_e\": real, pred = reals[cname], preds[:,4]\\n        elif cname == \"lep2_pz\": real, pred = reals[cname], preds[:,6]\\n        elif cname == \"lep1_px\": \\n            real = get_rotated_pxpy(reals[\"lep1_px\"], reals[\"lep1_py\"], reals[\"lep2_px\"], reals[\"lep2_py\"])[0]\\n            pred = preds[:,1]\\n        elif cname == \"lep1_py\":\\n            real = get_rotated_pxpy(reals[\"lep1_px\"], reals[\"lep1_py\"], reals[\"lep2_px\"], reals[\"lep2_py\"])[1]\\n            pred = preds[:,2]\\n        elif cname == \"lep2_px\":\\n            real = get_rotated_pxpy(reals[\"lep1_px\"], reals[\"lep1_py\"], reals[\"lep2_px\"], reals[\"lep2_py\"])[2]\\n            pred = preds[:,5]\\n        elif cname == \"dphi\":\\n            real = get_dphi(reals[\"lep1_px\"], reals[\"lep1_py\"], reals[\"lep2_px\"], reals[\"lep2_py\"])\\n            pred = get_dphi(preds[:,1], preds[:,2], preds[:,5], np.zeros(len(preds)))\\n        elif cname == \"nvtxs\": real, pred = reals[cname], np.round(preds[:,7])\\n        elif cname == \"lep1_charge\": real, pred = reals[cname], preds[:,8]\\n        elif cname == \"lep2_charge\": real, pred = reals[cname], preds[:,9]\\n        elif cname == \"lep1_iso\": real, pred = reals[cname], preds[:,10]\\n        elif cname == \"lep2_iso\": real, pred = reals[cname], preds[:,11]\\n        elif cname == \"met\": real, pred = reals[cname], preds[:,12]\\n        elif cname == \"metphi\": real, pred = reals[cname], METPhiMap(preds[:,13])\\n        elif cname == \"genjet_pt1\": real, pred = reals[cname], preds[:,14]\\n        elif cname == \"genjet_pt2\": real, pred = reals[cname], preds[:,15]\\n        elif cname == \"genjet_pt3\": real, pred = reals[cname], preds[:,16]\\n        elif cname == \"genjet_pt4\": real, pred = reals[cname], preds[:,17]\\n        elif cname == \"genjet_pt5\": real, pred = reals[cname], preds[:,18]\\n        idx = ic // ncols, ic % ncols\\n        bins_real = axs[idx].hist(real, range=crange[:2],bins=crange[-1], histtype=\"step\", lw=2,density=True)\\n        bins_pred = axs[idx].hist(pred, range=crange[:2],bins=crange[-1], histtype=\"step\", lw=2,density=True)\\n        axs[idx].set_xlabel(\"{}\".format(cname))\\n        axs[idx].get_yaxis().set_visible(False)\\n    #     axs[idx].set_yscale(\"log\", nonposy=\\'clip\\')\\n    _ = axs[0,0].legend([\"True\",\"Pred\"], loc=\\'upper right\\')\\n    _ = axs[0,0].set_title(title)\\n    plt.tight_layout()\\n    if fname:\\n        fig.savefig(fname)'"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def METPhiMap(metphis):\n",
    "    \"\"\"Maps number line to period boundary conditions between [-pi,pi]\"\"\"\n",
    "    #Add or subtract the proper number of factors of 2pi. If number is between [-3pi, -pi] add 2pi,\n",
    "    #if between [-5pi, -3pi], add 4pi. For positive intervals subtract instead of add. To get the \n",
    "    #number of 2pis to subtract or add, take the floor of the abs of the number over pi, that gives\n",
    "    #the integer number of pis away from 0. Subtract 2pi for every 2pi if you are greater than pi,\n",
    "    #and subtract another for every 2pi greater than pi. The same holds in reverse for negative values.\n",
    "    return metphis - np.sign(metphis)*np.ceil(np.floor(np.abs(metphis)/(np.pi))/2)*2*np.pi\n",
    "\n",
    "def M4(E,px,py,pz):\n",
    "    \"\"\"Takes in 4 vector components in cartesian and outputs minkowski scalar invariant\"\"\"\n",
    "    return np.sqrt(E*E - px*px - py*py - pz*pz)\n",
    "        \n",
    "#a = np.array([100,200,300,400,500,600,700,800,900])\n",
    "#b = np.array([-1,-2,-3,-4,-5,-6,-7,-8,-9])\n",
    "#c = np.array([10,20,30,40,50,60,70,80,90])\n",
    "#d = np.array([-10,-20,-30,-40,-50,-60,-70,-80,-90])\n",
    "#print(M4(a,b,c,d))\n",
    "\n",
    "def make_MNIST_plots(preds, output_dir, epoch, nExamples=100):\n",
    "    generatedImages = preds.reshape(nExamples, 28, 28)\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for i in range(nExamples):\n",
    "        plt.subplot(np.ceil(np.sqrt(nExamples)), np.ceil(np.sqrt(nExamples)), i+1)\n",
    "        plt.imshow(generatedImages[i], interpolation='nearest', cmap='gray_r')\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('%s/gan_generated_image_epoch_%d.png' %(output_dir, epoch))\n",
    "    plt.close()\n",
    "\n",
    "def make_plots(preds,reals,title=\"\",fname=\"\",show_pred=True,wspace=0.1,hspace=0.3,tightlayout=True,visible=False):\n",
    "    nrows, ncols = 5,5\n",
    "    fig, axs = plt.subplots(nrows,ncols,figsize=(16,13))\n",
    "#     fig, axs = plt.subplots(nrows,ncols,figsize=(12,10))\n",
    "#     fig.subplots_adjust(wspace=0.1,hspace=0.3)\n",
    "    fig.subplots_adjust(wspace=wspace,hspace=hspace)\n",
    "\n",
    "\n",
    "    info = [\n",
    "        [\"lep1_e\",(0,250,50)],\n",
    "        [\"lep1_px\",(-100,100,50)],\n",
    "        [\"lep1_py\",(-100,100,50)],\n",
    "        [\"lep1_pz\",(-200,200,50)],\n",
    "        [\"lep2_e\",(0,250,50)],\n",
    "        [\"lep2_px\",(-100,100,50)],\n",
    "        [\"lep2_pz\",(-200,200,50)],\n",
    "        [\"dphi\",(-4,4,50)],\n",
    "        [\"nvtxs\",(0,50,350)],\n",
    "        [\"met\",(0,150,50)],\n",
    "        [\"metphi\",(-6,6,50)],\n",
    "        [\"lep1_charge\",(-7,7,30)],\n",
    "        [\"lep2_charge\",(-7,7,30)],\n",
    "        [\"lep1_iso\",(0,2.0,30)],\n",
    "        [\"lep2_iso\",(0,2.0,30)],\n",
    "        [\"jet_pt1\",(0,100,50)],\n",
    "        [\"jet_pt2\",(0,100,50)],\n",
    "        [\"jet_pt3\",(0,100,50)],\n",
    "        [\"jet_pt4\",(0,100,50)],\n",
    "        [\"jet_pt5\",(0,100,50)],\n",
    "        [\"mll\",(60,120,50)],\n",
    "        [\"lep1_mass\",(0,1,50)],\n",
    "        [\"lep2_mass\",(0,1,50)],\n",
    "        [\"njets\",(0,7,7)],\n",
    "\n",
    "    ]\n",
    "    for axx in axs:\n",
    "        for ax in axx:\n",
    "            ax.get_yaxis().set_visible(False)\n",
    "    for ic,(cname,crange) in enumerate(info):\n",
    "        if cname == \"mll\":\n",
    "            real = reals[\"mll\"]\n",
    "            pred = Minv(preds,ptetaphi=False,nopy2=True)\n",
    "        elif cname == \"lep1_mass\": real, pred = M4(reals[\"lep1_e\"], reals[\"lep1_px\"], reals[\"lep1_py\"], reals[\"lep1_pz\"]), M4(preds[:,0], preds[:,1], preds[:,2], preds[:,3])\n",
    "        elif cname == \"lep2_mass\": real, pred = M4(reals[\"lep2_e\"], reals[\"lep2_px\"], 0, reals[\"lep2_pz\"]), M4(preds[:,4], preds[:,5], preds[:,6], preds[:,7])\n",
    "        elif cname == \"lep1_e\": real, pred = reals[cname], preds[:,0]\n",
    "        elif cname == \"lep1_pz\": real, pred = reals[cname], preds[:,3]\n",
    "        elif cname == \"lep2_e\": real, pred = reals[cname], preds[:,4]\n",
    "        elif cname == \"lep2_pz\": real, pred = reals[cname], preds[:,6]\n",
    "        elif cname == \"lep1_px\": \n",
    "            real = reals[cname]\n",
    "            pred = preds[:,1]\n",
    "        elif cname == \"lep1_py\":\n",
    "            real = reals[cname]\n",
    "            pred = preds[:,2]\n",
    "        elif cname == \"lep2_px\":\n",
    "            real = reals[cname]\n",
    "            pred = preds[:,5]\n",
    "        elif cname == \"dphi\":\n",
    "            real = get_dphi(reals[\"lep1_px\"], reals[\"lep1_py\"], reals[\"lep2_px\"], np.zeros(len(reals)))\n",
    "            pred = get_dphi(preds[:,1], preds[:,2], preds[:,5], np.zeros(len(preds)))\n",
    "        elif cname == \"nvtxs\": real, pred = reals[cname], np.round(preds[:,7])\n",
    "        elif cname == \"lep1_charge\": real, pred = reals[cname], preds[:,8]\n",
    "        elif cname == \"lep2_charge\": real, pred = reals[cname], preds[:,9]\n",
    "        elif cname == \"lep1_iso\": real, pred = reals[cname], preds[:,10]\n",
    "        elif cname == \"lep2_iso\": real, pred = reals[cname], preds[:,11]\n",
    "        elif cname == \"met\": real, pred = reals[cname], preds[:,12]\n",
    "        elif cname == \"metphi\": real, pred = reals[cname], METPhiMap(preds[:,13])\n",
    "        elif cname == \"jet_pt1\": real, pred = reals[cname], preds[:,14]\n",
    "        elif cname == \"jet_pt2\": real, pred = reals[cname], preds[:,15]\n",
    "        elif cname == \"jet_pt3\": real, pred = reals[cname], preds[:,16]\n",
    "        elif cname == \"jet_pt4\": real, pred = reals[cname], preds[:,17]\n",
    "        elif cname == \"jet_pt5\": real, pred = reals[cname], preds[:,18]\n",
    "        elif cname == \"njets\":\n",
    "            real = \\\n",
    "                1*(reals[\"jet_pt1\"] > 10) + \\\n",
    "                1*(reals[\"jet_pt2\"] > 10) + \\\n",
    "                1*(reals[\"jet_pt3\"] > 10) + \\\n",
    "                1*(reals[\"jet_pt4\"] > 10) + \\\n",
    "                1*(reals[\"jet_pt5\"] > 10)\n",
    "            pred = \\\n",
    "                1*(preds[:,14] > 10) + \\\n",
    "                1*(preds[:,15] > 10) + \\\n",
    "                1*(preds[:,16] > 10) + \\\n",
    "                1*(preds[:,17] > 10) + \\\n",
    "                1*(preds[:,18] > 10)\n",
    "        idx = ic // ncols, ic % ncols\n",
    "        bins_real = axs[idx].hist(real, range=crange[:2],bins=crange[-1], histtype=\"step\", lw=1.5,density=True)\n",
    "        if show_pred:\n",
    "            bins_pred = axs[idx].hist(pred, range=crange[:2],bins=crange[-1], histtype=\"step\", lw=1.5,density=True)\n",
    "        axs[idx].set_xlabel(\"{}\".format(cname))\n",
    "        axs[idx].get_yaxis().set_visible(False)\n",
    "    #     axs[idx].set_yscale(\"log\", nonposy='clip')\n",
    "    _ = axs[0,0].legend([\"True\",\"Pred\"], loc='upper right')\n",
    "    _ = axs[0,0].set_title(title)\n",
    "    if tightlayout:\n",
    "        plt.tight_layout()\n",
    "    if fname:\n",
    "        fig.savefig(fname)\n",
    "    if not visible:\n",
    "        plt.close(fig)\n",
    "\n",
    "\"\"\"def make_plots(preds,reals,title=\"\",fname=\"\"):\n",
    "    nrows, ncols = 5,5\n",
    "    fig, axs = plt.subplots(nrows,ncols,figsize=(16,13))\n",
    "    fig.subplots_adjust(wspace=0.1,hspace=0.3)\n",
    "\n",
    "\n",
    "    #print(preds)\n",
    "    info = [\n",
    "        [\"mll\",(60,120,50)],\n",
    "        [\"lep1_e\",(0,250,50)],\n",
    "        [\"lep1_px\",(-100,100,50)],\n",
    "        [\"lep1_py\",(-100,100,50)],\n",
    "        [\"lep1_pz\",(-200,200,50)],\n",
    "        [\"lep2_e\",(0,250,50)],\n",
    "        [\"lep2_px\",(-100,100,50)],\n",
    "        [\"lep2_pz\",(-200,200,50)],\n",
    "        [\"dphi\",(-4,4,50)],\n",
    "        [\"nvtxs\",(0,50,350)],\n",
    "        [\"met\",(0,150,50)],\n",
    "        [\"metphi\",(-6,6,50)],\n",
    "        [\"lep1_charge\",(-7,7,30)],\n",
    "        [\"lep2_charge\",(-7,7,30)],\n",
    "        [\"lep1_iso\",(0,0.2,30)],\n",
    "        [\"lep2_iso\",(0,0.2,30)],\n",
    "        [\"genjet_pt1\",(0,100,50)],\n",
    "        [\"genjet_pt2\",(0,100,50)],\n",
    "        [\"genjet_pt3\",(0,100,50)],\n",
    "        [\"genjet_pt4\",(0,100,50)],\n",
    "        [\"genjet_pt5\",(0,100,50)],\n",
    "\n",
    "    ]\n",
    "    for ic,(cname,crange) in enumerate(info):\n",
    "        if cname == \"mll\":\n",
    "            real = reals[\"mll\"]\n",
    "            pred = Minv(preds,ptetaphi=False,nopy2=True)\n",
    "        elif cname == \"lep1_e\": real, pred = reals[cname], preds[:,0]\n",
    "        elif cname == \"lep1_pz\": real, pred = reals[cname], preds[:,3]\n",
    "        elif cname == \"lep2_e\": real, pred = reals[cname], preds[:,4]\n",
    "        elif cname == \"lep2_pz\": real, pred = reals[cname], preds[:,6]\n",
    "        elif cname == \"lep1_px\": \n",
    "            real = get_rotated_pxpy(reals[\"lep1_px\"], reals[\"lep1_py\"], reals[\"lep2_px\"], reals[\"lep2_py\"])[0]\n",
    "            pred = preds[:,1]\n",
    "        elif cname == \"lep1_py\":\n",
    "            real = get_rotated_pxpy(reals[\"lep1_px\"], reals[\"lep1_py\"], reals[\"lep2_px\"], reals[\"lep2_py\"])[1]\n",
    "            pred = preds[:,2]\n",
    "        elif cname == \"lep2_px\":\n",
    "            real = get_rotated_pxpy(reals[\"lep1_px\"], reals[\"lep1_py\"], reals[\"lep2_px\"], reals[\"lep2_py\"])[2]\n",
    "            pred = preds[:,5]\n",
    "        elif cname == \"dphi\":\n",
    "            real = get_dphi(reals[\"lep1_px\"], reals[\"lep1_py\"], reals[\"lep2_px\"], reals[\"lep2_py\"])\n",
    "            pred = get_dphi(preds[:,1], preds[:,2], preds[:,5], np.zeros(len(preds)))\n",
    "        elif cname == \"nvtxs\": real, pred = reals[cname], np.round(preds[:,7])\n",
    "        elif cname == \"lep1_charge\": real, pred = reals[cname], preds[:,8]\n",
    "        elif cname == \"lep2_charge\": real, pred = reals[cname], preds[:,9]\n",
    "        elif cname == \"lep1_iso\": real, pred = reals[cname], preds[:,10]\n",
    "        elif cname == \"lep2_iso\": real, pred = reals[cname], preds[:,11]\n",
    "        elif cname == \"met\": real, pred = reals[cname], preds[:,12]\n",
    "        elif cname == \"metphi\": real, pred = reals[cname], METPhiMap(preds[:,13])\n",
    "        elif cname == \"genjet_pt1\": real, pred = reals[cname], preds[:,14]\n",
    "        elif cname == \"genjet_pt2\": real, pred = reals[cname], preds[:,15]\n",
    "        elif cname == \"genjet_pt3\": real, pred = reals[cname], preds[:,16]\n",
    "        elif cname == \"genjet_pt4\": real, pred = reals[cname], preds[:,17]\n",
    "        elif cname == \"genjet_pt5\": real, pred = reals[cname], preds[:,18]\n",
    "        idx = ic // ncols, ic % ncols\n",
    "        bins_real = axs[idx].hist(real, range=crange[:2],bins=crange[-1], histtype=\"step\", lw=2,density=True)\n",
    "        bins_pred = axs[idx].hist(pred, range=crange[:2],bins=crange[-1], histtype=\"step\", lw=2,density=True)\n",
    "        axs[idx].set_xlabel(\"{}\".format(cname))\n",
    "        axs[idx].get_yaxis().set_visible(False)\n",
    "    #     axs[idx].set_yscale(\"log\", nonposy='clip')\n",
    "    _ = axs[0,0].legend([\"True\",\"Pred\"], loc='upper right')\n",
    "    _ = axs[0,0].set_title(title)\n",
    "    plt.tight_layout()\n",
    "    if fname:\n",
    "        fig.savefig(fname)\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN():\n",
    "    def __init__(self, **kwargs):\n",
    "\n",
    "        self.args = dict(kwargs)\n",
    "\n",
    "        self.tag = kwargs[\"tag\"]\n",
    "        self.input_file = str(kwargs[\"input_file\"])\n",
    "        self.noise_shape = (int(kwargs[\"noise_size\"]),)\n",
    "        self.output_shape = (int(kwargs[\"output_size\"]),)\n",
    "        self.noise_type = int(kwargs[\"noise_type\"])\n",
    "        self.ntest_samples = int(kwargs[\"ntest_samples\"])\n",
    "        self.nepochs_dump_pred_metrics = int(kwargs[\"nepochs_dump_pred_metrics\"])\n",
    "        self.nepochs_dump_models = int(kwargs[\"nepochs_dump_models\"])\n",
    "        self.nepochs_dump_plots = int(kwargs[\"nepochs_dump_plots\"])\n",
    "        self.nepochs_max = int(kwargs[\"nepochs_max\"])\n",
    "        self.batch_size = int(kwargs[\"batch_size\"])\n",
    "        self.do_concatenate_disc = kwargs[\"do_concatenate_disc\"]\n",
    "        self.do_concatenate_gen = kwargs[\"do_concatenate_gen\"]\n",
    "        self.do_batch_normalization_disc = kwargs[\"do_batch_normalization_disc\"]\n",
    "        self.do_batch_normalization_gen = kwargs[\"do_batch_normalization_gen\"]\n",
    "        self.do_soft_labels = kwargs[\"do_soft_labels\"]\n",
    "        self.do_noisy_labels = kwargs[\"do_noisy_labels\"]\n",
    "        self.do_tanh_gen = kwargs[\"do_tanh_gen\"]\n",
    "        self.nepochs_decay_noisy_labels = int(kwargs[\"nepochs_decay_noisy_labels\"])\n",
    "        self.use_ptetaphi_additionally = kwargs[\"use_ptetaphi_additionally\"]\n",
    "        self.optimizer_gen = kwargs[\"optimizer_gen\"]\n",
    "        self.optimizer_disc = kwargs[\"optimizer_disc\"]\n",
    "        self.depth_disc = kwargs[\"depth_disc\"]\n",
    "        self.width_disc = kwargs[\"width_disc\"]\n",
    "        self.depth_gen = kwargs[\"depth_gen\"]\n",
    "        self.width_gen = kwargs[\"width_gen\"]\n",
    "        self.beefy_generator = kwargs[\"beefy_generator\"]\n",
    "        self.beefy_discriminator = kwargs[\"beefy_discriminator\"]\n",
    "        self.add_invmass_disc = kwargs[\"add_invmass_disc\"]\n",
    "        self.fix_delphes_outputs = kwargs[\"fix_delphes_outputs\"]\n",
    "        self.use_delphes = kwargs[\"use_delphes\"]\n",
    "        self.use_mll_loss = kwargs[\"use_mll_loss\"]\n",
    "        self.loss_mll_weight = kwargs[\"loss_mll_weight\"]\n",
    "        self.do_skip_connection = kwargs[\"do_skip_connection\"]\n",
    "        self.terminate_early = kwargs[\"terminate_early\"]\n",
    "        self.loss_type = kwargs[\"loss_type\"]\n",
    "        self.gen_adam_beta1= kwargs[\"gen_adam_beta1\"] if kwargs.has_key(\"gen_adam_beta1\") else False\n",
    "        self.gen_learning_rate= kwargs[\"gen_learning_rate\"] if kwargs.has_key(\"gen_learning_rate\") else False\n",
    "        self.disc_adam_beta1= kwargs[\"disc_adam_beta1\"] if kwargs.has_key(\"disc_adam_beta1\") else False\n",
    "        self.disc_learning_rate= kwargs[\"disc_learning_rate\"] if kwargs.has_key(\"disc_learning_rate\") else False\n",
    "        self.mnist_test = kwargs[\"mnist_test\"] if kwargs.has_key(\"mnist_test\") else False\n",
    "        \n",
    "        \n",
    "        if self.use_ptetaphi_additionally: self.output_shape = (self.output_shape[0]+8,)\n",
    "            \n",
    "        print(self.__dict__)\n",
    "\n",
    "        os.system(\"mkdir -p /nfs-7/userdata/bhashemi/DY-GAN/models/progress/{}/\".format(self.tag))\n",
    "        print(os.listdir(\"/nfs-7/userdata/bhashemi/DY-GAN/models/progress/{}/\".format(self.tag)))\n",
    "\n",
    "        self.scaler_type = kwargs[\"scaler_type\"]\n",
    "        self.scaler = None\n",
    "        if self.scaler_type.lower() == \"minmax\":\n",
    "            self.scaler = MinMaxScaler(feature_range=(-1.,1.))\n",
    "        elif self.scaler_type.lower() == \"robust\":\n",
    "            self.scaler = RobustScaler()\n",
    "        elif self.scaler_type.lower() == \"standard\":\n",
    "            self.scaler = StandardScaler()\n",
    "\n",
    "        self.data = None\n",
    "        self.data_ref = None\n",
    "        self.d_epochinfo = {}\n",
    "        self.X_train = None\n",
    "\n",
    "        if (self.optimizer_gen == \"adam\"): \n",
    "            if self.gen_adam_beta1 and self.gen_learning_rate:\n",
    "                self.optimizer_gen=keras.optimizers.Adam(lr=self.gen_learning_rate, beta_1=self.gen_adam_beta1)\n",
    "            elif self.gen_learning_rate:\n",
    "                self.optimizer_gen=keras.optimizers.Adam(lr=self.gen_learning_rate)\n",
    "            elif self.gen_adam_beta1:\n",
    "                self.optimizer_gen=keras.optimizers.Adam(beta_1=self.gen_adam_beta1)\n",
    "    \n",
    "        if (self.optimizer_disc == \"adam\"): \n",
    "            if self.disc_adam_beta1 and self.disc_learning_rate:\n",
    "                self.optimizer_disc=keras.optimizers.Adam(lr=self.disc_learning_rate, beta_1=self.disc_adam_beta1)\n",
    "            elif self.disc_learning_rate:\n",
    "                self.optimizer_disc=keras.optimizers.Adam(lr=self.disc_learning_rate)\n",
    "            elif self.disc_adam_beta1:\n",
    "                self.optimizer_disc=keras.optimizers.Adam(beta_1=self.disc_adam_beta1)\n",
    "    \n",
    "        \n",
    "        if self.mnist_test:\n",
    "            print(\"Performing MNIST Test...\")\n",
    "            #self.noise_shape = (784,)\n",
    "            self.noise_shape = (100,)\n",
    "            self.output_shape = (784,)\n",
    "            self.use_mll_loss = False\n",
    "            self.optimizer_disc = keras.optimizers.Adam(lr=0.0002, beta_1=0.5)\n",
    "            self.optimizer_gen = keras.optimizers.Adam(lr=0.0002, beta_1=0.5)\n",
    "        \n",
    "        optimizer_d = self.optimizer_disc\n",
    "        optimizer_g = self.optimizer_gen\n",
    "        \n",
    "        # Build and compile the discriminator\n",
    "        if self.use_mll_loss:\n",
    "            loss = custom_loss(c=self.loss_mll_weight, loss_type=self.loss_type)\n",
    "        else:\n",
    "            loss = \"binary_crossentropy\"\n",
    "            \n",
    "        self.loss=loss\n",
    "            \n",
    "        # Build and compile the two independent models\n",
    "        print(\"Compiling Generator\")\n",
    "        self.generator = self.build_generator()\n",
    "        self.generator.compile(loss=loss, optimizer=optimizer_g)\n",
    "\n",
    "        print(\"Compiling Discriminator\")\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss=custom_loss(\"disc\"), optimizer=optimizer_d, metrics=['accuracy'])\n",
    "        \n",
    "        # Make the combined model\n",
    "        # The combined model (stacked generator and discriminator) takes\n",
    "        # noise as input => generates images => determines validity\n",
    "        self.discriminator.trainable = False #only train the generator\n",
    "        z = Input(shape=self.noise_shape)\n",
    "        img = self.generator(z)\n",
    "        valid = self.discriminator(img)\n",
    "        self.combined = Model(z, valid)\n",
    "        self.combined.compile(loss=loss, optimizer=optimizer_g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(GAN):\n",
    "    \n",
    "    def build_generator(self):\n",
    "    \n",
    "        inputs = Input(shape=self.noise_shape)\n",
    "\n",
    "        ## Head\n",
    "        if self.mnist_test: \n",
    "            x=Dense(256, kernel_initializer=keras.initializers.RandomNormal(stddev=0.02))(inputs)\n",
    "            x=LeakyReLU(0.2)(x)\n",
    "            x=Dense(512)(x)\n",
    "            x=LeakyReLU(0.2)(x)\n",
    "            x=Dense(1024)(x)\n",
    "            x=LeakyReLU(0.2)(x)\n",
    "            x=Dense(784, activation='tanh')(x)\n",
    "            #for size in [784*4,784*4,784*2,784*2]:\n",
    "            #    x = Dense(size)(x)\n",
    "            #    x = LeakyReLU(alpha=0.2)(x)\n",
    "        else:\n",
    "            x = Dense(64)(inputs)\n",
    "\n",
    "            if self.do_batch_normalization_gen:\n",
    "                x = BatchNormalization()(x)\n",
    "            x = LeakyReLU(alpha=0.2)(x)\n",
    "            if self.do_concatenate_gen:\n",
    "                x = Lambda(lambda x: K.concatenate([x*x,x]))(x)\n",
    "                x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "            ## Main Body\n",
    "            if self.depth_gen > 0 and self.width_gen > 0:\n",
    "                for level in xrange(0,self.depth_gen):\n",
    "                    size=self.width_gen/(2**level)\n",
    "                    if(size<self.output_shape[0]):\n",
    "                        raise ValueError(\"The layer size %d would be smaller than the output size, make sure you have a wide enough network to deal with %s layers\" % (size, self.depth_gen))\n",
    "                    x = Dense(size)(x) #Triangle with width halved at each level\n",
    "                    x = LeakyReLU(alpha=0.2)(x)\n",
    "            elif self.beefy_generator:\n",
    "                for size in [128,256,512,256,128]:\n",
    "                    x = Dense(size)(x)\n",
    "                    x = LeakyReLU(alpha=0.2)(x)\n",
    "            else:\n",
    "                for size in [128,128,128,64,32]:\n",
    "                    x = Dense(size)(x)\n",
    "                    x = LeakyReLU(alpha=0.2)(x)\n",
    " \n",
    "        ## Tail\n",
    "        x = Dense(self.output_shape[0])(x)\n",
    "        \n",
    "#         if False:\n",
    "        if self.do_skip_connection:\n",
    "            # get the non-noise part of the input, and add it to the tail\n",
    "            y = Lambda(get_first_N, arguments={'N': self.output_shape[0]})(inputs)\n",
    "#             print y\n",
    "            x = Add()([x,y])\n",
    "#             print x\n",
    "            \n",
    "        if self.do_tanh_gen:\n",
    "            x = Activation(\"tanh\")(x)\n",
    "        elif self.fix_delphes_outputs:\n",
    "            x = Lambda(fix_outputs,\n",
    "                input_shape=self.output_shape,\n",
    "                output_shape=self.output_shape\n",
    "                )(x)\n",
    "            \n",
    "#         model = Model(inputs=inputs, outputs=concatenate([out,mll]))\n",
    "        model = Model(inputs=inputs, outputs=[x])\n",
    "        \n",
    "        print \"Generator params: {}\".format(model.count_params())\n",
    "#         model.summary()\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(GAN):\n",
    "    \n",
    "    def build_discriminator_mnist(self):\n",
    "        discriminator = Sequential()\n",
    "        discriminator.add(Dense(1024, input_dim=784, kernel_initializer=keras.initializers.RandomNormal(stddev=0.02)))\n",
    "        discriminator.add(LeakyReLU(0.2))\n",
    "        discriminator.add(Dropout(0.3))\n",
    "        discriminator.add(Dense(512))\n",
    "        discriminator.add(LeakyReLU(0.2))\n",
    "        discriminator.add(Dropout(0.3))\n",
    "        discriminator.add(Dense(256))\n",
    "        discriminator.add(LeakyReLU(0.2))\n",
    "        discriminator.add(Dropout(0.3))\n",
    "        discriminator.add(Dense(1, activation='sigmoid'))\n",
    "        return discriminator\n",
    "    \n",
    "    def build_discriminator(self):\n",
    "\n",
    "        if self.mnist_test: \n",
    "            return self.build_discriminator_mnist()\n",
    "        \n",
    "        inputs = Input(self.output_shape)\n",
    "        if self.mnist_test:\n",
    "            x = Dense(1024, kernel_initializer=keras.initializers.RandomNormal(stddev=0.02))(inputs)\n",
    "            x = LeakyReLU(0.2)(x)\n",
    "            x = Dropout(0.3)(x)\n",
    "            x = Dense(512)(x)\n",
    "            x = LeakyReLU(0.2)(x)\n",
    "            x = Dropout(0.3)(x)\n",
    "            x = Dense(256)(x)\n",
    "            x = LeakyReLU(0.2)(x)\n",
    "            x = Dropout(0.3)(x)\n",
    "        else:\n",
    "            mll = Lambda(invmass_from_8cartesian_nopy2)(inputs)\n",
    "            x = Dense(128)(inputs)\n",
    "            if self.do_batch_normalization_disc:\n",
    "                x = BatchNormalization()(x)\n",
    "            x = LeakyReLU(alpha=0.2)(x)\n",
    "            if self.do_concatenate_disc:\n",
    "                x = Lambda(lambda x: K.concatenate([x*x,x]))(x)\n",
    "                x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "            ## Main Body\n",
    "            if self.depth_disc > 0 and self.width_disc > 0:\n",
    "                for level in xrange(0,self.depth_disc):\n",
    "                    x = Dense(self.width_disc/(2**level))(x) #Triangle with width halved at each level\n",
    "                    x = LeakyReLU(alpha=0.2)(x)\n",
    "            elif self.beefy_generator:\n",
    "                for size in [128,256,256,128,64,32,16,8]:\n",
    "                    x = Dense(size)(x)\n",
    "                    x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "            else:\n",
    "                for size in [128]*5 + [64,32,16,8]:\n",
    "                    x = Dense(size)(x)\n",
    "                    x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "        ## Tail\n",
    "        out = Dense(1,activation='sigmoid')(x)\n",
    "        \n",
    "        if self.use_mll_loss:\n",
    "            model = Model(inputs=inputs, outputs=concatenate([out,mll]))\n",
    "        else:\n",
    "            model = Model(inputs=inputs, outputs=out)\n",
    "#         print model.output_shape\n",
    "#         model.summary()\n",
    "        print \"Discriminator params: {}\".format(model.count_params())\n",
    "        \n",
    "        return model\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(GAN):\n",
    "    \n",
    "    def load_data(self):\n",
    "        if self.data is not None: return\n",
    "        \n",
    "        if self.mnist_test:\n",
    "            (self.data, y_train), (X_test, y_test) = mnist.load_data()\n",
    "            self.data = np.array([x.flatten() for x in self.data[:]])\n",
    "        elif self.use_delphes:\n",
    "            self.data = np.load(self.input_file)\n",
    "            self.data = self.data[self.data[\"genmll\"] > 50.]\n",
    "        else:\n",
    "            self.data = np.load(self.input_file)\n",
    "            self.data = self.data[self.data[\"genmll\"] > 50.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(GAN):\n",
    "\n",
    "    def get_noise(self, amount=1024):\n",
    "        # nominal\n",
    "        if self.noise_type == 1:\n",
    "            noise_half = np.random.normal(0, 1, (amount//2, self.noise_shape[0]))\n",
    "            noise_full = np.random.normal(0, 1, (amount, self.noise_shape[0]))\n",
    "\n",
    "        elif self.noise_type == 2: # random soup, 4,2,2 have to be modified to sum to noise_shape[0]\n",
    "            ngaus = self.noise_shape[0] // 2\n",
    "            nflat = (self.noise_shape[0] - ngaus) // 2\n",
    "            nexpo = self.noise_shape[0] - nflat - ngaus\n",
    "            noise_gaus = np.random.normal( 0, 1, (amount//2+amount, ngaus))\n",
    "            noise_flat = np.random.uniform(-1, 1, (amount//2+amount, nflat))\n",
    "            noise_expo = np.random.exponential( 1,    (amount//2+amount, nexpo))\n",
    "            noise = np.c_[ noise_gaus,noise_flat,noise_expo ]\n",
    "            noise_half = noise[:amount//2]\n",
    "            noise_full = noise[-amount:]\n",
    "            \n",
    "        elif self.noise_type == 3: # truth conditioned\n",
    "            \n",
    "#             noise_half = np.c_[ \n",
    "#                     self.X_train[np.random.randint(0, self.X_train.shape[0], amount//2)], \n",
    "#                     np.random.normal(0, 1, (amount//2,self.noise_shape[0]-self.X_train.shape[1]))\n",
    "#                     ]\n",
    "#             noise_full = np.c_[ \n",
    "#                     self.X_train[np.random.randint(0, self.X_train.shape[0], amount)], \n",
    "#                     np.random.normal(0, 1, (amount,self.noise_shape[0]-self.X_train.shape[1]))\n",
    "#                     ]\n",
    "            \n",
    "            npurenoise = self.noise_shape[0]-self.X_train.shape[1]\n",
    "            ngaus = npurenoise // 2\n",
    "            nflat = (npurenoise - ngaus) // 2\n",
    "            nexpo = npurenoise - nflat - ngaus\n",
    "            noise_gaus = np.random.normal( 0, 1, (amount//2+amount, ngaus))\n",
    "            noise_flat = np.random.uniform(-1, 1, (amount//2+amount, nflat))\n",
    "            noise_expo = np.random.exponential( 1,    (amount//2+amount, nexpo))\n",
    "            truenoise = self.X_train[np.random.randint(0, self.X_train.shape[0], amount//2+amount)]\n",
    "            noise = np.c_[ truenoise,noise_gaus,noise_flat,noise_expo ]\n",
    "            noise_half = noise[:amount//2]\n",
    "            noise_full = noise[-amount:]\n",
    "\n",
    "        return noise_half, noise_full\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(GAN):\n",
    "            \n",
    "    def train(self):\n",
    "\n",
    "        print(\"data loaded\")\n",
    "        self.load_data()\n",
    "        \n",
    "        \n",
    "        if self.mnist_test:\n",
    "            #print(\"Doing MNIST test! Loading in MNIST Data...\")\n",
    "            self.X_train = self.data\n",
    "            self.X_train = (self.X_train.astype(np.float32) - 127.5)/127.5\n",
    "            #print(\"self.X_train shape:\", self.X_train.shape)\n",
    "        elif self.use_delphes:\n",
    "            lepcoords = np.c_[\n",
    "                self.data[\"lep1_e\"],\n",
    "                self.data[\"lep1_px\"],\n",
    "                self.data[\"lep1_py\"],\n",
    "                self.data[\"lep1_pz\"],\n",
    "                self.data[\"lep2_e\"],\n",
    "                self.data[\"lep2_px\"],\n",
    "#                 self.data[\"lep2_py\"],\n",
    "                self.data[\"lep2_pz\"],\n",
    "            ]\n",
    "#             lepcoords_dphi = cartesian_zerophi2(lepcoords)\n",
    "            \n",
    "            nvtx_smeared = np.round(np.random.normal(self.data[\"nvtxs\"],0.5))\n",
    "            self.X_train = np.c_[\n",
    "#                 lepcoords_dphi, # 7 columns\n",
    "                lepcoords, # 7 columns\n",
    "                nvtx_smeared, # 1 column\n",
    "                self.data[\"lep1_charge\"], self.data[\"lep2_charge\"],\n",
    "                self.data[\"lep1_iso\"], self.data[\"lep2_iso\"],\n",
    "                self.data[\"met\"], self.data[\"metphi\"],\n",
    "                self.data[\"jet_pt1\"],\n",
    "                self.data[\"jet_pt2\"],\n",
    "                self.data[\"jet_pt3\"],\n",
    "                self.data[\"jet_pt4\"],\n",
    "                self.data[\"jet_pt5\"],\n",
    "            ].astype(np.float32)\n",
    "        else:\n",
    "            self.X_train = self.data[:,range(1,1+8)]\n",
    "            if self.use_ptetaphi_additionally:\n",
    "                self.X_train = np.c_[self.X_train, cartesian_to_ptetaphi(self.X_train)]\n",
    "\n",
    "        # # NOTE. StandardScaler should be fit on training set\n",
    "        # # and applied the same to train and test, otherwise we\n",
    "        # # introduce a bias\n",
    "        if self.scaler:\n",
    "            self.scaler.fit(self.X_train)\n",
    "            self.X_train = self.scaler.transform(self.X_train).astype(np.float32)\n",
    "            pickle.dump(self.scaler, open(\"/nfs-7/userdata/bhashemi/DY-GAN/models/progress/{}/scaler.pkl\".format(self.tag),'w'))\n",
    "\n",
    "        # make an alias to save typing\n",
    "        X_train = self.X_train\n",
    "        #print(\"X_train shape:\",X_train.shape)\n",
    "        \n",
    "        half_batch = int(self.batch_size / 2)\n",
    "\n",
    "        prev_gen_loss = -1\n",
    "        prev_disc_loss = -1\n",
    "        n_loss_same_gen = 0  # number of epochs for which generator loss has remained ~same (within 0.01%)\n",
    "        n_loss_same_disc = 0  # number of epochs for which discriminator loss has remained ~same (within 0.01%)\n",
    "        old_info = -1, -1\n",
    "        \n",
    "        logfile = open(\"/nfs-7/userdata/bhashemi/DY-GAN/models/progress/{}/log.txt\".format(self.tag),'w+')\n",
    "        os.system(\"cp {} /nfs-7/userdata/bhashemi/DY-GAN/models/progress/{}/\".format(filename, self.tag))\n",
    "        logfile.write(\"Getting Started! Copied notebook into progress directory.\\n\")\n",
    "        for epoch in range(self.nepochs_max):\n",
    "\n",
    "            if self.terminate_early:\n",
    "                if n_loss_same_gen > 1000 or n_loss_same_disc > 1000:\n",
    "                    print \"BREAKING because disc/gen loss has remained the same for {}/{} epochs!\".format(n_loss_same_disc,n_loss_same_gen)\n",
    "                    break\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random half batch of images\n",
    "            idx = np.random.randint(0, X_train.shape[0], half_batch)\n",
    "            imgs = X_train[idx]\n",
    "            \n",
    "            noise_half, noise_full = self.get_noise(self.batch_size)\n",
    "            \n",
    "            # Generate a half batch of new images\n",
    "            gen_imgs = self.generator.predict(noise_full)\n",
    "\n",
    "            # Train the discriminator\n",
    "            ones = np.ones((half_batch, 1))\n",
    "            zeros = np.zeros((half_batch, 1))\n",
    "\n",
    "            if self.do_soft_labels:\n",
    "                ones *= 0.9\n",
    "\n",
    "            if self.do_noisy_labels:\n",
    "                frac = 0.3*np.exp(-epoch/self.nepochs_decay_noisy_labels)\n",
    "                if frac > 0.005:\n",
    "                    ones[np.random.randint(0, len(ones), int(frac*len(ones)))] = 0\n",
    "                    zeros[np.random.randint(0, len(zeros), int(frac*len(zeros)))] = 1\n",
    "\n",
    "            #print(imgs.shape)\n",
    "            #print(ones.shape)\n",
    "            self.discriminator.trainable = True\n",
    "            \n",
    "            dw = np.array([])\n",
    "            gw = np.array([])\n",
    "            \n",
    "            if epoch == 1: \n",
    "                print(\"About to Train Discriminator --- \")\n",
    "                dw_=np.array([x.flatten() for x in self.discriminator.get_weights()])\n",
    "                gw_=np.array([x.flatten() for x in self.generator.get_weights()])\n",
    "                for i in range(len(dw_)): dw=np.concatenate([dw, dw_[i]])\n",
    "                for i in range(len(gw_)): gw=np.concatenate([gw, gw_[i]])\n",
    "                \n",
    "            d_loss = self.discriminator.train_on_batch(np.concatenate([imgs, gen_imgs[:half_batch]]), np.concatenate([ones, zeros]))\n",
    "            #d_loss_real = self.discriminator.train_on_batch(imgs, ones)\n",
    "            #print(\"Real Disc loss: %s \" % str(d_loss_real[0]))\n",
    "            #d_loss_fake = self.discriminator.train_on_batch(gen_imgs, zeros)\n",
    "            #print(\"Fake Disc loss: %s \" % str(d_loss_real[0]))\n",
    "            #d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "            #print(\"Avg Disc loss: %s \" % str(d_loss[0]))\n",
    "            self.discriminator.trainable = False\n",
    "            \"\"\"if epoch == 1: \n",
    "                print(\"Just Trained Discriminator --- \")\n",
    "                dw_tmp=np.array([x.flatten() for x in self.discriminator.get_weights()])\n",
    "                gw_tmp=np.array([x.flatten() for x in self.generator.get_weights()])\n",
    "                dw_ = np.array([])\n",
    "                gw_ = np.array([])\n",
    "                for i in range(dw_tmp.shape[0]): dw_=np.concatenate([dw_, dw_tmp[i]])\n",
    "                for i in range(gw_tmp.shape[0]): gw_=np.concatenate([gw_, gw_tmp[i]])\n",
    "                if str(dw_) != str(dw):\n",
    "                    diff = {}\n",
    "                    print(\"Disc Weights Changed As Intended\")\n",
    "                    for i in xrange(dw_.shape[0]):\n",
    "                        if (dw_[i] != dw[i]): diff[i] = np.abs(dw_[i] - dw[i])\n",
    "                    print(\"Number different:\", len(diff.keys()))\n",
    "                    #print(diff)\n",
    "                    dw = dw_\n",
    "                if str(gw_) != str(gw):\n",
    "                    diff = {}\n",
    "                    print(\"Generator Weights Changed When Training Discriminator!\")\n",
    "                    for i in xrange(gw_.shape[0]):\n",
    "                        if (gw_[i] != gw[i]): diff[i] = np.abs(gw_[i] - gw[i])\n",
    "                    print(\"Number different:\", len(diff.keys()))\n",
    "                    #print(diff)\n",
    "                    gw=gw_\"\"\"\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "            # The generator wants the discriminator to label the generated samples\n",
    "            # as valid (ones)\n",
    "            valid_y = np.array([1] * self.batch_size)\n",
    "\n",
    "            # Train the generator\n",
    "            g_loss = self.combined.train_on_batch(noise_full, valid_y)\n",
    "\n",
    "            if (g_loss - prev_gen_loss) < 0.0001: n_loss_same_gen += 1\n",
    "            else: n_loss_same_gen = 0\n",
    "            prev_gen_loss = g_loss\n",
    "\n",
    "            if (d_loss[0] - prev_disc_loss) < 0.0001: n_loss_same_disc += 1\n",
    "            else: n_loss_same_disc = 0\n",
    "            prev_disc_loss = d_loss[0]\n",
    "            \n",
    "            \"\"\"if epoch == 1: \n",
    "                print(\"Just Trained Generator --- \")\n",
    "                dw_tmp=np.array([x.flatten() for x in self.discriminator.get_weights()])\n",
    "                gw_tmp=np.array([x.flatten() for x in self.generator.get_weights()])\n",
    "                dw_ = np.array([])\n",
    "                gw_ = np.array([])\n",
    "                for i in range(dw_tmp.shape[0]): dw_=np.concatenate([dw_, dw_tmp[i]])\n",
    "                for i in range(gw_tmp.shape[0]): gw_=np.concatenate([gw_, gw_tmp[i]])\n",
    "                if str(dw_) != str(dw):\n",
    "                    diff = {}\n",
    "                    print(\"Disc Weights Changed When Training Generator!\")\n",
    "                    for i in xrange(dw_.shape[0]):\n",
    "                        if (dw_[i] != dw[i]): diff[i] = np.abs(dw_[i] - dw[i])\n",
    "                    print(\"Number different:\", len(diff.keys()))\n",
    "                    #print(diff)\n",
    "                    dw = dw_\n",
    "                if str(gw_) != str(gw):\n",
    "                    diff = {}\n",
    "                    print(\"Generator Weights Changed As Intended\")\n",
    "                    for i in xrange(gw_.shape[0]):\n",
    "                        if (gw_[i] != gw[i]): diff[i] = np.abs(gw_[i] - gw[i])\n",
    "                    print(\"Number different:\", len(diff.keys()))\n",
    "                    #print(diff)\n",
    "                    gw=gw_\"\"\"\n",
    "\n",
    "            # Plot the progress\n",
    "#             print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "            \n",
    "            if self.mnist_test:\n",
    "                sys.stdout.write(\"\\r{} [D loss: {}, acc.: {:.2f}%] [G loss: {}]\".format(epoch, d_loss[0], 100.0*d_loss[1], g_loss))\n",
    "                if epoch % self.nepochs_dump_pred_metrics == 0 and epoch > 0:\n",
    "                    make_MNIST_plots(gen_imgs[:100], \"/nfs-7/userdata/bhashemi/DY-GAN/models/progress/{}\".format(self.tag), epoch)\n",
    "                continue\n",
    "            \n",
    "            sys.stdout.write(\"\\r{} [D loss: {}, acc.: {:.2f}%] [G loss: {}] [mll={:.3f}+-{:.3f}]\".format(epoch, d_loss[0], 100.0*d_loss[1], g_loss, old_info[0], old_info[1]))\n",
    "\n",
    "            if epoch % self.nepochs_dump_pred_metrics == 0 and epoch > 0:\n",
    "            \n",
    "                _, noise_test = self.get_noise(self.ntest_samples)\n",
    "            \n",
    "                sys.stdout.write(\"\\n\") # break up the stream of text\n",
    "\n",
    "                gen_imgs = self.generator.predict(noise_test)\n",
    "                \n",
    "                if self.scaler:\n",
    "                    gen_imgs = self.scaler.inverse_transform(gen_imgs)\n",
    "\n",
    "                masses = Minv(gen_imgs,nopy2=True)\n",
    "                masses = masses[np.isfinite(masses)]\n",
    "                old_info = masses.mean(), masses.std()\n",
    "\n",
    "                cov_diff, avg_diff = covariance_metrics(X_train, gen_imgs)\n",
    "                \n",
    "                if \"epoch\" not in self.d_epochinfo:\n",
    "                    self.d_epochinfo[\"epoch\"] = []\n",
    "                    self.d_epochinfo[\"d_acc\"] = []\n",
    "                    self.d_epochinfo[\"d_loss\"] = []\n",
    "                    self.d_epochinfo[\"g_loss\"] = []\n",
    "                    self.d_epochinfo[\"mass_mu\"] = []\n",
    "                    self.d_epochinfo[\"mass_sig\"] = []\n",
    "                    self.d_epochinfo[\"time\"] = []\n",
    "                    self.d_epochinfo[\"avg_cov_diff\"] = []\n",
    "                    self.d_epochinfo[\"max_cov_diff\"] = []\n",
    "                    self.d_epochinfo[\"std_dev_cov_diff\"] = []\n",
    "                    self.d_epochinfo[\"avg_mean_diff\"] = []\n",
    "                    self.d_epochinfo[\"max_mean_diff\"] = []\n",
    "                    self.d_epochinfo[\"std_dev_mean_diff\"] = []\n",
    "                    self.d_epochinfo[\"args\"] = self.args\n",
    "                else:\n",
    "                    self.d_epochinfo[\"epoch\"].append(epoch)\n",
    "                    self.d_epochinfo[\"d_acc\"].append(100*d_loss[1])\n",
    "                    self.d_epochinfo[\"d_loss\"].append(d_loss[0])\n",
    "                    self.d_epochinfo[\"g_loss\"].append(g_loss)\n",
    "                    self.d_epochinfo[\"mass_mu\"].append(masses.mean())\n",
    "                    self.d_epochinfo[\"mass_sig\"].append(masses.std())\n",
    "                    self.d_epochinfo[\"time\"].append(time.time())\n",
    "                    self.d_epochinfo[\"avg_cov_diff\"].append(cov_diff.mean())\n",
    "                    self.d_epochinfo[\"max_cov_diff\"].append(cov_diff.max())\n",
    "                    self.d_epochinfo[\"std_dev_cov_diff\"].append(cov_diff.std())\n",
    "                    self.d_epochinfo[\"avg_mean_diff\"].append(avg_diff.mean())\n",
    "                    self.d_epochinfo[\"max_mean_diff\"].append(avg_diff.max())\n",
    "                    self.d_epochinfo[\"std_dev_mean_diff\"].append(avg_diff.std())\n",
    "                    \n",
    "\n",
    "                pickle.dump(self.d_epochinfo, open(\"/nfs-7/userdata/bhashemi/DY-GAN/models/progress/{}/history.pkl\".format(self.tag),'w'))\n",
    "\n",
    "            if epoch % self.nepochs_dump_plots == 0 and epoch > 0:\n",
    "                _, noise = self.get_noise(self.ntest_samples)\n",
    "                preds = gan.generator.predict(noise)\n",
    "                reals = self.data[:15000]\n",
    "                _ = make_plots(preds,reals,title=\"{}: epoch {}\".format(self.tag,epoch),\n",
    "                               fname=\"/nfs-7/userdata/bhashemi/DY-GAN/models/progress/{}/plots_{:06d}.png\".format(self.tag,epoch))\n",
    "            \n",
    "            if epoch % self.nepochs_dump_models == 0 and epoch > 0:\n",
    "                self.discriminator.save(\"/nfs-7/userdata/bhashemi/DY-GAN/models/progress/{}/disc_{}.weights\".format(self.tag,epoch))\n",
    "                self.generator.save(\"/nfs-7/userdata/bhashemi/DY-GAN/models/progress/{}/gen_{}.weights\".format(self.tag,epoch))\n",
    "                #self.discriminator.save(\"test.weights\")\n",
    "                score = get_score(X_train, gen_imgs)\n",
    "                mll_ks_score = ks_2samp(Minv(X_train,ptetaphi=False,nopy2=True), Minv(gen_imgs,ptetaphi=False,nopy2=True))\n",
    "                MetPhi_ks_score = ks_2samp(METPhiMap(X_train[:,13]), METPhiMap(gen_imgs[:,13]))\n",
    "                Lep1Iso_ks_score = ks_2samp(X_train[:,10], gen_imgs[:,10])\n",
    "                trial = self.tag[1:self.tag.index('_')]\n",
    "                print \"epoch %d trial %s StatsScore %f MLLKSStatistic %f MLLKSPval %f METPhiKSStatistic %f METPhiKSPval %f Lep1IsoKSStatistic %f Lep1IsoKSPval %f \" % (epoch, trial, score, mll_ks_score[0], mll_ks_score[1], MetPhi_ks_score[0], MetPhi_ks_score[1], Lep1Iso_ks_score[0], Lep1Iso_ks_score[1])\n",
    "                logfile.write(\"epoch %d trial %s StatsScore %f MLLKSStatistic %f MLLKSPval %f METPhiKSStatistic %f METPhiKSPval %f Lep1IsoKSStatistic %f Lep1IsoKSPval %f\\n\" % (epoch, trial, score, mll_ks_score[0], mll_ks_score[1], MetPhi_ks_score[0], MetPhi_ks_score[1], Lep1Iso_ks_score[0], Lep1Iso_ks_score[1]))\n",
    "                logfile.write(\"\\r{} [D loss: {}, acc.: {:.2f}%] [G loss: {}] [mll={:.3f}+-{:.3f}]\".format(epoch, d_loss[0], 100.0*d_loss[1], g_loss, old_info[0], old_info[1]))\n",
    "        \n",
    "        logfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'width_disc': 0, 'ntest_samples': 10000, 'optimizer_disc': 'adadelta', 'output_size': 19, 'terminate_early': False, 'do_batch_normalization_disc': False, 'use_delphes': True, 'nepochs_dump_plots': 500, 'use_ptetaphi_additionally': False, 'do_noisy_labels': False, 'tag': 'v1_kstest', 'nepochs_dump_pred_metrics': 250, 'do_batch_normalization_gen': False, 'add_invmass_disc': False, 'width_gen': 0, 'fix_delphes_outputs': False, 'loss_type': 'ks_test', 'nepochs_dump_models': 500, 'input_file': '/home/users/bhashemi/Projects/GIT/DY-GAN/delphes/total_Zmumu_13TeV_PU20_v2.npa', 'noise_type': 1, 'scaler_type': '', 'batch_size': 512, 'do_concatenate_disc': False, 'do_soft_labels': False, 'depth_gen': 0, 'noise_size': 19, 'loss_mll_weight': 0.0001, 'nepochs_max': 100001, 'beefy_discriminator': True, 'depth_disc': 0, 'do_tanh_gen': False, 'do_skip_connection': False, 'use_mll_loss': True, 'beefy_generator': True, 'nepochs_decay_noisy_labels': 2000, 'optimizer_gen': 'adadelta', 'do_concatenate_gen': False}\n",
      "{'width_disc': 0, 'ntest_samples': 10000, 'beefy_generator': True, 'loss_type': 'ks_test', 'terminate_early': False, 'do_batch_normalization_disc': False, 'use_delphes': True, 'use_ptetaphi_additionally': False, 'do_noisy_labels': False, 'tag': 'v1_kstest', 'noise_shape': (19,), 'fix_delphes_outputs': False, 'nepochs_dump_pred_metrics': 250, 'loss_mll_weight': 0.0001, 'do_batch_normalization_gen': False, 'add_invmass_disc': False, 'width_gen': 0, 'gen_learning_rate': False, 'output_shape': (19,), 'noise_type': 1, 'do_skip_connection': False, 'nepochs_dump_models': 500, 'input_file': '/home/users/bhashemi/Projects/GIT/DY-GAN/delphes/total_Zmumu_13TeV_PU20_v2.npa', 'args': {'width_disc': 0, 'ntest_samples': 10000, 'output_size': 19, 'terminate_early': False, 'do_batch_normalization_disc': False, 'use_delphes': True, 'use_ptetaphi_additionally': False, 'do_noisy_labels': False, 'tag': 'v1_kstest', 'fix_delphes_outputs': False, 'nepochs_dump_pred_metrics': 250, 'loss_mll_weight': 0.0001, 'do_batch_normalization_gen': False, 'add_invmass_disc': False, 'width_gen': 0, 'do_skip_connection': False, 'noise_type': 1, 'loss_type': 'ks_test', 'nepochs_dump_models': 500, 'input_file': '/home/users/bhashemi/Projects/GIT/DY-GAN/delphes/total_Zmumu_13TeV_PU20_v2.npa', 'scaler_type': '', 'batch_size': 512, 'do_concatenate_disc': False, 'do_soft_labels': False, 'depth_gen': 0, 'noise_size': 19, 'optimizer_disc': 'adadelta', 'nepochs_max': 100001, 'beefy_discriminator': True, 'depth_disc': 0, 'do_tanh_gen': False, 'nepochs_decay_noisy_labels': 2000, 'use_mll_loss': True, 'nepochs_dump_plots': 500, 'beefy_generator': True, 'optimizer_gen': 'adadelta', 'do_concatenate_gen': False}, 'batch_size': 512, 'disc_learning_rate': False, 'do_concatenate_disc': False, 'do_soft_labels': False, 'gen_adam_beta1': False, 'depth_gen': 0, 'optimizer_disc': 'adadelta', 'nepochs_max': 100001, 'beefy_discriminator': True, 'depth_disc': 0, 'do_tanh_gen': False, 'disc_adam_beta1': False, 'do_concatenate_gen': False, 'mnist_test': False, 'use_mll_loss': True, 'nepochs_dump_plots': 500, 'nepochs_decay_noisy_labels': 2000, 'optimizer_gen': 'adadelta'}\n",
      "['log.txt', 'gan_skip_2.ipynb', 'history.pkl', 'plots_000500.png', 'disc_500.weights', 'gen_500.weights', 'plots_001000.png', 'disc_1000.weights', 'gen_1000.weights']\n",
      "Compiling Generator\n",
      "Generator params: 340883\n",
      "Compiling Discriminator\n",
      "Discriminator params: 161793\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Expected float32, got 'disc' of type 'str' instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-218-8981389f869b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m \u001b[0mgan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;31m#plot_model(model, to_file='/nfs-7/userdata/bhashemi/DY-GAN/models/progress/%s/model.png' % params[\"tag\"], show_shapes=True, show_layer_names=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-212-41bb9de528d6>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Compiling Discriminator\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscriminator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_discriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"disc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer_d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;31m# Make the combined model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, **kwargs)\u001b[0m\n\u001b[1;32m    828\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m                     output_loss = weighted_loss(y_true, y_pred,\n\u001b[0;32m--> 830\u001b[0;31m                                                 sample_weight, mask)\n\u001b[0m\u001b[1;32m    831\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics_tensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mweighted\u001b[0;34m(y_true, y_pred, weights, mask)\u001b[0m\n\u001b[1;32m    427\u001b[0m         \"\"\"\n\u001b[1;32m    428\u001b[0m         \u001b[0;31m# score_array has ndim >= 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m         \u001b[0mscore_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0;31m# Cast the mask to floatX to avoid float64 upcasting in Theano\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-210-02af89c4ed48>\u001b[0m in \u001b[0;36mloss_func\u001b[0;34m(y_true, y_pred_mll)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m#         mll_loss = K.mean((mll_pred - pseudomll)**2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mbinary_crossentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmll_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mloss_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"force_z_width\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.pyc\u001b[0m in \u001b[0;36mr_binary_op_wrapper\u001b[0;34m(y, x)\u001b[0m\n\u001b[1;32m    907\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mr_binary_op_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 909\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"x\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    910\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, preferred_dtype)\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 836\u001b[0;31m       as_ref=False)\n\u001b[0m\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, ctx)\u001b[0m\n\u001b[1;32m    924\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/constant_op.pyc\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    227\u001b[0m                                          as_ref=False):\n\u001b[1;32m    228\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/constant_op.pyc\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name, verify_shape)\u001b[0m\n\u001b[1;32m    206\u001b[0m   tensor_value.tensor.CopyFrom(\n\u001b[1;32m    207\u001b[0m       tensor_util.make_tensor_proto(\n\u001b[0;32m--> 208\u001b[0;31m           value, dtype=dtype, shape=shape, verify_shape=verify_shape))\n\u001b[0m\u001b[1;32m    209\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m   const_tensor = g.create_op(\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_util.pyc\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[0;34m(values, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    381\u001b[0m       \u001b[0mnparray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp_dt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m       \u001b[0m_AssertCompatible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m       \u001b[0mnparray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp_dt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m       \u001b[0;31m# check to them.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_util.pyc\u001b[0m in \u001b[0;36m_AssertCompatible\u001b[0;34m(values, dtype)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m       raise TypeError(\"Expected %s, got %s of type '%s' instead.\" %\n\u001b[0;32m--> 303\u001b[0;31m                       (dtype.name, repr(mismatch), type(mismatch).__name__))\n\u001b[0m\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected float32, got 'disc' of type 'str' instead."
     ]
    }
   ],
   "source": [
    "# defaults\n",
    "params = {\n",
    "        \"input_file\": \"data_xyz.npy\",\n",
    "        \"output_size\": 8,\n",
    "        \"noise_size\": 8,\n",
    "        \"noise_type\": 1,\n",
    "        \"ntest_samples\": 10000,\n",
    "        #\"nepochs_dump_pred_metrics\": 2,    \n",
    "        \"nepochs_dump_pred_metrics\": 250,\n",
    "        \"nepochs_dump_plots\": 500,\n",
    "        \"nepochs_dump_models\": 5000,\n",
    "        \"nepochs_max\": 100001,\n",
    "        \"batch_size\": 200,\n",
    "        \"do_concatenate_disc\": False,\n",
    "        \"do_concatenate_gen\": False,\n",
    "        \"do_batch_normalization_disc\": False,\n",
    "        \"do_batch_normalization_gen\": False,\n",
    "        \"do_soft_labels\": False,\n",
    "        \"do_noisy_labels\": False,\n",
    "        \"do_tanh_gen\": False,\n",
    "        \"nepochs_decay_noisy_labels\": 3000,\n",
    "        \"use_ptetaphi_additionally\": False,\n",
    "        \"scaler_type\": \"\",\n",
    "        \"optimizer_disc\": \"adadelta\",\n",
    "        \"optimizer_gen\": \"adadelta\",\n",
    "        \"beefy_generator\": False,\n",
    "        \"beefy_discriminator\": False,\n",
    "        \"depth_gen\": 0,\n",
    "        \"width_gen\": 0,\n",
    "        \"depth_disc\": 0,\n",
    "        \"width_disc\": 0,\n",
    "        \"add_invmass_disc\": False,\n",
    "        \"fix_delphes_outputs\": True,\n",
    "        \"use_delphes\": False,\n",
    "        \"use_mll_loss\": True,\n",
    "        \"loss_mll_weight\": 0.0001,\n",
    "        \"do_skip_connection\": False,\n",
    "        \"terminate_early\": True,\n",
    "        \"loss_type\": \"force_mll\"\n",
    "        }\n",
    "\n",
    "# for delphes:\n",
    "params.update({\n",
    "    \"use_delphes\": True,\n",
    "    #\"fix_delphes_outputs\": True,\n",
    "    \"fix_delphes_outputs\": False,\n",
    "    \"do_soft_labels\": False,\n",
    "    \"do_noisy_labels\": False,\n",
    "    \"nepochs_decay_noisy_labels\": 2000,\n",
    "    \"input_file\": \"/home/users/bhashemi/Projects/GIT/DY-GAN/delphes/total_Zmumu_13TeV_PU20_v2.npa\",\n",
    "    \"output_size\": 19,\n",
    "})\n",
    "params.update({\n",
    "    \"noise_type\": 1,\n",
    "    \"noise_size\": 19, # 19 for the true events and 8 more for noise\n",
    "    #\"use_mll_loss\": True,\n",
    "    #\"loss_mll_weight\": 0.01,\n",
    "    \"nepochs_max\": 100001,\n",
    "    #\"nepochs_max\": 1001,\n",
    "    \"batch_size\": 512,\n",
    "    \"do_skip_connection\": False,\n",
    "    \"terminate_early\": False,\n",
    "    \"nepochs_dump_models\": 500,\n",
    "    \"beefy_generator\": True,\n",
    "    \"beefy_discriminator\": True,\n",
    "    #\"loss_type\": \"force_mll\",\n",
    "    #\"loss_type\": \"force_z_width\",\n",
    "    \"loss_type\": \"ks_test\",\n",
    "    #\"depth_gen\": 10,\n",
    "    #\"width_gen\": 10000,\n",
    "    #\"depth_disc\": 10,\n",
    "    #\"width_disc\": 10000,\n",
    "    #\"nepochs_dump_plots\": 1,\n",
    "    #\"nepochs_dump_models\": 1,\n",
    "    #\"nepochs_max\": 10,\n",
    "    #\"optimizer_disc\": \"adam\",\n",
    "    #\"optimizer_gen\": \"adam\",\n",
    "    #\"gen_adam_beta1\": 0.5,\n",
    "    #\"gen_learning_rate\": 0.0003,\n",
    "    #\"disc_adam_beta1\": 0.5,\n",
    "    #\"disc_learning_rate\": 0.0003,\n",
    "    #\"mnist_test\": False,\n",
    "})\n",
    "\n",
    "# change tag for provenance\n",
    "params[\"tag\"] = \"v1_kstest\"\n",
    "# params[\"tag\"] = \"mnist_test3_knownarch\"\n",
    "\n",
    "print params\n",
    "gan = GAN(**params)\n",
    "\n",
    "#plot_model(model, to_file='/nfs-7/userdata/bhashemi/DY-GAN/models/progress/%s/model.png' % params[\"tag\"], show_shapes=True, show_layer_names=True)\n",
    "#gan.discriminator.summary()\n",
    "#gan.generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected concatenate_10 to have shape (2,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-219-09fd49980d8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#for i in xrange(89,100):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \"\"\"for i in xrange(1,11):\n\u001b[1;32m      5\u001b[0m     \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tag\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"v%d_batch512_bgbd_liam1\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-207-367715db2af2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgw_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgw_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0md_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_imgs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mhalf_batch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzeros\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m             \u001b[0;31m#d_loss_real = self.discriminator.train_on_batch(imgs, ones)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;31m#print(\"Real Disc loss: %s \" % str(d_loss_real[0]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1875\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1876\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1877\u001b[0;31m             class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1878\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muses_learning_phase\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m   1478\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1479\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1480\u001b[0;31m                                     exception_prefix='target')\n\u001b[0m\u001b[1;32m   1481\u001b[0m         sample_weights = _standardize_sample_weights(sample_weight,\n\u001b[1;32m   1482\u001b[0m                                                      self._feed_output_names)\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    121\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    124\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected concatenate_10 to have shape (2,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "gan.train()\n",
    "\n",
    "#for i in xrange(89,100):\n",
    "\"\"\"for i in xrange(1,11):\n",
    "    params[\"tag\"] = \"v%d_batch512_bgbd_liam1\" % i\n",
    "    gan = GAN(**params)\n",
    "    gan.train()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "a = os.popen(\"mkdir -p /nfs-7/userdata/bhashemi/DY-GAN/models/progress/{}/\".format(\"testdir\"))\n",
    "print(a.read())\n",
    "os.listdir(\"/nfs-7/userdata/bhashemi/DY-GAN/models/progress/{}/\".format(\"testdir\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot masses\n",
    "plt.plot(gan.d_epochinfo[\"mass_sig\"])\n",
    "plt.plot(gan.d_epochinfo[\"mass_mu\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot losses\n",
    "plt.plot(gan.d_epochinfo[\"d_loss\"])\n",
    "plt.plot(gan.d_epochinfo[\"g_loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get noise, predict from it, and plot stuff. easy.\n",
    "\n",
    "# You must load the config of the model you want into the gan first by running the block of code with the\n",
    "# proper config settings or the loss function will be messed up here.\n",
    "\n",
    "#tag = gan.tag\n",
    "tag = \"v2_512_bgbd_mllANDwidth_NonTC\"\n",
    "gan.load_data()\n",
    "epoch=52000\n",
    "\n",
    "print(\"/nfs-7/userdata/bhashemi/DY-GAN/models/progress/%s/gen_%d.weights\" % (tag,epoch))\n",
    "model = load_model(\"/nfs-7/userdata/bhashemi/DY-GAN/models/progress/%s/gen_%d.weights\" % (tag,epoch), custom_objects={'loss_func': custom_loss(c=\"\", loss_type=gan.loss_type)})\n",
    "_, noise = gan.get_noise(50000)\n",
    "# print noise\n",
    "# print noise.shape\n",
    "# print noise\n",
    "preds = model.predict(noise)\n",
    "print (preds-noise[:,0:19]).mean(axis=0)\n",
    "cov_pred = np.cov(preds.T)\n",
    "cov_real = np.cov(noise[:,0:19].T)\n",
    "cov_diff = (cov_pred - cov_real)\n",
    "print(cov_real[2,2], cov_pred[2,2], cov_diff[2,2])\n",
    "print(cov_diff.shape)\n",
    "print(cov_real)\n",
    "print(cov_pred)\n",
    "print(cov_diff)\n",
    "# make_plots(noise,gan.data[:5000],title=\"epoch {}\".format(3000))\n",
    "make_plots(preds,gan.data[:5000],title=\"epoch {}\".format(3000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mfake = Minv(preds,nopy2=True)\n",
    "mreal = Minv(noise[:,0:19],nopy2=True)\n",
    "mreal = mreal[np.isfinite(mreal)]\n",
    "mfake = mfake[np.isfinite(mfake)]\n",
    "print mreal.mean(), mreal.std()\n",
    "print mfake.mean(), mfake.std()\n",
    "#print (mreal-mfake)[:100]\n",
    "print(np.mean(noise[:,0]))\n",
    "#_ = plt.hist(mreal-mfake,bins=np.linspace(-50,50,100))\n",
    "\n",
    "_ = plt.hist((preds-noise[:,0:19])[:,18],bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## I need to add a step that computes the covariance matrix elements between the variables, but for some reason I'm\n",
    "## struggling to understand how to write the function.\n",
    "import itertools\n",
    "\n",
    "mean = [0, 0]\n",
    "cov = [[4, 2], [2, 5]]  # diagonal covariance\n",
    "\n",
    "x, y = np.random.multivariate_normal(mean, cov, 5000).T\n",
    "#z = np.random.multivariate_normal(mean, cov, 5000).T\n",
    "\n",
    "#print(x,y)\n",
    "\n",
    "def get_covariance(row1, row2):\n",
    "    mean1 = np.mean(row1)\n",
    "    mean2 = np.mean(row2)\n",
    "    #print(\"row1: %s, \\n mean1: %d, sum1: %d \\n var1 = %d \" % (row1, mean1, np.mean(row1), np.sum((row1 - mean1)*(row1 - mean1)) ) )\n",
    "    #print(\"row2: %s, \\n mean2: %d, sum2: %d \\n var2 = %d \" % (row2, mean2, np.mean(row2), np.sum(row2 - mean2)) )\n",
    "    return (np.mean((row1 - mean1)*(row2-mean2)))\n",
    "\n",
    "print(get_covariance(x,y))\n",
    "np.cov(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(0,100,50)\n",
    "_ = plt.hist(gan.data[\"met\"][gan.data[\"nvtxs\"] < 18],bins=bins,histtype=\"step\", density=True, label=\"low PU\")\n",
    "_ = plt.hist(gan.data[\"met\"][gan.data[\"nvtxs\"] > 28],bins=bins,histtype=\"step\", density=True, label=\"high PU\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(0,100,50)\n",
    "_ = plt.hist(preds[:,12][preds[:,7] < 18],bins=bins,histtype=\"step\", density=True, label=\"low PU\")\n",
    "_ = plt.hist(preds[:,12][preds[:,7] > 28],bins=bins,histtype=\"step\", density=True, label=\"high PU\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

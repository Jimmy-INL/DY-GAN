{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import tensorflow\n",
      "import keras\n",
      "import matplotlib\n",
      "import sklearn\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "\"\"\"This version of the file is just for cooking up new ideas. gan_skip.ipynb is the master file.\"\"\"\n",
    "\n",
    "filename=\"gan_skip_2\" #important for bookkeeping since ipython can't use __file__\n",
    "\n",
    "import os\n",
    "# running with non gpu singularity container, so commented out the next line to use CPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(42)\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "print \"import tensorflow\"\n",
    "           \n",
    "import keras.backend.tensorflow_backend as K\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Dense, Dropout, Flatten, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D, LeakyReLU, Lambda\n",
    "from keras.layers import Input, merge, Concatenate, concatenate, Add\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.utils import plot_model\n",
    "print \"import keras\"\n",
    "\n",
    "import numpy as np\n",
    "#from tqdm import tqdm\n",
    "import time\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "print \"import matplotlib\"\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from scipy.stats import binned_statistic_2d\n",
    "\n",
    "print \"import sklearn\"\n",
    "\n",
    "np.random.seed(42)\n",
    "cov_hash = None\n",
    "cov_ans = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "def onetime(func):\n",
    "    \"\"\"stores the functions output, returns the output if called again on the same input, else computes new output\"\"\"\n",
    "    def decorated(*args, **kwargs):\n",
    "        global cov_ans\n",
    "        global cov_hash\n",
    "        new_hash=hashlib.md5(str(args)+str(kwargs)).hexdigest() \n",
    "        if new_hash != cov_hash:\n",
    "            #print(\"computing\")\n",
    "            cov_ans = func(*args, **kwargs)\n",
    "        cov_hash=new_hash\n",
    "        return cov_ans\n",
    "    return decorated\n",
    "    \n",
    "\n",
    "@onetime\n",
    "def covariance_metrics(real_data, predictions):\n",
    "    \"\"\"Takes in real_data matrix with real entries as rows and predictions matrix with generated events as rows and returns the covariance matricies for the two as well as the average, maximum, and std. dev of the difference between the entries in the coverance matrix as well as in the average of the variables.\"\"\"\n",
    "    \n",
    "    cov_pred = np.cov(predictions.T)\n",
    "    avg_pred = predictions.mean(axis=0)\n",
    "    cov_real = np.cov(real_data.T)\n",
    "    avg_real = real_data.mean(axis=0)\n",
    "    \n",
    "    #cov_diff = np.abs((cov_pred - cov_real)/np.sqrt(np.abs(np.outer(avg_real, avg_pred))))\n",
    "    cov_diff = np.abs((cov_pred - cov_real)/cov_real)\n",
    "    ar=avg_real\n",
    "    ar[ar == 0] = 1\n",
    "    avg_diff = np.abs((avg_pred - avg_real)/ar)\n",
    "    \n",
    "    return cov_diff, avg_diff\n",
    "\n",
    "\n",
    "def get_score(real_data, predictions, weight_cov = (1/361.), weight_avg = (1/19.)):\n",
    "    cov_diff, avg_diff = covariance_metrics(real_data, predictions)\n",
    "    return weight_cov*np.sum(cov_diff)+weight_avg*np.sum(avg_diff)\n",
    "\n",
    "def getKS(real_data, predictions):\n",
    "    return ks_2samp(Minv(real_data,ptetaphi=False,nopy2=True), Minv(predictions,ptetaphi=False,nopy2=True))\n",
    "#a = np.matrix([[1,2,3],[4,5,6],[7,8,9]])\n",
    "#ap = np.matrix([[2,2,4],[4,5,6],[7,8,9]])\n",
    "#b = np.matrix([[-1,-2,-3],[-4,-5,-6],[-7,-8,-9]])\n",
    "\n",
    "\n",
    "#print(get_score(a,ap))\n",
    "#print(covariance_metrics(a,b))\n",
    "#print(covariance_metrics(a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Minv(cols,ptetaphi=False,nopy2=False):\n",
    "    \"\"\"\n",
    "    Computes M for two objects given the cartesian momentum projections\n",
    "    if `ptetaphi` is True, then assumes the 8 input columns are cylindrical eptetaphi\n",
    "    if `nopy2` is True, input is 7 columns with no py2\n",
    "    \"\"\"\n",
    "    if ptetaphi:\n",
    "        cols = ptetaphi_to_cartesian(cols)\n",
    "    if nopy2:\n",
    "        M2 = (cols[:,0]+cols[:,4])**2\n",
    "        M2 -= (cols[:,1]+cols[:,5])**2\n",
    "        M2 -= (cols[:,2]          )**2\n",
    "        M2 -= (cols[:,3]+cols[:,6])**2\n",
    "    else:\n",
    "        M2 = (cols[:,0]+cols[:,4])**2\n",
    "        M2 -= (cols[:,1]+cols[:,5])**2\n",
    "        M2 -= (cols[:,2]+cols[:,6])**2\n",
    "        M2 -= (cols[:,3]+cols[:,7])**2\n",
    "    return np.sqrt(M2)\n",
    "\n",
    "def cartesian_to_ptetaphi(eight_cartesian_cols):\n",
    "    \"\"\"\n",
    "    Takes 8 columns as cartesian e px py pz e px py pz\n",
    "    and converts to e pt eta phi e pt eta phi\n",
    "    \"\"\"\n",
    "    e1 =  eight_cartesian_cols[:,0]\n",
    "    e2 =  eight_cartesian_cols[:,4]\n",
    "    px1 = eight_cartesian_cols[:,1]\n",
    "    px2 = eight_cartesian_cols[:,5]\n",
    "    py1 = eight_cartesian_cols[:,2]\n",
    "    py2 = eight_cartesian_cols[:,6]\n",
    "    pz1 = eight_cartesian_cols[:,3]\n",
    "    pz2 = eight_cartesian_cols[:,7]\n",
    "    p1 = np.sqrt(px1**2+py1**2+pz1**2)\n",
    "    p2 = np.sqrt(px2**2+py2**2+pz2**2)\n",
    "    pt1 = np.sqrt(px1**2+py1**2)\n",
    "    pt2 = np.sqrt(px2**2+py2**2)\n",
    "    phi1 = np.arctan2(py1,px1)\n",
    "    phi2 = np.arctan2(py2,px2)\n",
    "    eta1 = np.arctanh(pz1/p1)\n",
    "    eta2 = np.arctanh(pz2/p2)\n",
    "    return np.c_[e1,pt1,eta1,phi1,e2,pt2,eta2,phi2]\n",
    "\n",
    "def ptetaphi_to_cartesian(eight_eptetaphi_cols):\n",
    "    \"\"\"\n",
    "    Takes 8 columns as e pt eta phi e pt eta phi\n",
    "    and converts to e px py pz e px py pz\n",
    "    \"\"\"\n",
    "    e1 =  eight_eptetaphi_cols[:,0]\n",
    "    e2 =  eight_eptetaphi_cols[:,4]\n",
    "    pt1 =  eight_eptetaphi_cols[:,1]\n",
    "    pt2 =  eight_eptetaphi_cols[:,5]\n",
    "    eta1 =  eight_eptetaphi_cols[:,2]\n",
    "    eta2 =  eight_eptetaphi_cols[:,6]\n",
    "    phi1 =  eight_eptetaphi_cols[:,3]\n",
    "    phi2 =  eight_eptetaphi_cols[:,7]\n",
    "    px1 = np.abs(pt1)*np.cos(phi1)\n",
    "    px2 = np.abs(pt2)*np.cos(phi2)\n",
    "    py1 = np.abs(pt1)*np.sin(phi1)\n",
    "    py2 = np.abs(pt2)*np.sin(phi2)\n",
    "    pz1 = np.abs(pt1)/np.tan(2.0*np.arctan(np.exp(-1.*eta1)))\n",
    "    pz2 = np.abs(pt2)/np.tan(2.0*np.arctan(np.exp(-1.*eta2)))\n",
    "    return np.c_[e1,px1,py1,pz1,e2,px2,py2,pz2]\n",
    "\n",
    "def get_dphi(px1,py1,px2,py2):\n",
    "    phi1 = np.arctan2(py1,px1)\n",
    "    phi2 = np.arctan2(py2,px2)\n",
    "    dphi = phi1-phi2\n",
    "    dphi[dphi>np.pi] -= 2*np.pi\n",
    "    dphi[dphi<-np.pi] += 2*np.pi \n",
    "    return dphi\n",
    "\n",
    "def get_rotated_pxpy(px1,py1,px2,py2):\n",
    "    \"\"\"\n",
    "    rotates two leptons such that phi2 = 0\n",
    "    \"\"\"\n",
    "    pt1 = np.sqrt(px1**2+py1**2)\n",
    "    pt2 = np.sqrt(px2**2+py2**2)\n",
    "    phi1 = np.arctan2(py1,px1)\n",
    "    phi2 = np.arctan2(py2,px2)\n",
    "    px1p = pt1*np.cos(phi1-phi2)\n",
    "    py1p = pt1*np.sin(phi1-phi2)\n",
    "    px2p = pt2*np.cos(phi2-phi2)\n",
    "    return px1p,py1p,px2p,np.zeros(len(pt2))\n",
    "    \n",
    "def cartesian_zerophi2(coords,ptetaphi=False):\n",
    "    \"\"\"\n",
    "    returns 8-1=7 columns rotating leptons such that phi2 is 0 (and removing it)\n",
    "    if `ptetaphi` is True, then return eptetaphi instead of epxpypz\n",
    "    \"\"\"\n",
    "    lepcoords_cyl = cartesian_to_ptetaphi(coords)\n",
    "    phi1 = lepcoords_cyl[:,3]\n",
    "    phi2 = lepcoords_cyl[:,7]\n",
    "    dphi = phi1-phi2\n",
    "    dphi[dphi>np.pi] -= 2*np.pi\n",
    "    dphi[dphi<-np.pi] += 2*np.pi\n",
    "    lepcoords_cyl[:,3] = dphi\n",
    "    lepcoords_cyl[:,7] = 0.\n",
    "    if ptetaphi:\n",
    "        return np.delete(lepcoords_cyl, [7], axis=1)\n",
    "    else:\n",
    "        return np.delete(ptetaphi_to_cartesian(lepcoords_cyl), [6], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invmass_from_8cartesian(x):\n",
    "    invmass = K.sqrt(\n",
    "                (x[:,0:1]+x[:,4:5])**2-\n",
    "                (x[:,1:2]+x[:,5:6])**2-\n",
    "                (x[:,2:3]+x[:,6:7])**2-\n",
    "                (x[:,3:4]+x[:,7:8])**2\n",
    "                )\n",
    "    return invmass\n",
    "\n",
    "def invmass_from_8cartesian_nopy2(x):\n",
    "    invmass = K.sqrt(\n",
    "                (x[:,0:1]+x[:,4:5])**2-\n",
    "                (x[:,1:2]+x[:,5:6])**2-\n",
    "                (x[:,2:3]         )**2-\n",
    "                (x[:,3:4]+x[:,6:7])**2\n",
    "                )\n",
    "    return invmass\n",
    "\n",
    "def get_first_N(x,N=19):\n",
    "    return x[:,0:N]\n",
    "\n",
    "def add_invmass_from_8cartesian(x):\n",
    "    return K.concatenate([x,invmass_from_8cartesian(x)])\n",
    "\n",
    "\n",
    "def fix_outputs(x):\n",
    "    \"\"\"\n",
    "    Take nominal delphes format of 19 columns and fix some columns\n",
    "    \"\"\"\n",
    "    return K.concatenate([\n",
    "        # x[:,0:21],\n",
    "        x[:,0:7], # epxpypz for lep1,lep2 -1 for no py2\n",
    "        x[:,7:8], # nvtx\n",
    "        K.sign(x[:,8:10]), # q1 q2\n",
    "        x[:,10:12], # iso1 iso2\n",
    "        x[:,12:14], # met, metphi\n",
    "        x[:,14:19], # jet pts\n",
    "        ])\n",
    "\n",
    "def custom_loss(c, loss_type = \"force_mll\"):\n",
    "    if loss_type == \"force_mll\":\n",
    "        def loss_func(y_true, y_pred_mll):\n",
    "            y_true = y_true[:,0]\n",
    "            y_pred = y_pred_mll[:,0]\n",
    "            mll_pred = y_pred_mll[:,1]\n",
    "\n",
    "            mll_loss = K.mean(K.abs(mll_pred - 91.2))\n",
    "\n",
    "    #         pseudomll = K.random_normal_variable(shape=(1,1), mean=91.2, scale=2)\n",
    "    #         mll_loss = K.mean((mll_pred - pseudomll)**2)\n",
    "\n",
    "            return binary_crossentropy(y_true, y_pred) + c*mll_loss\n",
    "        return loss_func\n",
    "    elif loss_type == \"force_z_width\":\n",
    "        def loss_func(y_true, y_pred_mll):\n",
    "            y_true = y_true[:,0]\n",
    "            y_pred = y_pred_mll[:,0]\n",
    "            mll_pred = y_pred_mll[:,1]\n",
    "            \n",
    "            mll_loss = K.mean(K.abs(mll_pred - 91.2))\n",
    "            mll_sigma_loss = K.abs(K.std(mll_pred) - 7.67)\n",
    "\n",
    "            return binary_crossentropy(y_true, y_pred) + c*mll_loss + c*mll_sigma_loss\n",
    "        return loss_func\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(\"Can not make loss function of type %s\" % loss_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def make_plots(preds,reals,title=\"\",fname=\"\"):\\n    nrows, ncols = 5,5\\n    fig, axs = plt.subplots(nrows,ncols,figsize=(16,13))\\n    fig.subplots_adjust(wspace=0.1,hspace=0.3)\\n\\n\\n    #print(preds)\\n    info = [\\n        [\"mll\",(60,120,50)],\\n        [\"lep1_e\",(0,250,50)],\\n        [\"lep1_px\",(-100,100,50)],\\n        [\"lep1_py\",(-100,100,50)],\\n        [\"lep1_pz\",(-200,200,50)],\\n        [\"lep2_e\",(0,250,50)],\\n        [\"lep2_px\",(-100,100,50)],\\n        [\"lep2_pz\",(-200,200,50)],\\n        [\"dphi\",(-4,4,50)],\\n        [\"nvtxs\",(0,50,350)],\\n        [\"met\",(0,150,50)],\\n        [\"metphi\",(-6,6,50)],\\n        [\"lep1_charge\",(-7,7,30)],\\n        [\"lep2_charge\",(-7,7,30)],\\n        [\"lep1_iso\",(0,0.2,30)],\\n        [\"lep2_iso\",(0,0.2,30)],\\n        [\"genjet_pt1\",(0,100,50)],\\n        [\"genjet_pt2\",(0,100,50)],\\n        [\"genjet_pt3\",(0,100,50)],\\n        [\"genjet_pt4\",(0,100,50)],\\n        [\"genjet_pt5\",(0,100,50)],\\n\\n    ]\\n    for ic,(cname,crange) in enumerate(info):\\n        if cname == \"mll\":\\n            real = reals[\"mll\"]\\n            pred = Minv(preds,ptetaphi=False,nopy2=True)\\n        elif cname == \"lep1_e\": real, pred = reals[cname], preds[:,0]\\n        elif cname == \"lep1_pz\": real, pred = reals[cname], preds[:,3]\\n        elif cname == \"lep2_e\": real, pred = reals[cname], preds[:,4]\\n        elif cname == \"lep2_pz\": real, pred = reals[cname], preds[:,6]\\n        elif cname == \"lep1_px\": \\n            real = get_rotated_pxpy(reals[\"lep1_px\"], reals[\"lep1_py\"], reals[\"lep2_px\"], reals[\"lep2_py\"])[0]\\n            pred = preds[:,1]\\n        elif cname == \"lep1_py\":\\n            real = get_rotated_pxpy(reals[\"lep1_px\"], reals[\"lep1_py\"], reals[\"lep2_px\"], reals[\"lep2_py\"])[1]\\n            pred = preds[:,2]\\n        elif cname == \"lep2_px\":\\n            real = get_rotated_pxpy(reals[\"lep1_px\"], reals[\"lep1_py\"], reals[\"lep2_px\"], reals[\"lep2_py\"])[2]\\n            pred = preds[:,5]\\n        elif cname == \"dphi\":\\n            real = get_dphi(reals[\"lep1_px\"], reals[\"lep1_py\"], reals[\"lep2_px\"], reals[\"lep2_py\"])\\n            pred = get_dphi(preds[:,1], preds[:,2], preds[:,5], np.zeros(len(preds)))\\n        elif cname == \"nvtxs\": real, pred = reals[cname], np.round(preds[:,7])\\n        elif cname == \"lep1_charge\": real, pred = reals[cname], preds[:,8]\\n        elif cname == \"lep2_charge\": real, pred = reals[cname], preds[:,9]\\n        elif cname == \"lep1_iso\": real, pred = reals[cname], preds[:,10]\\n        elif cname == \"lep2_iso\": real, pred = reals[cname], preds[:,11]\\n        elif cname == \"met\": real, pred = reals[cname], preds[:,12]\\n        elif cname == \"metphi\": real, pred = reals[cname], METPhiMap(preds[:,13])\\n        elif cname == \"genjet_pt1\": real, pred = reals[cname], preds[:,14]\\n        elif cname == \"genjet_pt2\": real, pred = reals[cname], preds[:,15]\\n        elif cname == \"genjet_pt3\": real, pred = reals[cname], preds[:,16]\\n        elif cname == \"genjet_pt4\": real, pred = reals[cname], preds[:,17]\\n        elif cname == \"genjet_pt5\": real, pred = reals[cname], preds[:,18]\\n        idx = ic // ncols, ic % ncols\\n        bins_real = axs[idx].hist(real, range=crange[:2],bins=crange[-1], histtype=\"step\", lw=2,density=True)\\n        bins_pred = axs[idx].hist(pred, range=crange[:2],bins=crange[-1], histtype=\"step\", lw=2,density=True)\\n        axs[idx].set_xlabel(\"{}\".format(cname))\\n        axs[idx].get_yaxis().set_visible(False)\\n    #     axs[idx].set_yscale(\"log\", nonposy=\\'clip\\')\\n    _ = axs[0,0].legend([\"True\",\"Pred\"], loc=\\'upper right\\')\\n    _ = axs[0,0].set_title(title)\\n    plt.tight_layout()\\n    if fname:\\n        fig.savefig(fname)'"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def METPhiMap(metphis):\n",
    "    \"\"\"Maps number line to period boundary conditions between [-pi,pi]\"\"\"\n",
    "    #Add or subtract the proper number of factors of 2pi. If number is between [-3pi, -pi] add 2pi,\n",
    "    #if between [-5pi, -3pi], add 4pi. For positive intervals subtract instead of add. To get the \n",
    "    #number of 2pis to subtract or add, take the floor of the abs of the number over pi, that gives\n",
    "    #the integer number of pis away from 0. Subtract 2pi for every 2pi if you are greater than pi,\n",
    "    #and subtract another for every 2pi greater than pi. The same holds in reverse for negative values.\n",
    "    return metphis - np.sign(metphis)*np.ceil(np.floor(np.abs(metphis)/(np.pi))/2)*2*np.pi\n",
    "\n",
    "def make_plots(preds,reals,title=\"\",fname=\"\",show_pred=True,wspace=0.1,hspace=0.3,tightlayout=True,visible=False):\n",
    "    nrows, ncols = 5,5\n",
    "    fig, axs = plt.subplots(nrows,ncols,figsize=(16,13))\n",
    "#     fig, axs = plt.subplots(nrows,ncols,figsize=(12,10))\n",
    "#     fig.subplots_adjust(wspace=0.1,hspace=0.3)\n",
    "    fig.subplots_adjust(wspace=wspace,hspace=hspace)\n",
    "\n",
    "\n",
    "    info = [\n",
    "        [\"mll\",(60,120,50)],\n",
    "        [\"lep1_e\",(0,250,50)],\n",
    "        [\"lep1_px\",(-100,100,50)],\n",
    "        [\"lep1_py\",(-100,100,50)],\n",
    "        [\"lep1_pz\",(-200,200,50)],\n",
    "        [\"lep2_e\",(0,250,50)],\n",
    "        [\"lep2_px\",(-100,100,50)],\n",
    "        [\"lep2_pz\",(-200,200,50)],\n",
    "        [\"dphi\",(-4,4,50)],\n",
    "        [\"nvtxs\",(0,50,350)],\n",
    "        [\"met\",(0,150,50)],\n",
    "        [\"metphi\",(-6,6,50)],\n",
    "        [\"lep1_charge\",(-7,7,30)],\n",
    "        [\"lep2_charge\",(-7,7,30)],\n",
    "        [\"lep1_iso\",(0,2.0,30)],\n",
    "        [\"lep2_iso\",(0,2.0,30)],\n",
    "        [\"jet_pt1\",(0,100,50)],\n",
    "        [\"jet_pt2\",(0,100,50)],\n",
    "        [\"jet_pt3\",(0,100,50)],\n",
    "        [\"jet_pt4\",(0,100,50)],\n",
    "        [\"jet_pt5\",(0,100,50)],\n",
    "        [\"njets\",(0,7,7)],\n",
    "\n",
    "    ]\n",
    "    for axx in axs:\n",
    "        for ax in axx:\n",
    "            ax.get_yaxis().set_visible(False)\n",
    "    for ic,(cname,crange) in enumerate(info):\n",
    "        if cname == \"mll\":\n",
    "            real = reals[\"mll\"]\n",
    "            pred = Minv(preds,ptetaphi=False,nopy2=True)\n",
    "        elif cname == \"lep1_e\": real, pred = reals[cname], preds[:,0]\n",
    "        elif cname == \"lep1_pz\": real, pred = reals[cname], preds[:,3]\n",
    "        elif cname == \"lep2_e\": real, pred = reals[cname], preds[:,4]\n",
    "        elif cname == \"lep2_pz\": real, pred = reals[cname], preds[:,6]\n",
    "        elif cname == \"lep1_px\": \n",
    "            real = reals[cname]\n",
    "            pred = preds[:,1]\n",
    "        elif cname == \"lep1_py\":\n",
    "            real = reals[cname]\n",
    "            pred = preds[:,2]\n",
    "        elif cname == \"lep2_px\":\n",
    "            real = reals[cname]\n",
    "            pred = preds[:,5]\n",
    "        elif cname == \"dphi\":\n",
    "            real = get_dphi(reals[\"lep1_px\"], reals[\"lep1_py\"], reals[\"lep2_px\"], np.zeros(len(reals)))\n",
    "            pred = get_dphi(preds[:,1], preds[:,2], preds[:,5], np.zeros(len(preds)))\n",
    "        elif cname == \"nvtxs\": real, pred = reals[cname], np.round(preds[:,7])\n",
    "        elif cname == \"lep1_charge\": real, pred = reals[cname], preds[:,8]\n",
    "        elif cname == \"lep2_charge\": real, pred = reals[cname], preds[:,9]\n",
    "        elif cname == \"lep1_iso\": real, pred = reals[cname], preds[:,10]\n",
    "        elif cname == \"lep2_iso\": real, pred = reals[cname], preds[:,11]\n",
    "        elif cname == \"met\": real, pred = reals[cname], preds[:,12]\n",
    "        elif cname == \"metphi\": real, pred = reals[cname], METPhiMap(preds[:,13])\n",
    "        elif cname == \"jet_pt1\": real, pred = reals[cname], preds[:,14]\n",
    "        elif cname == \"jet_pt2\": real, pred = reals[cname], preds[:,15]\n",
    "        elif cname == \"jet_pt3\": real, pred = reals[cname], preds[:,16]\n",
    "        elif cname == \"jet_pt4\": real, pred = reals[cname], preds[:,17]\n",
    "        elif cname == \"jet_pt5\": real, pred = reals[cname], preds[:,18]\n",
    "        elif cname == \"njets\":\n",
    "            real = \\\n",
    "                1*(reals[\"jet_pt1\"] > 10) + \\\n",
    "                1*(reals[\"jet_pt2\"] > 10) + \\\n",
    "                1*(reals[\"jet_pt3\"] > 10) + \\\n",
    "                1*(reals[\"jet_pt4\"] > 10) + \\\n",
    "                1*(reals[\"jet_pt5\"] > 10)\n",
    "            pred = \\\n",
    "                1*(preds[:,14] > 10) + \\\n",
    "                1*(preds[:,15] > 10) + \\\n",
    "                1*(preds[:,16] > 10) + \\\n",
    "                1*(preds[:,17] > 10) + \\\n",
    "                1*(preds[:,18] > 10)\n",
    "        idx = ic // ncols, ic % ncols\n",
    "        bins_real = axs[idx].hist(real, range=crange[:2],bins=crange[-1], histtype=\"step\", lw=1.5,density=True)\n",
    "        if show_pred:\n",
    "            bins_pred = axs[idx].hist(pred, range=crange[:2],bins=crange[-1], histtype=\"step\", lw=1.5,density=True)\n",
    "        axs[idx].set_xlabel(\"{}\".format(cname))\n",
    "        axs[idx].get_yaxis().set_visible(False)\n",
    "    #     axs[idx].set_yscale(\"log\", nonposy='clip')\n",
    "    _ = axs[0,0].legend([\"True\",\"Pred\"], loc='upper right')\n",
    "    _ = axs[0,0].set_title(title)\n",
    "    if tightlayout:\n",
    "        plt.tight_layout()\n",
    "    if fname:\n",
    "        fig.savefig(fname)\n",
    "    if not visible:\n",
    "        plt.close(fig)\n",
    "\n",
    "\"\"\"def make_plots(preds,reals,title=\"\",fname=\"\"):\n",
    "    nrows, ncols = 5,5\n",
    "    fig, axs = plt.subplots(nrows,ncols,figsize=(16,13))\n",
    "    fig.subplots_adjust(wspace=0.1,hspace=0.3)\n",
    "\n",
    "\n",
    "    #print(preds)\n",
    "    info = [\n",
    "        [\"mll\",(60,120,50)],\n",
    "        [\"lep1_e\",(0,250,50)],\n",
    "        [\"lep1_px\",(-100,100,50)],\n",
    "        [\"lep1_py\",(-100,100,50)],\n",
    "        [\"lep1_pz\",(-200,200,50)],\n",
    "        [\"lep2_e\",(0,250,50)],\n",
    "        [\"lep2_px\",(-100,100,50)],\n",
    "        [\"lep2_pz\",(-200,200,50)],\n",
    "        [\"dphi\",(-4,4,50)],\n",
    "        [\"nvtxs\",(0,50,350)],\n",
    "        [\"met\",(0,150,50)],\n",
    "        [\"metphi\",(-6,6,50)],\n",
    "        [\"lep1_charge\",(-7,7,30)],\n",
    "        [\"lep2_charge\",(-7,7,30)],\n",
    "        [\"lep1_iso\",(0,0.2,30)],\n",
    "        [\"lep2_iso\",(0,0.2,30)],\n",
    "        [\"genjet_pt1\",(0,100,50)],\n",
    "        [\"genjet_pt2\",(0,100,50)],\n",
    "        [\"genjet_pt3\",(0,100,50)],\n",
    "        [\"genjet_pt4\",(0,100,50)],\n",
    "        [\"genjet_pt5\",(0,100,50)],\n",
    "\n",
    "    ]\n",
    "    for ic,(cname,crange) in enumerate(info):\n",
    "        if cname == \"mll\":\n",
    "            real = reals[\"mll\"]\n",
    "            pred = Minv(preds,ptetaphi=False,nopy2=True)\n",
    "        elif cname == \"lep1_e\": real, pred = reals[cname], preds[:,0]\n",
    "        elif cname == \"lep1_pz\": real, pred = reals[cname], preds[:,3]\n",
    "        elif cname == \"lep2_e\": real, pred = reals[cname], preds[:,4]\n",
    "        elif cname == \"lep2_pz\": real, pred = reals[cname], preds[:,6]\n",
    "        elif cname == \"lep1_px\": \n",
    "            real = get_rotated_pxpy(reals[\"lep1_px\"], reals[\"lep1_py\"], reals[\"lep2_px\"], reals[\"lep2_py\"])[0]\n",
    "            pred = preds[:,1]\n",
    "        elif cname == \"lep1_py\":\n",
    "            real = get_rotated_pxpy(reals[\"lep1_px\"], reals[\"lep1_py\"], reals[\"lep2_px\"], reals[\"lep2_py\"])[1]\n",
    "            pred = preds[:,2]\n",
    "        elif cname == \"lep2_px\":\n",
    "            real = get_rotated_pxpy(reals[\"lep1_px\"], reals[\"lep1_py\"], reals[\"lep2_px\"], reals[\"lep2_py\"])[2]\n",
    "            pred = preds[:,5]\n",
    "        elif cname == \"dphi\":\n",
    "            real = get_dphi(reals[\"lep1_px\"], reals[\"lep1_py\"], reals[\"lep2_px\"], reals[\"lep2_py\"])\n",
    "            pred = get_dphi(preds[:,1], preds[:,2], preds[:,5], np.zeros(len(preds)))\n",
    "        elif cname == \"nvtxs\": real, pred = reals[cname], np.round(preds[:,7])\n",
    "        elif cname == \"lep1_charge\": real, pred = reals[cname], preds[:,8]\n",
    "        elif cname == \"lep2_charge\": real, pred = reals[cname], preds[:,9]\n",
    "        elif cname == \"lep1_iso\": real, pred = reals[cname], preds[:,10]\n",
    "        elif cname == \"lep2_iso\": real, pred = reals[cname], preds[:,11]\n",
    "        elif cname == \"met\": real, pred = reals[cname], preds[:,12]\n",
    "        elif cname == \"metphi\": real, pred = reals[cname], METPhiMap(preds[:,13])\n",
    "        elif cname == \"genjet_pt1\": real, pred = reals[cname], preds[:,14]\n",
    "        elif cname == \"genjet_pt2\": real, pred = reals[cname], preds[:,15]\n",
    "        elif cname == \"genjet_pt3\": real, pred = reals[cname], preds[:,16]\n",
    "        elif cname == \"genjet_pt4\": real, pred = reals[cname], preds[:,17]\n",
    "        elif cname == \"genjet_pt5\": real, pred = reals[cname], preds[:,18]\n",
    "        idx = ic // ncols, ic % ncols\n",
    "        bins_real = axs[idx].hist(real, range=crange[:2],bins=crange[-1], histtype=\"step\", lw=2,density=True)\n",
    "        bins_pred = axs[idx].hist(pred, range=crange[:2],bins=crange[-1], histtype=\"step\", lw=2,density=True)\n",
    "        axs[idx].set_xlabel(\"{}\".format(cname))\n",
    "        axs[idx].get_yaxis().set_visible(False)\n",
    "    #     axs[idx].set_yscale(\"log\", nonposy='clip')\n",
    "    _ = axs[0,0].legend([\"True\",\"Pred\"], loc='upper right')\n",
    "    _ = axs[0,0].set_title(title)\n",
    "    plt.tight_layout()\n",
    "    if fname:\n",
    "        fig.savefig(fname)\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN():\n",
    "    def __init__(self, **kwargs):\n",
    "\n",
    "        self.args = dict(kwargs)\n",
    "\n",
    "        self.tag = kwargs[\"tag\"]\n",
    "        self.input_file = str(kwargs[\"input_file\"])\n",
    "        self.noise_shape = (int(kwargs[\"noise_size\"]),)\n",
    "        self.output_shape = (int(kwargs[\"output_size\"]),)\n",
    "        self.noise_type = int(kwargs[\"noise_type\"])\n",
    "        self.ntest_samples = int(kwargs[\"ntest_samples\"])\n",
    "        self.nepochs_dump_pred_metrics = int(kwargs[\"nepochs_dump_pred_metrics\"])\n",
    "        self.nepochs_dump_models = int(kwargs[\"nepochs_dump_models\"])\n",
    "        self.nepochs_dump_plots = int(kwargs[\"nepochs_dump_plots\"])\n",
    "        self.nepochs_max = int(kwargs[\"nepochs_max\"])\n",
    "        self.batch_size = int(kwargs[\"batch_size\"])\n",
    "        self.do_concatenate_disc = kwargs[\"do_concatenate_disc\"]\n",
    "        self.do_concatenate_gen = kwargs[\"do_concatenate_gen\"]\n",
    "        self.do_batch_normalization_disc = kwargs[\"do_batch_normalization_disc\"]\n",
    "        self.do_batch_normalization_gen = kwargs[\"do_batch_normalization_gen\"]\n",
    "        self.do_soft_labels = kwargs[\"do_soft_labels\"]\n",
    "        self.do_noisy_labels = kwargs[\"do_noisy_labels\"]\n",
    "        self.do_tanh_gen = kwargs[\"do_tanh_gen\"]\n",
    "        self.nepochs_decay_noisy_labels = int(kwargs[\"nepochs_decay_noisy_labels\"])\n",
    "        self.use_ptetaphi_additionally = kwargs[\"use_ptetaphi_additionally\"]\n",
    "        self.optimizer_gen = kwargs[\"optimizer_gen\"]\n",
    "        self.optimizer_disc = kwargs[\"optimizer_disc\"]\n",
    "        self.depth_disc = kwargs[\"depth_disc\"]\n",
    "        self.width_disc = kwargs[\"width_disc\"]\n",
    "        self.depth_gen = kwargs[\"depth_gen\"]\n",
    "        self.width_gen = kwargs[\"width_gen\"]\n",
    "        self.beefy_generator = kwargs[\"beefy_generator\"]\n",
    "        self.beefy_discriminator = kwargs[\"beefy_discriminator\"]\n",
    "        self.add_invmass_disc = kwargs[\"add_invmass_disc\"]\n",
    "        self.fix_delphes_outputs = kwargs[\"fix_delphes_outputs\"]\n",
    "        self.use_delphes = kwargs[\"use_delphes\"]\n",
    "        self.use_mll_loss = kwargs[\"use_mll_loss\"]\n",
    "        self.loss_mll_weight = kwargs[\"loss_mll_weight\"]\n",
    "        self.do_skip_connection = kwargs[\"do_skip_connection\"]\n",
    "        self.terminate_early = kwargs[\"terminate_early\"]\n",
    "        self.loss_type = kwargs[\"loss_type\"]\n",
    "        if self.use_ptetaphi_additionally: self.output_shape = (self.output_shape[0]+8,)\n",
    "            \n",
    "        print(self.__dict__)\n",
    "\n",
    "        os.system(\"mkdir -p progress/{}/\".format(self.tag))\n",
    "        os.system(\"cp {} progress/{}/\".format(filename, self.tag))\n",
    "\n",
    "        self.scaler_type = kwargs[\"scaler_type\"]\n",
    "        self.scaler = None\n",
    "        if self.scaler_type.lower() == \"minmax\":\n",
    "            self.scaler = MinMaxScaler(feature_range=(-1.,1.))\n",
    "        elif self.scaler_type.lower() == \"robust\":\n",
    "            self.scaler = RobustScaler()\n",
    "        elif self.scaler_type.lower() == \"standard\":\n",
    "            self.scaler = StandardScaler()\n",
    "\n",
    "        self.data = None\n",
    "        self.data_ref = None\n",
    "        self.d_epochinfo = {}\n",
    "        self.X_train = None\n",
    "\n",
    "        # optimizer = Adam(0.0002, 0.5)\n",
    "        optimizer_d = self.optimizer_disc\n",
    "        # optimizer_d = \"sgd\"\n",
    "        optimizer_g = self.optimizer_gen\n",
    "        # optimizer_g = \"adam\"\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        if self.use_mll_loss:\n",
    "            loss = custom_loss(c=self.loss_mll_weight, loss_type=self.loss_type)\n",
    "        else:\n",
    "            loss = \"binary_crossentropy\"\n",
    "            \n",
    "        self.loss=loss\n",
    "            \n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss=loss,\n",
    "            optimizer=optimizer_d,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build and compile the generator\n",
    "        self.generator = self.build_generator()\n",
    "        self.generator.compile(loss=loss, optimizer=optimizer_g)\n",
    "\n",
    "        # The generator takes noise as input and generated imgs\n",
    "        z = Input(shape=self.noise_shape)\n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The valid takes generated images as input and determines validity\n",
    "        valid = self.discriminator(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator) takes\n",
    "        # noise as input => generates images => determines validity\n",
    "        self.combined = Model(z, valid)\n",
    "        self.combined.compile(loss=loss, optimizer=optimizer_g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(GAN):\n",
    "    \n",
    "    def build_generator(self):\n",
    "\n",
    "        inputs = Input(shape=self.noise_shape)\n",
    "\n",
    "        ## Head\n",
    "        x = Dense(64)(inputs)\n",
    "        if self.do_batch_normalization_gen:\n",
    "            x = BatchNormalization()(x)\n",
    "        x = LeakyReLU(alpha=0.2)(x)\n",
    "        if self.do_concatenate_gen:\n",
    "            x = Lambda(lambda x: K.concatenate([x*x,x]))(x)\n",
    "            x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "        ## Main Body\n",
    "        if self.depth_gen > 0 and self.width_gen > 0:\n",
    "            for level in xrange(0,self.depth_gen):\n",
    "                size=self.width_gen/(2**level)\n",
    "                if(size<self.output_shape[0]):\n",
    "                    raise ValueError(\"The layer size %d would be smaller than the output size, make sure you have a wide enough network to deal with %s layers\" % (size, self.depth_gen))\n",
    "                x = Dense(size)(x) #Triangle with width halved at each level\n",
    "                x = LeakyReLU(alpha=0.2)(x)\n",
    "        elif self.beefy_generator:\n",
    "            for size in [128,256,512,256,128]:\n",
    "                x = Dense(size)(x)\n",
    "                x = LeakyReLU(alpha=0.2)(x)\n",
    "        else:\n",
    "            for size in [128,128,128,64,32]:\n",
    "                x = Dense(size)(x)\n",
    "                x = LeakyReLU(alpha=0.2)(x)\n",
    " \n",
    "        ## Tail\n",
    "        x = Dense(self.output_shape[0])(x)\n",
    "        \n",
    "#         if False:\n",
    "        if self.do_skip_connection:\n",
    "            # get the non-noise part of the input, and add it to the tail\n",
    "            y = Lambda(get_first_N, arguments={'N': self.output_shape[0]})(inputs)\n",
    "#             print y\n",
    "            x = Add()([x,y])\n",
    "#             print x\n",
    "            \n",
    "        if self.do_tanh_gen:\n",
    "            x = Activation(\"tanh\")(x)\n",
    "        elif self.fix_delphes_outputs:\n",
    "            x = Lambda(fix_outputs,\n",
    "                input_shape=self.output_shape,\n",
    "                output_shape=self.output_shape\n",
    "                )(x)\n",
    "            \n",
    "#         model = Model(inputs=inputs, outputs=concatenate([out,mll]))\n",
    "        model = Model(inputs=inputs, outputs=[x])\n",
    "        \n",
    "        print \"Generator params: {}\".format(model.count_params())\n",
    "#         model.summary()\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(GAN):\n",
    "    \n",
    "    def build_discriminator(self):\n",
    "\n",
    "        inputs = Input(self.output_shape)\n",
    "        mll = Lambda(invmass_from_8cartesian_nopy2)(inputs)\n",
    "        x = Dense(128)(inputs)\n",
    "        if self.do_batch_normalization_disc:\n",
    "            x = BatchNormalization()(x)\n",
    "        x = LeakyReLU(alpha=0.2)(x)\n",
    "        if self.do_concatenate_disc:\n",
    "            x = Lambda(lambda x: K.concatenate([x*x,x]))(x)\n",
    "            x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "        ## Main Body\n",
    "        if self.depth_disc > 0 and self.width_disc > 0:\n",
    "            for level in xrange(0,self.depth_disc):\n",
    "                x = Dense(self.width_disc/(2**level))(x) #Triangle with width halved at each level\n",
    "                x = LeakyReLU(alpha=0.2)(x)\n",
    "        elif self.beefy_generator:\n",
    "            for size in [128,256,256,128,64,32,16,8]:\n",
    "                x = Dense(size)(x)\n",
    "                x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "        else:\n",
    "            for size in [128]*5 + [64,32,16,8]:\n",
    "                x = Dense(size)(x)\n",
    "                x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "        ## Tail\n",
    "        out = Dense(1,activation='sigmoid')(x)\n",
    "        \n",
    "        if self.use_mll_loss:\n",
    "            model = Model(inputs=inputs, outputs=concatenate([out,mll]))\n",
    "        else:\n",
    "            model = Model(inputs=inputs, outputs=out)\n",
    "#         print model.output_shape\n",
    "#         model.summary()\n",
    "        print \"Discriminator params: {}\".format(model.count_params())\n",
    "        \n",
    "        return model\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(GAN):\n",
    "    \n",
    "    def load_data(self):\n",
    "        if self.data is not None: return\n",
    "        \n",
    "        if self.use_delphes:\n",
    "            self.data = np.load(self.input_file)\n",
    "        else:\n",
    "            self.data = np.load(self.input_file)\n",
    "            \n",
    "        self.data = self.data[self.data[\"genmll\"] > 50.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(GAN):\n",
    "\n",
    "    def get_noise(self, amount=1024):\n",
    "        # nominal\n",
    "        if self.noise_type == 1:\n",
    "            noise_half = np.random.normal(0, 1, (amount//2, self.noise_shape[0]))\n",
    "            noise_full = np.random.normal(0, 1, (amount, self.noise_shape[0]))\n",
    "\n",
    "        elif self.noise_type == 2: # random soup, 4,2,2 have to be modified to sum to noise_shape[0]\n",
    "            ngaus = self.noise_shape[0] // 2\n",
    "            nflat = (self.noise_shape[0] - ngaus) // 2\n",
    "            nexpo = self.noise_shape[0] - nflat - ngaus\n",
    "            noise_gaus = np.random.normal( 0, 1, (amount//2+amount, ngaus))\n",
    "            noise_flat = np.random.uniform(-1, 1, (amount//2+amount, nflat))\n",
    "            noise_expo = np.random.exponential( 1,    (amount//2+amount, nexpo))\n",
    "            noise = np.c_[ noise_gaus,noise_flat,noise_expo ]\n",
    "            noise_half = noise[:amount//2]\n",
    "            noise_full = noise[-amount:]\n",
    "            \n",
    "        elif self.noise_type == 3: # truth conditioned\n",
    "            \n",
    "#             noise_half = np.c_[ \n",
    "#                     self.X_train[np.random.randint(0, self.X_train.shape[0], amount//2)], \n",
    "#                     np.random.normal(0, 1, (amount//2,self.noise_shape[0]-self.X_train.shape[1]))\n",
    "#                     ]\n",
    "#             noise_full = np.c_[ \n",
    "#                     self.X_train[np.random.randint(0, self.X_train.shape[0], amount)], \n",
    "#                     np.random.normal(0, 1, (amount,self.noise_shape[0]-self.X_train.shape[1]))\n",
    "#                     ]\n",
    "            \n",
    "            npurenoise = self.noise_shape[0]-self.X_train.shape[1]\n",
    "            ngaus = npurenoise // 2\n",
    "            nflat = (npurenoise - ngaus) // 2\n",
    "            nexpo = npurenoise - nflat - ngaus\n",
    "            noise_gaus = np.random.normal( 0, 1, (amount//2+amount, ngaus))\n",
    "            noise_flat = np.random.uniform(-1, 1, (amount//2+amount, nflat))\n",
    "            noise_expo = np.random.exponential( 1,    (amount//2+amount, nexpo))\n",
    "            truenoise = self.X_train[np.random.randint(0, self.X_train.shape[0], amount//2+amount)]\n",
    "            noise = np.c_[ truenoise,noise_gaus,noise_flat,noise_expo ]\n",
    "            noise_half = noise[:amount//2]\n",
    "            noise_full = noise[-amount:]\n",
    "\n",
    "        return noise_half, noise_full\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(GAN):\n",
    "            \n",
    "    def train(self):\n",
    "\n",
    "        self.load_data()\n",
    "        \n",
    "        if self.use_delphes:\n",
    "            lepcoords = np.c_[\n",
    "                self.data[\"lep1_e\"],\n",
    "                self.data[\"lep1_px\"],\n",
    "                self.data[\"lep1_py\"],\n",
    "                self.data[\"lep1_pz\"],\n",
    "                self.data[\"lep2_e\"],\n",
    "                self.data[\"lep2_px\"],\n",
    "#                 self.data[\"lep2_py\"],\n",
    "                self.data[\"lep2_pz\"],\n",
    "            ]\n",
    "#             lepcoords_dphi = cartesian_zerophi2(lepcoords)\n",
    "            \n",
    "            nvtx_smeared = np.round(np.random.normal(self.data[\"nvtxs\"],0.5))\n",
    "            self.X_train = np.c_[\n",
    "#                 lepcoords_dphi, # 7 columns\n",
    "                lepcoords, # 7 columns\n",
    "                nvtx_smeared, # 1 column\n",
    "                self.data[\"lep1_charge\"], self.data[\"lep2_charge\"],\n",
    "                self.data[\"lep1_iso\"], self.data[\"lep2_iso\"],\n",
    "                self.data[\"met\"], self.data[\"metphi\"],\n",
    "                self.data[\"jet_pt1\"],\n",
    "                self.data[\"jet_pt2\"],\n",
    "                self.data[\"jet_pt3\"],\n",
    "                self.data[\"jet_pt4\"],\n",
    "                self.data[\"jet_pt5\"],\n",
    "            ].astype(np.float32)\n",
    "        else:\n",
    "            self.X_train = self.data[:,range(1,1+8)]\n",
    "            if self.use_ptetaphi_additionally:\n",
    "                self.X_train = np.c_[self.X_train, cartesian_to_ptetaphi(self.X_train)]\n",
    "\n",
    "        # # NOTE. StandardScaler should be fit on training set\n",
    "        # # and applied the same to train and test, otherwise we\n",
    "        # # introduce a bias\n",
    "        if self.scaler:\n",
    "            self.scaler.fit(self.X_train)\n",
    "            self.X_train = self.scaler.transform(self.X_train).astype(np.float32)\n",
    "            pickle.dump(self.scaler, open(\"progress/{}/scaler.pkl\".format(self.tag),'w'))\n",
    "\n",
    "        # make an alias to save typing\n",
    "        X_train = self.X_train\n",
    "        \n",
    "        half_batch = int(self.batch_size / 2)\n",
    "\n",
    "        prev_gen_loss = -1\n",
    "        prev_disc_loss = -1\n",
    "        n_loss_same_gen = 0  # number of epochs for which generator loss has remained ~same (within 0.01%)\n",
    "        n_loss_same_disc = 0  # number of epochs for which discriminator loss has remained ~same (within 0.01%)\n",
    "        old_info = -1, -1\n",
    "        for epoch in range(self.nepochs_max):\n",
    "\n",
    "            if self.terminate_early:\n",
    "                if n_loss_same_gen > 1000 or n_loss_same_disc > 1000:\n",
    "                    print \"BREAKING because disc/gen loss has remained the same for {}/{} epochs!\".format(n_loss_same_disc,n_loss_same_gen)\n",
    "                    break\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random half batch of images\n",
    "            idx = np.random.randint(0, X_train.shape[0], half_batch)\n",
    "            imgs = X_train[idx]\n",
    "            \n",
    "            noise_half, noise_full = self.get_noise(self.batch_size)\n",
    "            \n",
    "            # Generate a half batch of new images\n",
    "            gen_imgs = self.generator.predict(noise_half)\n",
    "\n",
    "            # Train the discriminator\n",
    "            ones = np.ones((half_batch, 1))\n",
    "            zeros = np.zeros((half_batch, 1))\n",
    "\n",
    "            if self.do_soft_labels:\n",
    "                ones *= 0.9\n",
    "\n",
    "            if self.do_noisy_labels:\n",
    "                frac = 0.3*np.exp(-epoch/self.nepochs_decay_noisy_labels)\n",
    "                if frac > 0.005:\n",
    "                    ones[np.random.randint(0, len(ones), int(frac*len(ones)))] = 0\n",
    "                    zeros[np.random.randint(0, len(zeros), int(frac*len(zeros)))] = 1\n",
    "\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, ones)\n",
    "            #print(\"Real Disc loss: %s \" % str(d_loss_real[0]))\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, zeros)\n",
    "            #print(\"Fake Disc loss: %s \" % str(d_loss_real[0]))\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "            #print(\"Avg Disc loss: %s \" % str(d_loss[0]))\n",
    "            \n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "            # The generator wants the discriminator to label the generated samples\n",
    "            # as valid (ones)\n",
    "            valid_y = np.array([1] * self.batch_size)\n",
    "\n",
    "            # Train the generator\n",
    "            g_loss = self.combined.train_on_batch(noise_full, valid_y)\n",
    "\n",
    "            if (g_loss - prev_gen_loss) < 0.0001: n_loss_same_gen += 1\n",
    "            else: n_loss_same_gen = 0\n",
    "            prev_gen_loss = g_loss\n",
    "\n",
    "            if (d_loss[0] - prev_disc_loss) < 0.0001: n_loss_same_disc += 1\n",
    "            else: n_loss_same_disc = 0\n",
    "            prev_disc_loss = d_loss[0]\n",
    "\n",
    "            # Plot the progress\n",
    "#             print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "            sys.stdout.write(\"\\r{} [D loss: {}, acc.: {:.2f}%] [G loss: {}] [mll={:.3f}+-{:.3f}]\".format(epoch, d_loss[0], 100.0*d_loss[1], g_loss, old_info[0], old_info[1]))\n",
    "\n",
    "            if epoch % self.nepochs_dump_pred_metrics == 0 and epoch > 0:\n",
    "            \n",
    "                _, noise_test = self.get_noise(self.ntest_samples)\n",
    "            \n",
    "                sys.stdout.write(\"\\n\") # break up the stream of text\n",
    "\n",
    "                gen_imgs = self.generator.predict(noise_test)\n",
    "                \n",
    "                if self.scaler:\n",
    "                    gen_imgs = self.scaler.inverse_transform(gen_imgs)\n",
    "\n",
    "                masses = Minv(gen_imgs,nopy2=True)\n",
    "                masses = masses[np.isfinite(masses)]\n",
    "                old_info = masses.mean(), masses.std()\n",
    "\n",
    "                cov_diff, avg_diff = covariance_metrics(X_train, gen_imgs)\n",
    "                \n",
    "                if \"epoch\" not in self.d_epochinfo:\n",
    "                    self.d_epochinfo[\"epoch\"] = []\n",
    "                    self.d_epochinfo[\"d_acc\"] = []\n",
    "                    self.d_epochinfo[\"d_loss\"] = []\n",
    "                    self.d_epochinfo[\"g_loss\"] = []\n",
    "                    self.d_epochinfo[\"mass_mu\"] = []\n",
    "                    self.d_epochinfo[\"mass_sig\"] = []\n",
    "                    self.d_epochinfo[\"time\"] = []\n",
    "                    self.d_epochinfo[\"avg_cov_diff\"] = []\n",
    "                    self.d_epochinfo[\"max_cov_diff\"] = []\n",
    "                    self.d_epochinfo[\"std_dev_cov_diff\"] = []\n",
    "                    self.d_epochinfo[\"avg_mean_diff\"] = []\n",
    "                    self.d_epochinfo[\"max_mean_diff\"] = []\n",
    "                    self.d_epochinfo[\"std_dev_mean_diff\"] = []\n",
    "                    self.d_epochinfo[\"args\"] = self.args\n",
    "                else:\n",
    "                    self.d_epochinfo[\"epoch\"].append(epoch)\n",
    "                    self.d_epochinfo[\"d_acc\"].append(100*d_loss[1])\n",
    "                    self.d_epochinfo[\"d_loss\"].append(d_loss[0])\n",
    "                    self.d_epochinfo[\"g_loss\"].append(g_loss)\n",
    "                    self.d_epochinfo[\"mass_mu\"].append(masses.mean())\n",
    "                    self.d_epochinfo[\"mass_sig\"].append(masses.std())\n",
    "                    self.d_epochinfo[\"time\"].append(time.time())\n",
    "                    self.d_epochinfo[\"avg_cov_diff\"].append(cov_diff.mean())\n",
    "                    self.d_epochinfo[\"max_cov_diff\"].append(cov_diff.max())\n",
    "                    self.d_epochinfo[\"std_dev_cov_diff\"].append(cov_diff.std())\n",
    "                    self.d_epochinfo[\"avg_mean_diff\"].append(avg_diff.mean())\n",
    "                    self.d_epochinfo[\"max_mean_diff\"].append(avg_diff.max())\n",
    "                    self.d_epochinfo[\"std_dev_mean_diff\"].append(avg_diff.std())\n",
    "                    \n",
    "\n",
    "                pickle.dump(self.d_epochinfo, open(\"progress/{}/history.pkl\".format(self.tag),'w'))\n",
    "\n",
    "            if epoch % self.nepochs_dump_plots == 0 and epoch > 0:\n",
    "                _, noise = self.get_noise(self.ntest_samples)\n",
    "                preds = gan.generator.predict(noise)\n",
    "                reals = self.data[:15000]\n",
    "                _ = make_plots(preds,reals,title=\"{}: epoch {}\".format(self.tag,epoch),\n",
    "                               fname=\"progress/{}/plots_{:06d}.png\".format(self.tag,epoch))\n",
    "            \n",
    "            if epoch % self.nepochs_dump_models == 0 and epoch > 0:\n",
    "                self.discriminator.save(\"progress/{}/disc_{}.weights\".format(self.tag,epoch))\n",
    "                self.generator.save(\"progress/{}/gen_{}.weights\".format(self.tag,epoch))\n",
    "                #self.discriminator.save(\"test.weights\")\n",
    "\n",
    "                print(\"Stats Score: %f\" %  get_score(X_train, gen_imgs))\n",
    "                print(ks_2samp(Minv(X_train,ptetaphi=False,nopy2=True), Minv(gen_imgs,ptetaphi=False,nopy2=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'width_disc': 0, 'ntest_samples': 10000, 'optimizer_disc': 'adadelta', 'output_size': 19, 'terminate_early': False, 'do_batch_normalization_disc': False, 'use_delphes': True, 'nepochs_dump_plots': 500, 'use_ptetaphi_additionally': False, 'do_noisy_labels': False, 'tag': 'v2_batch512_bgbd_mllANDwidth_NonTC_newdata_mllfix', 'nepochs_dump_pred_metrics': 250, 'do_batch_normalization_gen': False, 'add_invmass_disc': False, 'width_gen': 0, 'fix_delphes_outputs': False, 'loss_type': 'force_z_width', 'nepochs_dump_models': 500, 'input_file': '/home/users/bhashemi/Projects/GIT/DY-GAN/delphes/total_Zmumu_13TeV_PU20_v2.npa', 'noise_type': 1, 'scaler_type': '', 'batch_size': 512, 'do_concatenate_disc': False, 'do_soft_labels': False, 'depth_gen': 0, 'noise_size': 19, 'loss_mll_weight': 0.01, 'nepochs_max': 100001, 'beefy_discriminator': True, 'depth_disc': 0, 'do_tanh_gen': False, 'do_skip_connection': False, 'use_mll_loss': True, 'beefy_generator': True, 'nepochs_decay_noisy_labels': 2000, 'optimizer_gen': 'adadelta', 'do_concatenate_gen': False}\n",
      "{'width_disc': 0, 'ntest_samples': 10000, 'beefy_generator': True, 'loss_type': 'force_z_width', 'terminate_early': False, 'do_batch_normalization_disc': False, 'use_delphes': True, 'use_ptetaphi_additionally': False, 'do_noisy_labels': False, 'tag': 'v2_batch512_bgbd_mllANDwidth_NonTC_newdata_mllfix', 'noise_shape': (19,), 'fix_delphes_outputs': False, 'nepochs_dump_pred_metrics': 250, 'loss_mll_weight': 0.01, 'do_batch_normalization_gen': False, 'add_invmass_disc': False, 'width_gen': 0, 'output_shape': (19,), 'noise_type': 1, 'do_skip_connection': False, 'nepochs_dump_models': 500, 'input_file': '/home/users/bhashemi/Projects/GIT/DY-GAN/delphes/total_Zmumu_13TeV_PU20_v2.npa', 'args': {'width_disc': 0, 'ntest_samples': 10000, 'output_size': 19, 'terminate_early': False, 'do_batch_normalization_disc': False, 'use_delphes': True, 'use_ptetaphi_additionally': False, 'do_noisy_labels': False, 'tag': 'v2_batch512_bgbd_mllANDwidth_NonTC_newdata_mllfix', 'fix_delphes_outputs': False, 'nepochs_dump_pred_metrics': 250, 'loss_mll_weight': 0.01, 'do_batch_normalization_gen': False, 'add_invmass_disc': False, 'width_gen': 0, 'do_skip_connection': False, 'noise_type': 1, 'loss_type': 'force_z_width', 'nepochs_dump_models': 500, 'input_file': '/home/users/bhashemi/Projects/GIT/DY-GAN/delphes/total_Zmumu_13TeV_PU20_v2.npa', 'scaler_type': '', 'batch_size': 512, 'do_concatenate_disc': False, 'do_soft_labels': False, 'depth_gen': 0, 'noise_size': 19, 'optimizer_disc': 'adadelta', 'nepochs_max': 100001, 'beefy_discriminator': True, 'depth_disc': 0, 'do_tanh_gen': False, 'nepochs_decay_noisy_labels': 2000, 'use_mll_loss': True, 'nepochs_dump_plots': 500, 'beefy_generator': True, 'optimizer_gen': 'adadelta', 'do_concatenate_gen': False}, 'batch_size': 512, 'do_concatenate_disc': False, 'do_soft_labels': False, 'depth_gen': 0, 'optimizer_disc': 'adadelta', 'nepochs_max': 100001, 'beefy_discriminator': True, 'depth_disc': 0, 'do_tanh_gen': False, 'do_concatenate_gen': False, 'use_mll_loss': True, 'nepochs_dump_plots': 500, 'nepochs_decay_noisy_labels': 2000, 'optimizer_gen': 'adadelta'}\n",
      "Discriminator params: 161793\n",
      "Generator params: 340883\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_64 (InputLayer)           (None, 19)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_358 (Dense)               (None, 128)          2560        input_64[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_316 (LeakyReLU)     (None, 128)          0           dense_358[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_359 (Dense)               (None, 128)          16512       leaky_re_lu_316[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_317 (LeakyReLU)     (None, 128)          0           dense_359[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_360 (Dense)               (None, 256)          33024       leaky_re_lu_317[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_318 (LeakyReLU)     (None, 256)          0           dense_360[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_361 (Dense)               (None, 256)          65792       leaky_re_lu_318[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_319 (LeakyReLU)     (None, 256)          0           dense_361[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_362 (Dense)               (None, 128)          32896       leaky_re_lu_319[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_320 (LeakyReLU)     (None, 128)          0           dense_362[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_363 (Dense)               (None, 64)           8256        leaky_re_lu_320[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_321 (LeakyReLU)     (None, 64)           0           dense_363[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_364 (Dense)               (None, 32)           2080        leaky_re_lu_321[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_322 (LeakyReLU)     (None, 32)           0           dense_364[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_365 (Dense)               (None, 16)           528         leaky_re_lu_322[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_323 (LeakyReLU)     (None, 16)           0           dense_365[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_366 (Dense)               (None, 8)            136         leaky_re_lu_323[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_324 (LeakyReLU)     (None, 8)            0           dense_366[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_367 (Dense)               (None, 1)            9           leaky_re_lu_324[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_22 (Lambda)              (None, 1)            0           input_64[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_22 (Concatenate)    (None, 2)            0           dense_367[0][0]                  \n",
      "                                                                 lambda_22[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 323,586\n",
      "Trainable params: 161,793\n",
      "Non-trainable params: 161,793\n",
      "__________________________________________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_65 (InputLayer)        (None, 19)                0         \n",
      "_________________________________________________________________\n",
      "dense_368 (Dense)            (None, 64)                1280      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_325 (LeakyReLU)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_369 (Dense)            (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_326 (LeakyReLU)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_370 (Dense)            (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_327 (LeakyReLU)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_371 (Dense)            (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_328 (LeakyReLU)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_372 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_329 (LeakyReLU)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_373 (Dense)            (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_330 (LeakyReLU)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_374 (Dense)            (None, 19)                2451      \n",
      "=================================================================\n",
      "Total params: 340,883\n",
      "Trainable params: 340,883\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# defaults\n",
    "params = {\n",
    "        \"input_file\": \"data_xyz.npy\",\n",
    "        \"output_size\": 8,\n",
    "        \"noise_size\": 8,\n",
    "        \"noise_type\": 1,\n",
    "        \"ntest_samples\": 10000,\n",
    "        \"nepochs_dump_pred_metrics\": 250,\n",
    "        \"nepochs_dump_plots\": 500,\n",
    "        \"nepochs_dump_models\": 5000,\n",
    "        \"nepochs_max\": 100001,\n",
    "        \"batch_size\": 200,\n",
    "        \"do_concatenate_disc\": False,\n",
    "        \"do_concatenate_gen\": False,\n",
    "        \"do_batch_normalization_disc\": False,\n",
    "        \"do_batch_normalization_gen\": False,\n",
    "        \"do_soft_labels\": False,\n",
    "        \"do_noisy_labels\": False,\n",
    "        \"do_tanh_gen\": False,\n",
    "        \"nepochs_decay_noisy_labels\": 3000,\n",
    "        \"use_ptetaphi_additionally\": False,\n",
    "        \"scaler_type\": \"\",\n",
    "        \"optimizer_disc\": \"adadelta\",\n",
    "        \"optimizer_gen\": \"adadelta\",\n",
    "        \"beefy_generator\": False,\n",
    "        \"beefy_discriminator\": False,\n",
    "        \"depth_gen\": 0,\n",
    "        \"width_gen\": 0,\n",
    "        \"depth_disc\": 0,\n",
    "        \"width_disc\": 0,\n",
    "        \"add_invmass_disc\": False,\n",
    "        \"fix_delphes_outputs\": True,\n",
    "        \"use_delphes\": False,\n",
    "        \"use_mll_loss\": False,\n",
    "        \"loss_mll_weight\": 0.0001,\n",
    "        \"do_skip_connection\": False,\n",
    "        \"terminate_early\": True,\n",
    "        \"loss_type\": \"force_mll\"\n",
    "        }\n",
    "\n",
    "# for delphes:\n",
    "params.update({\n",
    "    \"use_delphes\": True,\n",
    "    #\"fix_delphes_outputs\": True,\n",
    "    \"fix_delphes_outputs\": False,\n",
    "    \"do_soft_labels\": False,\n",
    "    \"do_noisy_labels\": False,\n",
    "    \"nepochs_decay_noisy_labels\": 2000,\n",
    "    \"input_file\": \"/home/users/bhashemi/Projects/GIT/DY-GAN/delphes/total_Zmumu_13TeV_PU20_v2.npa\",\n",
    "    \"output_size\": 19,\n",
    "})\n",
    "params.update({\n",
    "    \"noise_type\": 1,\n",
    "    \"noise_size\": 19, # 19 for the true events and 8 more for noise\n",
    "    \"use_mll_loss\": True,\n",
    "    \"loss_mll_weight\": 0.01,\n",
    "    \"nepochs_max\": 100001,\n",
    "    \"batch_size\": 512,\n",
    "    \"do_skip_connection\": False,\n",
    "    \"terminate_early\": False,\n",
    "    \"nepochs_dump_models\": 500,\n",
    "    \"beefy_generator\": True,\n",
    "    \"beefy_discriminator\": True,\n",
    "    #\"loss_type\": \"force_mll\",\n",
    "    \"loss_type\": \"force_z_width\",\n",
    "    #\"depth_gen\": 8,\n",
    "    #\"width_gen\": 10000\n",
    "    #\"nepochs_dump_plots\": 1,\n",
    "    #\"nepochs_dump_models\": 1,\n",
    "    #\"nepochs_max\": 10,\n",
    "    \n",
    "})\n",
    "\n",
    "# change tag for provenance\n",
    "# params[\"tag\"] = \"v1_512_bgbd_nomll\"\n",
    "params[\"tag\"] = \"v2_batch512_bgbd_mllANDwidth_NonTC_newdata_mllfix\"\n",
    "\n",
    "print params\n",
    "gan = GAN(**params)\n",
    "\n",
    "#plot_model(model, to_file='progress/%s/model.png' % params[\"tag\"], show_shapes=True, show_layer_names=True)\n",
    "gan.discriminator.summary()\n",
    "gan.generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250 [D loss: 0.291139632463, acc.: 0.00%] [G loss: 5.59598112106] [mll=-1.000+--1.000]]\n",
      "500 [D loss: 0.471023768187, acc.: 13.09%] [G loss: 4.49681091309] [mll=57.928+-9.440]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in sqrt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats Score: 1650.143745\n",
      "Ks_2sampResult(statistic=0.8630215218041601, pvalue=0.0)\n",
      "750 [D loss: 0.380550742149, acc.: 1.17%] [G loss: 3.90091371536] [mll=58.557+-15.874]\n",
      "1000 [D loss: 0.638082683086, acc.: 0.00%] [G loss: 3.11517500877] [mll=76.493+-13.457]\n",
      "Stats Score: 2298.838124\n",
      "Ks_2sampResult(statistic=0.7075617121816807, pvalue=0.0)\n",
      "1250 [D loss: 0.390109419823, acc.: 0.00%] [G loss: 2.35288405418] [mll=100.652+-7.433]]\n",
      "1500 [D loss: 0.317288219929, acc.: 0.00%] [G loss: 2.18847203255] [mll=95.448+-8.088]\n",
      "Stats Score: 6898.866151\n",
      "Ks_2sampResult(statistic=0.8208325171517304, pvalue=0.0)\n",
      "1750 [D loss: 0.756548643112, acc.: 0.00%] [G loss: 2.10182547569] [mll=113.001+-14.478]\n",
      "2000 [D loss: 0.266668528318, acc.: 0.00%] [G loss: 2.08871102333] [mll=83.059+-10.640]\n",
      "Stats Score: 4900.874002\n",
      "Ks_2sampResult(statistic=0.5525932217528449, pvalue=0.0)\n",
      "2250 [D loss: 0.759331524372, acc.: 0.00%] [G loss: 1.35732913017] [mll=84.142+-6.562]\n",
      "2500 [D loss: 0.687050163746, acc.: 0.00%] [G loss: 1.13842439651] [mll=78.687+-9.252]\n",
      "Stats Score: 1584.489938\n",
      "Ks_2sampResult(statistic=0.46455785380232156, pvalue=0.0)\n",
      "2750 [D loss: 0.676330924034, acc.: 0.00%] [G loss: 1.4426920414] [mll=94.654+-11.420]]\n",
      "3000 [D loss: 0.492614656687, acc.: 0.00%] [G loss: 1.88532137871] [mll=97.094+-9.406]\n",
      "Stats Score: 3661.470316\n",
      "Ks_2sampResult(statistic=0.25079571893779656, pvalue=0.0)\n",
      "3250 [D loss: 0.366661190987, acc.: 0.00%] [G loss: 1.82362282276] [mll=90.132+-9.261]\n",
      "3500 [D loss: 0.411478996277, acc.: 0.00%] [G loss: 2.17250442505] [mll=86.046+-7.014]\n",
      "Stats Score: 1895.172972\n",
      "Ks_2sampResult(statistic=0.32063726029529427, pvalue=0.0)\n",
      "3750 [D loss: 0.380159258842, acc.: 0.00%] [G loss: 2.02402615547] [mll=86.036+-8.686]\n",
      "4000 [D loss: 0.304637283087, acc.: 0.00%] [G loss: 2.62784695625] [mll=89.910+-9.702]\n",
      "Stats Score: 2524.275484\n",
      "Ks_2sampResult(statistic=0.23977059533103262, pvalue=0.0)\n",
      "4250 [D loss: 0.699537575245, acc.: 0.00%] [G loss: 2.19926333427] [mll=91.750+-6.405]\n",
      "4500 [D loss: 0.341838359833, acc.: 0.39%] [G loss: 3.49276566505] [mll=93.462+-11.717]\n",
      "Stats Score: 1225.355906\n",
      "Ks_2sampResult(statistic=0.3553556469990248, pvalue=0.0)\n",
      "4750 [D loss: 0.395644754171, acc.: 0.00%] [G loss: 2.20983982086] [mll=96.049+-13.098]\n",
      "5000 [D loss: 0.409209191799, acc.: 0.20%] [G loss: 2.74008917809] [mll=85.687+-12.223]\n",
      "Stats Score: 4615.397758\n",
      "Ks_2sampResult(statistic=0.526532835258267, pvalue=0.0)\n",
      "5250 [D loss: 0.278872609138, acc.: 0.00%] [G loss: 3.13000535965] [mll=102.430+-16.404]\n",
      "5500 [D loss: 0.23037044704, acc.: 0.00%] [G loss: 2.97048735619] [mll=99.340+-11.936]]\n",
      "Stats Score: 3384.150922\n",
      "Ks_2sampResult(statistic=0.2462891680833108, pvalue=0.0)\n",
      "5750 [D loss: 0.459300875664, acc.: 0.00%] [G loss: 3.04648685455] [mll=89.724+-10.787]\n",
      "6000 [D loss: 0.311403870583, acc.: 0.00%] [G loss: 2.61141896248] [mll=89.248+-7.540]\n",
      "Stats Score: 1950.035698\n",
      "Ks_2sampResult(statistic=0.4766090392842116, pvalue=0.0)\n",
      "6250 [D loss: 0.312767237425, acc.: 0.00%] [G loss: 3.2157304287] [mll=96.682+-7.909]]\n",
      "6500 [D loss: 0.228767648339, acc.: 0.00%] [G loss: 3.01833367348] [mll=85.799+-6.218]\n",
      "Stats Score: 2713.834300\n",
      "Ks_2sampResult(statistic=0.10658464941400791, pvalue=9.654392167675743e-99)\n",
      "6750 [D loss: 0.34189414978, acc.: 0.00%] [G loss: 2.9802532196] [mll=91.228+-7.215]5]\n",
      "7000 [D loss: 0.183702349663, acc.: 0.00%] [G loss: 3.3258061409] [mll=92.475+-10.132]]\n",
      "Stats Score: 3595.777311\n",
      "Ks_2sampResult(statistic=0.2221103768881496, pvalue=0.0)\n",
      "7250 [D loss: 0.202072843909, acc.: 0.00%] [G loss: 3.56290102005] [mll=88.748+-5.724]\n",
      "7500 [D loss: 0.162426918745, acc.: 0.00%] [G loss: 3.73603820801] [mll=84.844+-7.887]\n",
      "Stats Score: 1521.065838\n",
      "Ks_2sampResult(statistic=0.2785025630923457, pvalue=0.0)\n",
      "7750 [D loss: 0.281259596348, acc.: 0.00%] [G loss: 3.141071558] [mll=87.236+-6.378]8]\n",
      "8000 [D loss: 0.326162099838, acc.: 0.00%] [G loss: 2.65624260902] [mll=93.720+-7.645]\n",
      "Stats Score: 1978.791282\n",
      "Ks_2sampResult(statistic=0.44567989994722546, pvalue=0.0)\n",
      "8250 [D loss: 0.254684269428, acc.: 0.00%] [G loss: 3.95542168617] [mll=97.206+-9.600]\n",
      "8500 [D loss: 0.376036167145, acc.: 1.17%] [G loss: 3.12948560715] [mll=88.348+-7.245]\n",
      "Stats Score: 3260.471544\n",
      "Ks_2sampResult(statistic=0.44684316301987187, pvalue=0.0)\n",
      "8750 [D loss: 0.273598611355, acc.: 0.39%] [G loss: 3.38920378685] [mll=98.015+-12.329]\n",
      "9000 [D loss: 0.222633972764, acc.: 0.00%] [G loss: 4.23311805725] [mll=95.932+-9.189]\n",
      "Stats Score: 3029.675327\n",
      "Ks_2sampResult(statistic=0.22971183779485044, pvalue=0.0)\n",
      "9250 [D loss: 0.249556273222, acc.: 0.00%] [G loss: 4.53668212891] [mll=89.328+-7.332]\n",
      "9500 [D loss: 0.303685098886, acc.: 0.00%] [G loss: 4.1816649437] [mll=82.663+-6.975]]\n",
      "Stats Score: 2319.551728\n",
      "Ks_2sampResult(statistic=0.1977117847770944, pvalue=0.0)\n",
      "9750 [D loss: 0.189875602722, acc.: 0.20%] [G loss: 3.92842626572] [mll=88.341+-10.691]\n",
      "10000 [D loss: 0.29329636693, acc.: 0.00%] [G loss: 3.09696340561] [mll=100.535+-15.307]\n",
      "Stats Score: 1786.057242\n",
      "Ks_2sampResult(statistic=0.3071020523708421, pvalue=0.0)\n",
      "10250 [D loss: 0.173123419285, acc.: 0.00%] [G loss: 4.2615070343] [mll=93.491+-7.708]]\n",
      "10500 [D loss: 0.231171697378, acc.: 0.00%] [G loss: 3.39766263962] [mll=85.326+-6.158]\n",
      "Stats Score: 2698.062442\n",
      "Ks_2sampResult(statistic=0.3617298268654103, pvalue=0.0)\n",
      "10750 [D loss: 0.290989339352, acc.: 0.00%] [G loss: 3.9524846077] [mll=93.219+-8.346]]\n",
      "11000 [D loss: 0.23733907938, acc.: 0.00%] [G loss: 3.62993049622] [mll=95.128+-10.359]]\n",
      "Stats Score: 2102.764543\n",
      "Ks_2sampResult(statistic=0.26024947748326177, pvalue=0.0)\n",
      "11250 [D loss: 0.267962872982, acc.: 0.00%] [G loss: 3.86984658241] [mll=92.029+-8.379]\n",
      "11500 [D loss: 0.225674375892, acc.: 0.00%] [G loss: 4.08311891556] [mll=95.934+-9.107]\n",
      "Stats Score: 1925.238285\n",
      "Ks_2sampResult(statistic=0.4885000783106305, pvalue=0.0)\n",
      "11750 [D loss: 0.208152323961, acc.: 0.00%] [G loss: 3.92312812805] [mll=82.957+-8.237]\n",
      "12000 [D loss: 0.194894731045, acc.: 0.00%] [G loss: 4.09067201614] [mll=97.637+-9.702]\n",
      "Stats Score: 2527.821657\n",
      "Ks_2sampResult(statistic=0.543204666778539, pvalue=0.0)\n",
      "12250 [D loss: 0.23699760437, acc.: 0.00%] [G loss: 4.25700044632] [mll=80.577+-10.140]]\n",
      "12500 [D loss: 0.199723139405, acc.: 0.00%] [G loss: 3.64695572853] [mll=86.504+-6.534]\n",
      "Stats Score: 5199.452256\n",
      "Ks_2sampResult(statistic=0.21571680735828092, pvalue=0.0)\n",
      "12750 [D loss: 0.255991131067, acc.: 0.00%] [G loss: 3.97363424301] [mll=92.435+-7.885]\n",
      "13000 [D loss: 0.154798030853, acc.: 0.00%] [G loss: 4.12805604935] [mll=97.159+-11.766]\n",
      "Stats Score: 3805.308315\n",
      "Ks_2sampResult(statistic=0.15434406334892248, pvalue=1.3661092604521203e-206)\n",
      "13250 [D loss: 0.422297894955, acc.: 0.00%] [G loss: 4.09576225281] [mll=91.943+-7.760]\n",
      "13500 [D loss: 0.231244817376, acc.: 0.00%] [G loss: 4.19838762283] [mll=92.623+-9.127]\n",
      "Stats Score: 4766.572165\n",
      "Ks_2sampResult(statistic=0.20966613478670082, pvalue=0.0)\n",
      "13750 [D loss: 0.208100751042, acc.: 0.00%] [G loss: 3.85112333298] [mll=92.969+-8.710]\n",
      "14000 [D loss: 0.225343704224, acc.: 0.00%] [G loss: 3.83532905579] [mll=81.811+-10.831]\n",
      "Stats Score: 11123.204574\n",
      "Ks_2sampResult(statistic=0.630557139278618, pvalue=0.0)\n",
      "14250 [D loss: 0.212811261415, acc.: 0.00%] [G loss: 3.62964463234] [mll=81.711+-6.920]]\n",
      "14500 [D loss: 0.214137256145, acc.: 0.00%] [G loss: 3.76539969444] [mll=91.160+-6.730]\n",
      "Stats Score: 9295.151265\n",
      "Ks_2sampResult(statistic=0.509272520021499, pvalue=0.0)\n",
      "14750 [D loss: 0.353177964687, acc.: 0.00%] [G loss: 3.84865903854] [mll=80.983+-9.594]\n",
      "15000 [D loss: 0.163059070706, acc.: 0.00%] [G loss: 4.14513587952] [mll=92.871+-9.594]\n",
      "Stats Score: 3374.705143\n",
      "Ks_2sampResult(statistic=0.13302954013176616, pvalue=1.3982908975386209e-153)\n",
      "15250 [D loss: 0.324356555939, acc.: 0.00%] [G loss: 3.12336564064] [mll=91.341+-8.246]\n",
      "15500 [D loss: 0.152434974909, acc.: 0.00%] [G loss: 4.28403043747] [mll=90.861+-7.882]\n",
      "Stats Score: 5791.254230\n",
      "Ks_2sampResult(statistic=0.3361478047487371, pvalue=0.0)\n",
      "15750 [D loss: 0.225751966238, acc.: 0.00%] [G loss: 3.72607207298] [mll=87.790+-5.286]\n",
      "16000 [D loss: 0.199929118156, acc.: 0.00%] [G loss: 4.23176336288] [mll=88.064+-6.619]\n",
      "Stats Score: 1423.190204\n",
      "Ks_2sampResult(statistic=0.3519774674536642, pvalue=0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16250 [D loss: 0.171397149563, acc.: 0.00%] [G loss: 3.89292907715] [mll=86.508+-6.922]\n",
      "16500 [D loss: 0.203816205263, acc.: 0.20%] [G loss: 4.15406036377] [mll=94.473+-9.994]\n"
     ]
    }
   ],
   "source": [
    "gan.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot masses\n",
    "plt.plot(gan.d_epochinfo[\"mass_sig\"])\n",
    "plt.plot(gan.d_epochinfo[\"mass_mu\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot losses\n",
    "plt.plot(gan.d_epochinfo[\"d_loss\"])\n",
    "plt.plot(gan.d_epochinfo[\"g_loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get noise, predict from it, and plot stuff. easy.\n",
    "\n",
    "# You must load the config of the model you want into the gan first by running the block of code with the\n",
    "# proper config settings or the loss function will be messed up here.\n",
    "\n",
    "#tag = gan.tag\n",
    "tag = \"v2_512_bgbd_mllANDwidth_NonTC\"\n",
    "gan.load_data()\n",
    "epoch=52000\n",
    "\n",
    "print(\"progress/%s/gen_%d.weights\" % (tag,epoch))\n",
    "model = load_model(\"progress/%s/gen_%d.weights\" % (tag,epoch), custom_objects={'loss_func': custom_loss(c=\"\", loss_type=gan.loss_type)})\n",
    "_, noise = gan.get_noise(50000)\n",
    "# print noise\n",
    "# print noise.shape\n",
    "# print noise\n",
    "preds = model.predict(noise)\n",
    "print (preds-noise[:,0:19]).mean(axis=0)\n",
    "cov_pred = np.cov(preds.T)\n",
    "cov_real = np.cov(noise[:,0:19].T)\n",
    "cov_diff = (cov_pred - cov_real)\n",
    "print(cov_real[2,2], cov_pred[2,2], cov_diff[2,2])\n",
    "print(cov_diff.shape)\n",
    "print(cov_real)\n",
    "print(cov_pred)\n",
    "print(cov_diff)\n",
    "# make_plots(noise,gan.data[:5000],title=\"epoch {}\".format(3000))\n",
    "make_plots(preds,gan.data[:5000],title=\"epoch {}\".format(3000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mfake = Minv(preds,nopy2=True)\n",
    "mreal = Minv(noise[:,0:19],nopy2=True)\n",
    "mreal = mreal[np.isfinite(mreal)]\n",
    "mfake = mfake[np.isfinite(mfake)]\n",
    "print mreal.mean(), mreal.std()\n",
    "print mfake.mean(), mfake.std()\n",
    "#print (mreal-mfake)[:100]\n",
    "print(np.mean(noise[:,0]))\n",
    "#_ = plt.hist(mreal-mfake,bins=np.linspace(-50,50,100))\n",
    "\n",
    "_ = plt.hist((preds-noise[:,0:19])[:,18],bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## I need to add a step that computes the covariance matrix elements between the variables, but for some reason I'm\n",
    "## struggling to understand how to write the function.\n",
    "import itertools\n",
    "\n",
    "mean = [0, 0]\n",
    "cov = [[4, 2], [2, 5]]  # diagonal covariance\n",
    "\n",
    "x, y = np.random.multivariate_normal(mean, cov, 5000).T\n",
    "#z = np.random.multivariate_normal(mean, cov, 5000).T\n",
    "\n",
    "#print(x,y)\n",
    "\n",
    "def get_covariance(row1, row2):\n",
    "    mean1 = np.mean(row1)\n",
    "    mean2 = np.mean(row2)\n",
    "    #print(\"row1: %s, \\n mean1: %d, sum1: %d \\n var1 = %d \" % (row1, mean1, np.mean(row1), np.sum((row1 - mean1)*(row1 - mean1)) ) )\n",
    "    #print(\"row2: %s, \\n mean2: %d, sum2: %d \\n var2 = %d \" % (row2, mean2, np.mean(row2), np.sum(row2 - mean2)) )\n",
    "    return (np.mean((row1 - mean1)*(row2-mean2)))\n",
    "\n",
    "print(get_covariance(x,y))\n",
    "np.cov(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(0,100,50)\n",
    "_ = plt.hist(gan.data[\"met\"][gan.data[\"nvtxs\"] < 18],bins=bins,histtype=\"step\", density=True, label=\"low PU\")\n",
    "_ = plt.hist(gan.data[\"met\"][gan.data[\"nvtxs\"] > 28],bins=bins,histtype=\"step\", density=True, label=\"high PU\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(0,100,50)\n",
    "_ = plt.hist(preds[:,12][preds[:,7] < 18],bins=bins,histtype=\"step\", density=True, label=\"low PU\")\n",
    "_ = plt.hist(preds[:,12][preds[:,7] > 28],bins=bins,histtype=\"step\", density=True, label=\"high PU\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

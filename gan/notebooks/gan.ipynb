{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import tensorflow\n",
      "import keras\n",
      "import matplotlib\n",
      "import sklearn\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "# running with non gpu singularity container, so commented out the next line to use CPU\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(42)\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "print \"import tensorflow\"\n",
    "           \n",
    "import keras.backend.tensorflow_backend as K\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Dense, Dropout, Flatten, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D, LeakyReLU, Lambda\n",
    "from keras.layers import Input, merge, Concatenate, concatenate\n",
    "print \"import keras\"\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "print \"import matplotlib\"\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from scipy.stats import binned_statistic_2d\n",
    "\n",
    "print \"import sklearn\"\n",
    "\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Minv(cols,ptetaphi=False,nopy2=False):\n",
    "    \"\"\"\n",
    "    Computes M for two objects given the cartesian momentum projections\n",
    "    if `ptetaphi` is True, then assumes the 8 input columns are cylindrical eptetaphi\n",
    "    if `nopy2` is True, input is 7 columns with no py2\n",
    "    \"\"\"\n",
    "    if ptetaphi:\n",
    "        cols = ptetaphi_to_cartesian(cols)\n",
    "    if nopy2:\n",
    "        M2 = (cols[:,0]+cols[:,4])**2\n",
    "        M2 -= (cols[:,1]+cols[:,5])**2\n",
    "        M2 -= (cols[:,2]          )**2\n",
    "        M2 -= (cols[:,3]+cols[:,6])**2\n",
    "    else:\n",
    "        M2 = (cols[:,0]+cols[:,4])**2\n",
    "        M2 -= (cols[:,1]+cols[:,5])**2\n",
    "        M2 -= (cols[:,2]+cols[:,6])**2\n",
    "        M2 -= (cols[:,3]+cols[:,7])**2\n",
    "    return np.sqrt(M2)\n",
    "\n",
    "def cartesian_to_ptetaphi(eight_cartesian_cols):\n",
    "    \"\"\"\n",
    "    Takes 8 columns as cartesian e px py pz e px py pz\n",
    "    and converts to e pt eta phi e pt eta phi\n",
    "    \"\"\"\n",
    "    e1 =  eight_cartesian_cols[:,0]\n",
    "    e2 =  eight_cartesian_cols[:,4]\n",
    "    px1 = eight_cartesian_cols[:,1]\n",
    "    px2 = eight_cartesian_cols[:,5]\n",
    "    py1 = eight_cartesian_cols[:,2]\n",
    "    py2 = eight_cartesian_cols[:,6]\n",
    "    pz1 = eight_cartesian_cols[:,3]\n",
    "    pz2 = eight_cartesian_cols[:,7]\n",
    "    p1 = np.sqrt(px1**2+py1**2+pz1**2)\n",
    "    p2 = np.sqrt(px2**2+py2**2+pz2**2)\n",
    "    pt1 = np.sqrt(px1**2+py1**2)\n",
    "    pt2 = np.sqrt(px2**2+py2**2)\n",
    "    phi1 = np.arctan2(py1,px1)\n",
    "    phi2 = np.arctan2(py2,px2)\n",
    "    eta1 = np.arctanh(pz1/p1)\n",
    "    eta2 = np.arctanh(pz2/p2)\n",
    "    return np.c_[e1,pt1,eta1,phi1,e2,pt2,eta2,phi2]\n",
    "\n",
    "def ptetaphi_to_cartesian(eight_eptetaphi_cols):\n",
    "    \"\"\"\n",
    "    Takes 8 columns as e pt eta phi e pt eta phi\n",
    "    and converts to e px py pz e px py pz\n",
    "    \"\"\"\n",
    "    e1 =  eight_eptetaphi_cols[:,0]\n",
    "    e2 =  eight_eptetaphi_cols[:,4]\n",
    "    pt1 =  eight_eptetaphi_cols[:,1]\n",
    "    pt2 =  eight_eptetaphi_cols[:,5]\n",
    "    eta1 =  eight_eptetaphi_cols[:,2]\n",
    "    eta2 =  eight_eptetaphi_cols[:,6]\n",
    "    phi1 =  eight_eptetaphi_cols[:,3]\n",
    "    phi2 =  eight_eptetaphi_cols[:,7]\n",
    "    px1 = np.abs(pt1)*np.cos(phi1)\n",
    "    px2 = np.abs(pt2)*np.cos(phi2)\n",
    "    py1 = np.abs(pt1)*np.sin(phi1)\n",
    "    py2 = np.abs(pt2)*np.sin(phi2)\n",
    "    pz1 = np.abs(pt1)/np.tan(2.0*np.arctan(np.exp(-1.*eta1)))\n",
    "    pz2 = np.abs(pt2)/np.tan(2.0*np.arctan(np.exp(-1.*eta2)))\n",
    "    return np.c_[e1,px1,py1,pz1,e2,px2,py2,pz2]\n",
    "\n",
    "def cartesian_zerophi2(coords,ptetaphi=False):\n",
    "    \"\"\"\n",
    "    returns 8-1=7 columns rotating leptons such that phi2 is 0 (and removing it)\n",
    "    if `ptetaphi` is True, then return eptetaphi instead of epxpypz\n",
    "    \"\"\"\n",
    "    lepcoords_cyl = cartesian_to_ptetaphi(lepcoords)\n",
    "    phi1 = lepcoords_cyl[:,3]\n",
    "    phi2 = lepcoords_cyl[:,7]\n",
    "    dphi = phi1-phi2\n",
    "    dphi[dphi>np.pi] -= 2*np.pi\n",
    "    dphi[dphi<-np.pi] += 2*np.pi\n",
    "    lepcoords_cyl[:,3] = dphi\n",
    "    lepcoords_cyl[:,7] = 0.\n",
    "    if ptetaphi:\n",
    "        return np.delete(lepcoords_cyl, [7], axis=1)\n",
    "    else:\n",
    "        return np.delete(ptetaphi_to_cartesian(lepcoords_cyl), [6], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_invmass_from_8cartesian(x):\n",
    "    invmass = K.sqrt(\n",
    "                (x[:,0:1]+x[:,4:5])**2-\n",
    "                (x[:,1:2]+x[:,5:6])**2-\n",
    "                (x[:,2:3]+x[:,6:7])**2-\n",
    "                (x[:,3:4]+x[:,7:8])**2\n",
    "                )\n",
    "    return K.concatenate([x,invmass])\n",
    "\n",
    "def fix_outputs(x):\n",
    "    \"\"\"\n",
    "    Take nominal delphes format of 19 columns and fix some columns\n",
    "    \"\"\"\n",
    "    return K.concatenate([\n",
    "        # x[:,0:21],\n",
    "        x[:,0:7], # epxpypz for lep1,lep2 -1 for no py2\n",
    "        x[:,7:8], # nvtx\n",
    "        K.sign(x[:,8:10]), # q1 q2\n",
    "        x[:,10:12], # iso1 iso2\n",
    "        x[:,12:14], # met, metphi\n",
    "        x[:,14:19], # jet pts\n",
    "#         x[:,0:8], # epxpypz for lep1,lep2\n",
    "#         K.sign(x[:,8:9]), # lep1 charge\n",
    "#         x[:,9:10], # lep1 iso\n",
    "#         K.sign(x[:,10:11]), # lep2 charge\n",
    "#         x[:,11:12], # lep2 iso\n",
    "#         K.round(x[:,12:13]), # nvtxs\n",
    "#         x[:,13:15], # met, metphi\n",
    "#         K.round(x[:,15:16]), # ngenjets\n",
    "#         x[:,16:21], # jet pts\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN():\n",
    "    def __init__(self, **kwargs):\n",
    "\n",
    "        self.args = dict(kwargs)\n",
    "\n",
    "        self.tag = kwargs[\"tag\"]\n",
    "        self.input_file = str(kwargs[\"input_file\"])\n",
    "        self.noise_shape = (int(kwargs[\"noise_size\"]),)\n",
    "        self.output_shape = (int(kwargs[\"output_size\"]),)\n",
    "        self.noise_type = int(kwargs[\"noise_type\"])\n",
    "        self.ntest_samples = int(kwargs[\"ntest_samples\"])\n",
    "        self.nepochs_dump_pred_metrics = int(kwargs[\"nepochs_dump_pred_metrics\"])\n",
    "        self.nepochs_dump_models = int(kwargs[\"nepochs_dump_models\"])\n",
    "        self.nepochs_max = int(kwargs[\"nepochs_max\"])\n",
    "        self.batch_size = int(kwargs[\"batch_size\"])\n",
    "        self.do_concatenate_disc = kwargs[\"do_concatenate_disc\"]\n",
    "        self.do_concatenate_gen = kwargs[\"do_concatenate_gen\"]\n",
    "        self.do_batch_normalization_disc = kwargs[\"do_batch_normalization_disc\"]\n",
    "        self.do_batch_normalization_gen = kwargs[\"do_batch_normalization_gen\"]\n",
    "        self.do_soft_labels = kwargs[\"do_soft_labels\"]\n",
    "        self.do_noisy_labels = kwargs[\"do_noisy_labels\"]\n",
    "        self.do_tanh_gen = kwargs[\"do_tanh_gen\"]\n",
    "        self.nepochs_decay_noisy_labels = int(kwargs[\"nepochs_decay_noisy_labels\"])\n",
    "        self.use_ptetaphi_additionally = kwargs[\"use_ptetaphi_additionally\"]\n",
    "        self.optimizer_gen = kwargs[\"optimizer_gen\"]\n",
    "        self.optimizer_disc = kwargs[\"optimizer_disc\"]\n",
    "        self.depth_disc = kwargs[\"depth_disc\"]\n",
    "        self.width_disc = kwargs[\"width_disc\"]\n",
    "        self.depth_gen = kwargs[\"depth_gen\"]\n",
    "        self.width_gen = kwargs[\"width_gen\"]\n",
    "        self.beefy_generator = kwargs[\"beefy_generator\"]\n",
    "        self.add_invmass_disc = kwargs[\"add_invmass_disc\"]\n",
    "        self.fix_delphes_outputs = kwargs[\"fix_delphes_outputs\"]\n",
    "        self.use_delphes = kwargs[\"use_delphes\"]\n",
    "        if self.use_ptetaphi_additionally: self.output_shape = (self.output_shape[0]+8,)\n",
    "\n",
    "        os.system(\"mkdir -p progress/{}/\".format(self.tag))\n",
    "        os.system(\"cp gan.py progress/{}/\".format(self.tag))\n",
    "\n",
    "        self.scaler_type = kwargs[\"scaler_type\"]\n",
    "        self.scaler = None\n",
    "        if self.scaler_type.lower() == \"minmax\":\n",
    "            self.scaler = MinMaxScaler(feature_range=(-1.,1.))\n",
    "        elif self.scaler_type.lower() == \"robust\":\n",
    "            self.scaler = RobustScaler()\n",
    "        elif self.scaler_type.lower() == \"standard\":\n",
    "            self.scaler = StandardScaler()\n",
    "\n",
    "        self.data = None\n",
    "        self.data_ref = None\n",
    "        self.d_epochinfo = {}\n",
    "\n",
    "        # optimizer = Adam(0.0002, 0.5)\n",
    "        optimizer_d = self.optimizer_disc\n",
    "        # optimizer_d = \"sgd\"\n",
    "        optimizer_g = self.optimizer_gen\n",
    "        # optimizer_g = \"adam\"\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer_d,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build and compile the generator\n",
    "        self.generator = self.build_generator()\n",
    "        self.generator.compile(loss='binary_crossentropy', optimizer=optimizer_g)\n",
    "\n",
    "        # The generator takes noise as input and generated imgs\n",
    "        z = Input(shape=self.noise_shape)\n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The valid takes generated images as input and determines validity\n",
    "        valid = self.discriminator(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator) takes\n",
    "        # noise as input => generates images => determines validity\n",
    "        self.combined = Model(z, valid)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer_g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(GAN):\n",
    "    \n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        ## Head\n",
    "        model.add(Dense(64, input_shape=self.noise_shape))\n",
    "        if self.do_batch_normalization_gen:\n",
    "            model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        if self.do_concatenate_gen:\n",
    "            model.add(Lambda(lambda x: K.concatenate([x*x,x])))\n",
    "            model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        ## Main Body\n",
    "        if self.depth_gen > 0 and self.width_gen > 0:\n",
    "            for level in xrange(0,self.depth_gen):\n",
    "                model.add(Dense(width_gen/(2**level))) #Triangle with width halved at each level\n",
    "                model.add(LeakyReLU(alpha=0.2))\n",
    "        elif self.beefy_generator:\n",
    "            model.add(Dense(128))\n",
    "            model.add(LeakyReLU(alpha=0.2))\n",
    "            model.add(Dense(256))\n",
    "            model.add(LeakyReLU(alpha=0.2))\n",
    "            model.add(Dense(512))\n",
    "            model.add(LeakyReLU(alpha=0.2))\n",
    "            model.add(Dense(1024))\n",
    "            model.add(LeakyReLU(alpha=0.2))\n",
    "            model.add(Dense(1024))\n",
    "            model.add(LeakyReLU(alpha=0.2))\n",
    "            model.add(Dense(512))\n",
    "            model.add(LeakyReLU(alpha=0.2))\n",
    "            model.add(Dense(256))\n",
    "            model.add(LeakyReLU(alpha=0.2))\n",
    "            model.add(Dense(128))\n",
    "            model.add(LeakyReLU(alpha=0.2))\n",
    "            model.add(Dense(64))\n",
    "            model.add(LeakyReLU(alpha=0.2))\n",
    "            model.add(Dense(32))\n",
    "            model.add(LeakyReLU(alpha=0.2))\n",
    "        else:\n",
    "            model.add(Dense(128))\n",
    "            model.add(LeakyReLU(alpha=0.2))\n",
    "            model.add(Dense(128))\n",
    "            model.add(LeakyReLU(alpha=0.2))\n",
    "            model.add(Dense(128))\n",
    "            model.add(LeakyReLU(alpha=0.2))\n",
    "            model.add(Dense(64))\n",
    "            model.add(LeakyReLU(alpha=0.2))\n",
    "            model.add(Dense(32))\n",
    "            model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        ## Tail\n",
    "        model.add(Dense(self.output_shape[0]))\n",
    "        if self.do_tanh_gen:\n",
    "            model.add(Activation(\"tanh\"))\n",
    "        elif self.fix_delphes_outputs:\n",
    "            model.add(Lambda(fix_outputs,\n",
    "                input_shape=self.output_shape,\n",
    "                output_shape=self.output_shape\n",
    "                ))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=self.noise_shape)\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(GAN):\n",
    "    \n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        ## Head\n",
    "        if self.add_invmass_disc:\n",
    "            model.add(Lambda(add_invmass_from_8cartesian,\n",
    "                input_shape=self.output_shape,\n",
    "                output_shape=(self.output_shape[0]+1,),\n",
    "                ))\n",
    "        else:\n",
    "            model.add(Dense(128,input_shape=self.output_shape))\n",
    "\n",
    "        if self.do_batch_normalization_disc:\n",
    "            model.add(BatchNormalization())\n",
    "        # model.add(LeakyReLU(alpha=0.2))\n",
    "        if self.do_concatenate_disc:\n",
    "            model.add(Lambda(lambda x: K.concatenate([x*x,x])))\n",
    "            model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        ## Main Body\n",
    "        if self.depth_disc > 0 and self.width_disc > 0:\n",
    "            for level in xrange(0,self.depth_disc):\n",
    "                model.add(Dense(self.width_disc/(2**level))) #Triangle with width halved at each level\n",
    "                model.add(LeakyReLU(alpha=0.2))\n",
    "        else:\n",
    "            model.add(Dense(128))\n",
    "            model.add(LeakyReLU(alpha=0.2))\n",
    "            model.add(Dense(128))\n",
    "            model.add(LeakyReLU(alpha=0.2))\n",
    "            model.add(Dense(128))\n",
    "            model.add(LeakyReLU(alpha=0.2))\n",
    "            model.add(Dense(128))\n",
    "            model.add(LeakyReLU(alpha=0.2))\n",
    "            model.add(Dense(128))\n",
    "            model.add(LeakyReLU(alpha=0.2))\n",
    "            model.add(Dense(64))\n",
    "            model.add(LeakyReLU(alpha=0.2))\n",
    "            model.add(Dense(32))\n",
    "            model.add(LeakyReLU(alpha=0.2))\n",
    "            model.add(Dense(16))\n",
    "            model.add(LeakyReLU(alpha=0.2))\n",
    "            model.add(Dense(8))\n",
    "            model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        ## Tail\n",
    "        model.add(Dense(1,activation='sigmoid'))\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=self.output_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(GAN):\n",
    "    \n",
    "    def load_data(self):\n",
    "        if self.data is not None: return\n",
    "        \n",
    "        if self.use_delphes:\n",
    "            self.data = np.load(self.input_file)\n",
    "        else:\n",
    "            self.data = np.load(self.input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(GAN):\n",
    "            \n",
    "    def train(self):\n",
    "\n",
    "        self.load_data()\n",
    "        \n",
    "        if self.use_delphes:\n",
    "            lepcoords = np.c_[\n",
    "                self.data[\"lep1_e\"],\n",
    "                self.data[\"lep1_px\"],\n",
    "                self.data[\"lep1_py\"],\n",
    "                self.data[\"lep1_pz\"],\n",
    "                self.data[\"lep2_e\"],\n",
    "                self.data[\"lep2_px\"],\n",
    "                self.data[\"lep2_py\"],\n",
    "                self.data[\"lep2_pz\"],\n",
    "            ]\n",
    "            lepcoords_dphi = cartesian_zerophi2(lepcoords)\n",
    "            \n",
    "            nvtx_smeared = np.round(np.random.normal(self.data[\"nvtxs\"],0.5))\n",
    "            X_train = np.c_[\n",
    "                lepcoords_dphi, # 7 columns\n",
    "                nvtx_smeared, # 1 column\n",
    "                self.data[\"lep1_charge\"], self.data[\"lep2_charge\"],\n",
    "                self.data[\"lep1_iso\"], self.data[\"lep2_iso\"],\n",
    "                self.data[\"met\"], self.data[\"metphi\"],\n",
    "                self.data[\"genjet_pt1\"],\n",
    "                self.data[\"genjet_pt2\"],\n",
    "                self.data[\"genjet_pt3\"],\n",
    "                self.data[\"genjet_pt4\"],\n",
    "                self.data[\"genjet_pt5\"],\n",
    "            ].astype(np.float32)\n",
    "        else:\n",
    "            X_train = self.data[:,range(1,1+8)]\n",
    "            if self.use_ptetaphi_additionally:\n",
    "                X_train = np.c_[X_train, cartesian_to_ptetaphi(X_train)]\n",
    "\n",
    "        # # NOTE. StandardScaler should be fit on training set\n",
    "        # # and applied the same to train and test, otherwise we\n",
    "        # # introduce a bias\n",
    "        if self.scaler:\n",
    "            self.scaler.fit(X_train)\n",
    "            X_train = self.scaler.transform(X_train).astype(np.float32)\n",
    "            pickle.dump(self.scaler, open(\"progress/{}/scaler.pkl\".format(self.tag),'w'))\n",
    "\n",
    "        half_batch = int(self.batch_size / 2)\n",
    "\n",
    "        prev_gen_loss = -1\n",
    "        prev_disc_loss = -1\n",
    "        n_loss_same_gen = 0  # number of epochs for which generator loss has remained ~same (within 0.01%)\n",
    "        n_loss_same_disc = 0  # number of epochs for which discriminator loss has remained ~same (within 0.01%)\n",
    "        old_info = -1, -1\n",
    "        for epoch in range(self.nepochs_max):\n",
    "\n",
    "            if n_loss_same_gen > 500 or n_loss_same_disc > 500:\n",
    "                print \"BREAKING because disc/gen loss has remained the same for {}/{} epochs!\".format(n_loss_same_disc,n_loss_same_gen)\n",
    "                break\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random half batch of images\n",
    "            idx = np.random.randint(0, X_train.shape[0], half_batch)\n",
    "            imgs = X_train[idx]\n",
    "\n",
    "            # nominal\n",
    "            if self.noise_type == 1:\n",
    "                noise_half = np.random.normal(0, 1, (half_batch, self.noise_shape[0]))\n",
    "                noise_full = np.random.normal(0, 1, (self.batch_size, self.noise_shape[0]))\n",
    "\n",
    "                if epoch % self.nepochs_dump_pred_metrics == 0 and epoch > 0:\n",
    "                    noise_test = np.random.normal(0, 1, (self.ntest_samples,self.noise_shape[0]))\n",
    "\n",
    "            elif self.noise_type == 2: # random soup, 4,2,2 have to be modified to sum to noise_shape[0]\n",
    "                ngaus = self.noise_shape[0] // 2\n",
    "                nflat = (self.noise_shape[0] - ngaus) // 2\n",
    "                nexpo = self.noise_shape[0] - nflat - ngaus\n",
    "                noise_gaus = np.random.normal( 0, 1, (half_batch+self.batch_size, ngaus))\n",
    "                noise_flat = np.random.uniform(-1, 1, (half_batch+self.batch_size, nflat))\n",
    "                noise_expo = np.random.exponential( 1,    (half_batch+self.batch_size, nexpo))\n",
    "                noise = np.c_[ noise_gaus,noise_flat,noise_expo ]\n",
    "                noise_half = noise[:half_batch]\n",
    "                noise_full = noise[-self.batch_size:]\n",
    "\n",
    "                if epoch % self.nepochs_dump_pred_metrics == 0 and epoch > 0:\n",
    "                    noise_gaus = np.random.normal( 0, 1, (self.ntest_samples, ngaus))\n",
    "                    noise_flat = np.random.uniform(-1, 1, (self.ntest_samples, nflat))\n",
    "                    noise_expo = np.random.exponential( 1,    (self.ntest_samples, nexpo))\n",
    "                    noise_test = np.c_[ noise_gaus,noise_flat,noise_expo ]\n",
    "\n",
    "            # truth conditioned\n",
    "            elif self.noise_type == 3:\n",
    "                noise_half = np.c_[\n",
    "                        X_train[np.random.randint(0, X_train.shape[0], half_batch)],\n",
    "                        np.random.normal(0, 1, (half_batch,self.noise_shape[0]-X_train.shape[1]))\n",
    "                        ]\n",
    "                noise_full = np.c_[\n",
    "                        X_train[np.random.randint(0, X_train.shape[0], self.batch_size)],\n",
    "                        np.random.normal(0, 1, (self.batch_size,self.noise_shape[0]-X_train.shape[1]))\n",
    "                        ]\n",
    "\n",
    "                if epoch % self.nepochs_dump_pred_metrics == 0 and epoch > 0:\n",
    "                    noise_test = np.c_[\n",
    "                            X_train[np.random.randint(0, X_train.shape[0], self.ntest_samples)],\n",
    "                            np.random.normal(0, 1, (self.ntest_samples,self.noise_shape[0]-X_train.shape[1]))\n",
    "                            ]\n",
    "\n",
    "            # Generate a half batch of new images\n",
    "            gen_imgs = self.generator.predict(noise_half)\n",
    "\n",
    "            # Train the discriminator\n",
    "            ones = np.ones((half_batch, 1))\n",
    "            zeros = np.zeros((half_batch, 1))\n",
    "\n",
    "            if self.do_soft_labels:\n",
    "                ones *= 0.9\n",
    "\n",
    "            if self.do_noisy_labels:\n",
    "                frac = 0.3*np.exp(-epoch/self.nepochs_decay_noisy_labels)\n",
    "                if frac > 0.005:\n",
    "                    ones[np.random.randint(0, len(ones), int(frac*len(ones)))] = 0\n",
    "                    zeros[np.random.randint(0, len(zeros), int(frac*len(zeros)))] = 1\n",
    "\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, ones)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, zeros)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "            # The generator wants the discriminator to label the generated samples\n",
    "            # as valid (ones)\n",
    "            valid_y = np.array([1] * self.batch_size)\n",
    "\n",
    "            # Train the generator\n",
    "            g_loss = self.combined.train_on_batch(noise_full, valid_y)\n",
    "\n",
    "            if (g_loss - prev_gen_loss) < 0.0001: n_loss_same_gen += 1\n",
    "            else: n_loss_same_gen = 0\n",
    "            prev_gen_loss = g_loss\n",
    "\n",
    "            if (d_loss[0] - prev_disc_loss) < 0.0001: n_loss_same_disc += 1\n",
    "            else: n_loss_same_disc = 0\n",
    "            prev_disc_loss = d_loss[0]\n",
    "\n",
    "            # Plot the progress\n",
    "#             print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "            sys.stdout.write(\"\\r{} [D loss: {}, acc.: {:.2f}%] [G loss: {}] [mll={:.3f}+-{:.3f}]\".format(epoch, d_loss[0], 100*d_loss[1], g_loss, old_info[0], old_info[1]))\n",
    "\n",
    "            if epoch % self.nepochs_dump_pred_metrics == 0 and epoch > 0:\n",
    "            \n",
    "                sys.stdout.write(\"\\n\") # break up the stream of text\n",
    "\n",
    "                gen_imgs = self.generator.predict(noise_test)\n",
    "\n",
    "                if self.scaler:\n",
    "                    gen_imgs = self.scaler.inverse_transform(gen_imgs)\n",
    "\n",
    "                masses = Minv(gen_imgs)\n",
    "                masses = masses[np.isfinite(masses)]\n",
    "                old_info = masses.mean(), masses.std()\n",
    "\n",
    "                if \"epoch\" not in self.d_epochinfo:\n",
    "                    self.d_epochinfo[\"epoch\"] = []\n",
    "                    self.d_epochinfo[\"d_acc\"] = []\n",
    "                    self.d_epochinfo[\"d_loss\"] = []\n",
    "                    self.d_epochinfo[\"g_loss\"] = []\n",
    "                    self.d_epochinfo[\"mass_mu\"] = []\n",
    "                    self.d_epochinfo[\"mass_sig\"] = []\n",
    "                    self.d_epochinfo[\"time\"] = []\n",
    "                    self.d_epochinfo[\"args\"] = self.args\n",
    "                else:\n",
    "                    self.d_epochinfo[\"epoch\"].append(epoch)\n",
    "                    self.d_epochinfo[\"d_acc\"].append(100*d_loss[1])\n",
    "                    self.d_epochinfo[\"d_loss\"].append(d_loss[0])\n",
    "                    self.d_epochinfo[\"g_loss\"].append(g_loss)\n",
    "                    self.d_epochinfo[\"mass_mu\"].append(masses.mean())\n",
    "                    self.d_epochinfo[\"mass_sig\"].append(masses.std())\n",
    "                    self.d_epochinfo[\"time\"].append(time.time())\n",
    "\n",
    "                pickle.dump(self.d_epochinfo, open(\"progress/{}/history.pkl\".format(self.tag),'w'))\n",
    "\n",
    "            if epoch % self.nepochs_dump_models == 0 and epoch > 0:\n",
    "                self.discriminator.save(\"progress/{}/disc_{}.weights\".format(self.tag,epoch))\n",
    "                self.generator.save(\"progress/{}/gen_{}.weights\".format(self.tag,epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'width_disc': 0, 'ntest_samples': 5000, 'output_size': 19, 'do_batch_normalization_disc': False, 'use_delphes': True, 'use_ptetaphi_additionally': False, 'optimizer_disc': 'adadelta', 'fix_delphes_outputs': True, 'nepochs_dump_pred_metrics': 250, 'do_batch_normalization_gen': False, 'add_invmass_disc': False, 'width_gen': 0, 'noise_type': 1, 'nepochs_dump_models': 5000, 'input_file': '/home/users/namin/2017/gan/DY-GAN/delphes/data_Nov10.npa', 'scaler_type': '', 'batch_size': 200, 'do_concatenate_disc': False, 'do_soft_labels': True, 'depth_gen': 0, 'noise_size': 8, 'do_noisy_labels': True, 'nepochs_max': 30001, 'depth_disc': 0, 'do_tanh_gen': False, 'do_concatenate_gen': False, 'beefy_generator': False, 'nepochs_decay_noisy_labels': 2000, 'optimizer_gen': 'adadelta'}\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_235 (Dense)            (None, 128)               2560      \n",
      "_________________________________________________________________\n",
      "dense_236 (Dense)            (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_196 (LeakyReLU)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_237 (Dense)            (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_197 (LeakyReLU)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_238 (Dense)            (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_198 (LeakyReLU)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_239 (Dense)            (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_199 (LeakyReLU)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_240 (Dense)            (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_200 (LeakyReLU)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_241 (Dense)            (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_201 (LeakyReLU)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_242 (Dense)            (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_202 (LeakyReLU)  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_243 (Dense)            (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_203 (LeakyReLU)  (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_244 (Dense)            (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_204 (LeakyReLU)  (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_245 (Dense)            (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 96,129\n",
      "Trainable params: 96,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_246 (Dense)            (None, 64)                576       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_205 (LeakyReLU)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_247 (Dense)            (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_206 (LeakyReLU)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_248 (Dense)            (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_207 (LeakyReLU)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_249 (Dense)            (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_208 (LeakyReLU)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_250 (Dense)            (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_209 (LeakyReLU)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_251 (Dense)            (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_210 (LeakyReLU)  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_252 (Dense)            (None, 19)                627       \n",
      "_________________________________________________________________\n",
      "lambda_10 (Lambda)           (None, 19)                0         \n",
      "=================================================================\n",
      "Total params: 52,883\n",
      "Trainable params: 52,883\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# defaults\n",
    "params = {\n",
    "        \"input_file\": \"data_xyz.npy\",\n",
    "        \"output_size\": 8,\n",
    "        \"noise_size\": 8,\n",
    "        \"noise_type\": 1,\n",
    "        \"ntest_samples\": 5000,\n",
    "        \"nepochs_dump_pred_metrics\": 250,\n",
    "        \"nepochs_dump_models\": 5000,\n",
    "        \"nepochs_max\": 30001,\n",
    "        \"batch_size\": 200,\n",
    "        \"do_concatenate_disc\": False,\n",
    "        \"do_concatenate_gen\": False,\n",
    "        \"do_batch_normalization_disc\": False,\n",
    "        \"do_batch_normalization_gen\": False,\n",
    "        \"do_soft_labels\": False,\n",
    "        \"do_noisy_labels\": False,\n",
    "        \"do_tanh_gen\": False,\n",
    "        \"nepochs_decay_noisy_labels\": 3000,\n",
    "        \"use_ptetaphi_additionally\": False,\n",
    "        \"scaler_type\": \"\",\n",
    "        \"optimizer_disc\": \"adadelta\",\n",
    "        \"optimizer_gen\": \"adadelta\",\n",
    "        \"beefy_generator\": False,\n",
    "        \"depth_gen\": 0,\n",
    "        \"width_gen\": 0,\n",
    "        \"depth_disc\": 0,\n",
    "        \"width_disc\": 0,\n",
    "        \"add_invmass_disc\": False,\n",
    "        \"fix_delphes_outputs\": False,\n",
    "        \"use_delphes\": False,\n",
    "        }\n",
    "\n",
    "# for delphes:\n",
    "params.update({\n",
    "    \"use_delphes\": True,\n",
    "    \"fix_delphes_outputs\": True,\n",
    "    \"do_soft_labels\": True,\n",
    "    \"do_noisy_labels\": True,\n",
    "    \"nepochs_decay_noisy_labels\": 2000,\n",
    "    \"input_file\": \"/home/users/namin/2017/gan/DY-GAN/delphes/data_Nov10.npa\",\n",
    "    \"output_size\": 19,\n",
    "})\n",
    "print params\n",
    "    \n",
    "# change tag for provenance\n",
    "params[\"tag\"] = \"vtest\"\n",
    "\n",
    "gan = GAN(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250 [D loss: 0.459872126579, acc.: 44.50%] [G loss: 2.82814383507] [mll=-1.000+--1.000]]\n",
      "500 [D loss: 0.416689604521, acc.: 45.00%] [G loss: 2.34388518333] [mll=52.769+-8.571]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in sqrt\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:163: RuntimeWarning: Mean of empty slice.\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:179: RuntimeWarning: Mean of empty slice.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750 [D loss: 0.435890734196, acc.: 44.50%] [G loss: 2.132147789] [mll=nan+-nan]n]]\n",
      "1000 [D loss: 0.42704257369, acc.: 44.50%] [G loss: 2.21776294708] [mll=51.629+-6.831]\n",
      "1250 [D loss: 0.421676516533, acc.: 44.50%] [G loss: 2.30919766426] [mll=18.056+-1.792]\n",
      "1500 [D loss: 0.420112669468, acc.: 44.50%] [G loss: 2.21528601646] [mll=61.426+-7.152]]\n",
      "1750 [D loss: 0.415970265865, acc.: 44.50%] [G loss: 2.24930381775] [mll=43.094+-4.217]\n",
      "2000 [D loss: 0.472355365753, acc.: 44.50%] [G loss: 2.24461960793] [mll=14.042+-0.746]\n",
      "2250 [D loss: 0.294913113117, acc.: 48.00%] [G loss: 3.34412455559] [mll=107.564+-8.315]\n",
      "2500 [D loss: 0.286200761795, acc.: 48.00%] [G loss: 3.25450134277] [mll=180.351+-23.186]\n",
      "2750 [D loss: 0.285238087177, acc.: 48.00%] [G loss: 3.55289006233] [mll=17.265+-1.079]\n",
      "3000 [D loss: 0.285041481256, acc.: 48.00%] [G loss: 3.20800995827] [mll=nan+-nan]\n",
      "3250 [D loss: 0.284811139107, acc.: 48.00%] [G loss: 3.22374820709] [mll=nan+-nan]\n",
      "3500 [D loss: 0.283866852522, acc.: 48.00%] [G loss: 3.18980336189] [mll=63.438+-5.674]\n",
      "3750 [D loss: 0.634894430637, acc.: 10.50%] [G loss: 3.62506580353] [mll=127.439+-12.028]\n",
      "4000 [D loss: 0.287416487932, acc.: 48.00%] [G loss: 3.37093448639] [mll=nan+-nan]\n",
      "4250 [D loss: 0.20296394825, acc.: 49.50%] [G loss: 4.6004576683] [mll=263.155+-26.541]1]\n",
      "4500 [D loss: 0.289672553539, acc.: 49.50%] [G loss: 6.83204078674] [mll=nan+-nan]\n",
      "4750 [D loss: 0.236076936126, acc.: 49.50%] [G loss: 4.54376363754] [mll=280.232+-28.895]\n",
      "5000 [D loss: 0.225720882416, acc.: 49.50%] [G loss: 4.45466566086] [mll=383.462+-36.452]\n",
      "5250 [D loss: 0.643084406853, acc.: 18.50%] [G loss: 2.02712702751] [mll=206.120+-13.684]\n",
      "5500 [D loss: 0.205253586173, acc.: 49.50%] [G loss: 4.81654071808] [mll=159.670+-40.938]\n",
      "5750 [D loss: 0.545569181442, acc.: 26.50%] [G loss: 6.4935259819] [mll=35.027+-1.552]]\n",
      "6000 [D loss: 0.281944364309, acc.: 49.50%] [G loss: 4.80425548553] [mll=266.148+-14.840]\n",
      "6250 [D loss: 0.382117420435, acc.: 50.00%] [G loss: 3.18087220192] [mll=92.672+-6.354]\n",
      "6500 [D loss: 0.488868772984, acc.: 50.00%] [G loss: 2.65555214882] [mll=142.135+-67.209]\n",
      "6750 [D loss: 0.447162300348, acc.: 49.50%] [G loss: 2.9666082859] [mll=132.553+-74.160]]\n",
      "7000 [D loss: 0.474529325962, acc.: 50.00%] [G loss: 2.85649132729] [mll=178.289+-115.795]\n",
      "7250 [D loss: 0.403779834509, acc.: 43.00%] [G loss: 2.88322091103] [mll=109.182+-33.516]\n",
      "7500 [D loss: 0.236796855927, acc.: 49.00%] [G loss: 4.17976522446] [mll=164.252+-58.133]\n",
      "7750 [D loss: 0.368606179953, acc.: 45.50%] [G loss: 3.56329274178] [mll=167.208+-88.972]\n",
      "8000 [D loss: 0.338658750057, acc.: 48.00%] [G loss: 3.30790829659] [mll=103.478+-59.805]\n",
      "8250 [D loss: 0.457990407944, acc.: 41.00%] [G loss: 3.01963949203] [mll=131.912+-86.152]\n",
      "8500 [D loss: 0.271274358034, acc.: 46.50%] [G loss: 10.2808923721] [mll=118.755+-64.410]\n",
      "8750 [D loss: 0.186393275857, acc.: 49.50%] [G loss: 5.63198471069] [mll=104.345+-48.878]\n",
      "9000 [D loss: 0.17244964838, acc.: 50.00%] [G loss: 9.60325527191] [mll=172.345+-47.741]]\n",
      "9250 [D loss: 0.166074424982, acc.: 50.00%] [G loss: 10.5606136322] [mll=212.643+-93.384]\n",
      "9500 [D loss: 0.22733733058, acc.: 49.00%] [G loss: 9.93330669403] [mll=192.084+-53.071]]\n",
      "9750 [D loss: 0.250195771456, acc.: 48.00%] [G loss: 7.71242570877] [mll=239.463+-85.267]\n",
      "10000 [D loss: 0.210568413138, acc.: 50.00%] [G loss: 5.99842023849] [mll=359.252+-131.957]\n",
      "10250 [D loss: 0.226267442107, acc.: 47.50%] [G loss: 6.6478843689] [mll=207.503+-111.297]]\n",
      "10447 [D loss: 0.206835985184, acc.: 50.00%] [G loss: 5.97774362564] [mll=202.789+-60.589]"
     ]
    }
   ],
   "source": [
    "gan.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"/home/users/namin/2017/gan/DY-GAN/delphes/data_Nov10.npa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('mll', 'lep1_e', 'lep1_px', 'lep1_py', 'lep1_pz', 'lep2_e', 'lep2_px', 'lep2_py', 'lep2_pz', 'lep1_charge', 'lep1_iso', 'lep2_charge', 'lep2_iso', 'nvtxs', 'met', 'metphi', 'ngenjets', 'genjet_pt1', 'genjet_pt2', 'genjet_pt3', 'genjet_pt4', 'genjet_pt5')\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x288 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# gan.load_data()\n",
    "print data.dtype.names\n",
    "nvtx = data[\"nvtxs\"]\n",
    "# for each nvtx, draw a gaussian distributed number with mu,sig = (nvtx,0.5)\n",
    "# these will be the continuous numbers we use with the generator\n",
    "# then round it (these will be the values at the end of the day)\n",
    "# so these rounded gaus numbers should ~match original nvtx distribution\n",
    "nvtx_smeared_unrounded = np.random.normal(nvtx,0.5)\n",
    "nvtx_smeared = np.round(nvtx_smeared_unrounded)\n",
    "\n",
    "fix,(ax1,ax2,ax3) = plt.subplots(1,3,figsize=(13,4))\n",
    "_ = ax1.hist(nvtx,bins=np.linspace(0,50,50), histtype=\"step\",lw=2,density=True,label=\"true nvtx\")\n",
    "_ = ax1.hist(nvtx_smeared,bins=np.linspace(0,50,50), histtype=\"step\",lw=2,density=True,label=\"smeared, rounded nvtx\")\n",
    "_ = ax2.hist(nvtx_smeared_unrounded,bins=np.linspace(0,50,500), histtype=\"step\",lw=2,density=True,label=\"smeared\")\n",
    "_ = ax3.hist(nvtx-nvtx_smeared,bins=np.linspace(-5,5,10), histtype=\"step\",lw=2,density=True,label=\"orig-rounded smeared\")\n",
    "_ = ax1.legend()\n",
    "_ = ax2.legend()\n",
    "_ = ax3.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
